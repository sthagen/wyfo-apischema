{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview \u00b6 apischema \u00b6 JSON (de)serialization, GraphQL and JSON schema generation using Python typing. apischema makes your life easier when dealing with API data. Install \u00b6 pip install apischema It requires only Python 3.8+. PyPy3 is also fully supported. Why another library? \u00b6 This library fulfills the following goals: stay as close as possible to the standard library (dataclasses, typing, etc.) \u2014 as a consequence we do not need plugins for editors/linters/etc.; avoid object-oriented limitations \u2014 do not require a base class \u2014 thus handle easily every type ( Foo , list[Bar] , NewType(Id, int) , etc.) the same way. be adaptable, provide tools to support any types (ORM, etc.); avoid dynamic things like using raw strings for attributes name - play nicely with your IDE. No known alternative achieves all of this, and apischema is also (a lot) faster than all of them. On top of that, because APIs are not only JSON, apischema is also a complete GraphQL library Note Actually, apischema is even adaptable enough to enable support of competitor libraries in a few dozens of line of code ( pydantic support example using conversions feature ) Example \u00b6 from collections.abc import Collection from dataclasses import dataclass , field from uuid import UUID , uuid4 import pytest from graphql import print_schema from apischema import ValidationError , deserialize , serialize from apischema.graphql import graphql_schema from apischema.json_schema import deserialization_schema # Define a schema with standard dataclasses @dataclass class Resource : id : UUID name : str tags : set [ str ] = field ( default_factory = set ) # Get some data uuid = uuid4 () data = { \"id\" : str ( uuid ), \"name\" : \"wyfo\" , \"tags\" : [ \"some_tag\" ]} # Deserialize data resource = deserialize ( Resource , data ) assert resource == Resource ( uuid , \"wyfo\" , { \"some_tag\" }) # Serialize objects assert serialize ( Resource , resource ) == data # Validate during deserialization with pytest . raises ( ValidationError ) as err : # pytest checks exception is raised deserialize ( Resource , { \"id\" : \"42\" , \"name\" : \"wyfo\" }) assert err . value . errors == [ { \"loc\" : [ \"id\" ], \"err\" : \"badly formed hexadecimal UUID string\" } ] # Generate JSON Schema assert deserialization_schema ( Resource ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"type\" : \"object\" , \"properties\" : { \"id\" : { \"type\" : \"string\" , \"format\" : \"uuid\" }, \"name\" : { \"type\" : \"string\" }, \"tags\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" }, \"uniqueItems\" : True , \"default\" : [], }, }, \"required\" : [ \"id\" , \"name\" ], \"additionalProperties\" : False , } # Define GraphQL operations def resources ( tags : Collection [ str ] | None = None ) -> Collection [ Resource ] | None : ... # Generate GraphQL schema schema = graphql_schema ( query = [ resources ], id_types = { UUID }) schema_str = \"\"\" \\ type Query { resources(tags: [String!]): [Resource!] } type Resource { id: ID! name: String! tags: [String!]! }\"\"\" assert print_schema ( schema ) == schema_str apischema works out of the box with your data model. Note This example and further ones are using pytest API because they are in fact run as tests in the library CI Run the documentation examples \u00b6 All documentation examples are written using the last Python minor version \u2014 currently 3.10 \u2014 in order to provide up-to-date documentation. Because Python 3.10 specificities (like PEP 585 ) are used, this version is \"mandatory\" to execute the examples as-is. In addition to pytest , some examples use third-party libraries like SQLAlchemy or attrs . All of this dependencies can be downloaded using the examples extra with pip install apischema [ examples ] Once dependencies are installed, you can simply copy-paste examples and execute them, using the proper Python version. FAQ \u00b6 What is the difference between apischema and pydantic ? \u00b6 See the dedicated section \u2014 there are many differences. I already have my data model with my SQLAlchemy /ORM tables, will I have to duplicate my code, making one dataclass per table? \u00b6 No, apischema works with user-defined types as well as types from foreign libraries. Using the conversion feature, you can add default serialization for all your tables, or register a different serializer that you can select according to your API endpoint, or both. I need more accurate validation than \"ensure this is an integer and not a string \", can I do that? \u00b6 See the validation section. You can use standard JSON schema validation ( maxItems , pattern , etc.) that will be embedded in your schema or add custom Python validators for each class/fields/ NewType you want.","title":"Overview"},{"location":"#overview","text":"","title":"Overview"},{"location":"#apischema","text":"JSON (de)serialization, GraphQL and JSON schema generation using Python typing. apischema makes your life easier when dealing with API data.","title":"apischema"},{"location":"#install","text":"pip install apischema It requires only Python 3.8+. PyPy3 is also fully supported.","title":"Install"},{"location":"#why-another-library","text":"This library fulfills the following goals: stay as close as possible to the standard library (dataclasses, typing, etc.) \u2014 as a consequence we do not need plugins for editors/linters/etc.; avoid object-oriented limitations \u2014 do not require a base class \u2014 thus handle easily every type ( Foo , list[Bar] , NewType(Id, int) , etc.) the same way. be adaptable, provide tools to support any types (ORM, etc.); avoid dynamic things like using raw strings for attributes name - play nicely with your IDE. No known alternative achieves all of this, and apischema is also (a lot) faster than all of them. On top of that, because APIs are not only JSON, apischema is also a complete GraphQL library Note Actually, apischema is even adaptable enough to enable support of competitor libraries in a few dozens of line of code ( pydantic support example using conversions feature )","title":"Why another library?"},{"location":"#example","text":"from collections.abc import Collection from dataclasses import dataclass , field from uuid import UUID , uuid4 import pytest from graphql import print_schema from apischema import ValidationError , deserialize , serialize from apischema.graphql import graphql_schema from apischema.json_schema import deserialization_schema # Define a schema with standard dataclasses @dataclass class Resource : id : UUID name : str tags : set [ str ] = field ( default_factory = set ) # Get some data uuid = uuid4 () data = { \"id\" : str ( uuid ), \"name\" : \"wyfo\" , \"tags\" : [ \"some_tag\" ]} # Deserialize data resource = deserialize ( Resource , data ) assert resource == Resource ( uuid , \"wyfo\" , { \"some_tag\" }) # Serialize objects assert serialize ( Resource , resource ) == data # Validate during deserialization with pytest . raises ( ValidationError ) as err : # pytest checks exception is raised deserialize ( Resource , { \"id\" : \"42\" , \"name\" : \"wyfo\" }) assert err . value . errors == [ { \"loc\" : [ \"id\" ], \"err\" : \"badly formed hexadecimal UUID string\" } ] # Generate JSON Schema assert deserialization_schema ( Resource ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"type\" : \"object\" , \"properties\" : { \"id\" : { \"type\" : \"string\" , \"format\" : \"uuid\" }, \"name\" : { \"type\" : \"string\" }, \"tags\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" }, \"uniqueItems\" : True , \"default\" : [], }, }, \"required\" : [ \"id\" , \"name\" ], \"additionalProperties\" : False , } # Define GraphQL operations def resources ( tags : Collection [ str ] | None = None ) -> Collection [ Resource ] | None : ... # Generate GraphQL schema schema = graphql_schema ( query = [ resources ], id_types = { UUID }) schema_str = \"\"\" \\ type Query { resources(tags: [String!]): [Resource!] } type Resource { id: ID! name: String! tags: [String!]! }\"\"\" assert print_schema ( schema ) == schema_str apischema works out of the box with your data model. Note This example and further ones are using pytest API because they are in fact run as tests in the library CI","title":"Example"},{"location":"#run-the-documentation-examples","text":"All documentation examples are written using the last Python minor version \u2014 currently 3.10 \u2014 in order to provide up-to-date documentation. Because Python 3.10 specificities (like PEP 585 ) are used, this version is \"mandatory\" to execute the examples as-is. In addition to pytest , some examples use third-party libraries like SQLAlchemy or attrs . All of this dependencies can be downloaded using the examples extra with pip install apischema [ examples ] Once dependencies are installed, you can simply copy-paste examples and execute them, using the proper Python version.","title":"Run the documentation examples"},{"location":"#faq","text":"","title":"FAQ"},{"location":"#what-is-the-difference-between-apischema-and-pydantic","text":"See the dedicated section \u2014 there are many differences.","title":"What is the difference between apischema and pydantic?"},{"location":"#i-already-have-my-data-model-with-my-sqlalchemyorm-tables-will-i-have-to-duplicate-my-code-making-one-dataclass-per-table","text":"No, apischema works with user-defined types as well as types from foreign libraries. Using the conversion feature, you can add default serialization for all your tables, or register a different serializer that you can select according to your API endpoint, or both.","title":"I already have my data model with my SQLAlchemy/ORM tables, will I have to duplicate my code, making one dataclass per table?"},{"location":"#i-need-more-accurate-validation-than-ensure-this-is-an-integer-and-not-a-string-can-i-do-that","text":"See the validation section. You can use standard JSON schema validation ( maxItems , pattern , etc.) that will be embedded in your schema or add custom Python validators for each class/fields/ NewType you want.","title":"I need more accurate validation than \"ensure this is an integer and not a string \", can I do that?"},{"location":"conversions/","text":"Conversions \u2013 (de)serialization customization \u00b6 apischema covers the majority of standard data types, but of course that's not enough, which is why it enables you to add support for all your classes and the libraries you use. Actually, apischema itself uses this conversion feature to provide a basic support for standard library data types like UUID/datetime/etc. (see std_types.py ) ORM support can easily be achieved with this feature (see SQLAlchemy example ). In fact, you can even add support for competitor libraries like Pydantic (see Pydantic compatibility example ) Principle - apischema conversions \u00b6 An apischema conversion is composed of a source type, let's call it Source , a target type Target and a converter function with signature (Source) -> Target . When a class (actually, a non-builtin class, so not int / list /etc.) is deserialized, apischema will check if there is a conversion where this type is the target. If found, the source type of conversion will be deserialized, then the converter will be applied to get an object of the expected type. Serialization works the same way but inverted: look for a conversion with type as source, apply then converter, and get the target type. Conversions are also handled in schema generation: for a deserialization schema, source schema is merged to target schema, while target schema is merged to source schema for a serialization schema. Register a conversion \u00b6 Conversion is registered using apischema.deserializer / apischema.serializer for deserialization/serialization respectively. When used as function decorator, the Source / Target types are directly extracted from the conversion function signature. serializer can be called on methods/properties, in which case Source type is inferred to be the owning type. from dataclasses import dataclass from apischema import deserialize , schema , serialize from apischema.conversions import deserializer , serializer from apischema.json_schema import deserialization_schema , serialization_schema @schema ( pattern = r \"^#[0-9a-fA-F] {6} $\" ) @dataclass class RGB : red : int green : int blue : int @serializer @property def hexa ( self ) -> str : return f \"# { self . red : 02x }{ self . green : 02x }{ self . blue : 02x } \" # serializer can also be called with methods/properties outside of the class # For example, `serializer(RGB.hexa)` would have the same effect as the decorator above @deserializer def from_hexa ( hexa : str ) -> RGB : return RGB ( int ( hexa [ 1 : 3 ], 16 ), int ( hexa [ 3 : 5 ], 16 ), int ( hexa [ 5 : 7 ], 16 )) assert deserialize ( RGB , \"#000000\" ) == RGB ( 0 , 0 , 0 ) assert serialize ( RGB , RGB ( 0 , 0 , 42 )) == \"#00002a\" assert ( deserialization_schema ( RGB ) == serialization_schema ( RGB ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"type\" : \"string\" , \"pattern\" : \"^#[0-9a-fA-F] {6} $\" , } ) Warning (De)serializer methods cannot be used with typing.NamedTuple ; in fact, apischema uses the __set_name__ magic method but it is not called on NamedTuple subclass fields. Multiple deserializers \u00b6 Sometimes, you want to have several possibilities to deserialize a type. If it's possible to register a deserializer with a Union param, it's not very practical. That's why apischema make it possible to register several deserializers for the same type. They will be handled with a Union source type (ordered by deserializers registration), with the right serializer selected according to the matching alternative. from dataclasses import dataclass from apischema import deserialize , deserializer from apischema.json_schema import deserialization_schema @dataclass class Expression : value : int @deserializer def evaluate_expression ( expr : str ) -> Expression : return Expression ( int ( eval ( expr ))) # Could be shorten into deserializer(Expression), because class is callable too @deserializer def expression_from_value ( value : int ) -> Expression : return Expression ( value ) assert deserialization_schema ( Expression ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"type\" : [ \"string\" , \"integer\" ], } assert deserialize ( Expression , 0 ) == deserialize ( Expression , \"1 - 1\" ) == Expression ( 0 ) On the other hand, serializer registration overwrites the previous registration if any. apischema.conversions.reset_deserializers / apischema.conversions.reset_serializers can be used to reset (de)serializers (even those of the standard types embedded in apischema ) Inheritance \u00b6 All serializers are naturally inherited. In fact, with a conversion function (Source) -> Target , you can always pass a subtype of Source and get a Target in return. Moreover, when serializer is a method/property, overriding this method/property in a subclass will override the inherited serializer. from apischema import serialize , serializer class Foo : pass @serializer def serialize_foo ( foo : Foo ) -> int : return 0 class Foo2 ( Foo ): pass # Deserializer is inherited assert serialize ( Foo , Foo ()) == serialize ( Foo2 , Foo2 ()) == 0 class Bar : @serializer def serialize ( self ) -> int : return 0 class Bar2 ( Bar ): def serialize ( self ) -> int : return 1 # Deserializer is inherited and overridden assert serialize ( Bar , Bar ()) == 0 != serialize ( Bar2 , Bar2 ()) == 1 Note Inheritance can also be toggled off in specific cases, like in the Class as union of its subclasses example On the other hand, deserializers cannot be inherited, because the same Source passed to a conversion function (Source) -> Target will always give the same Target (not ensured to be the desired subtype). Note Pseudo-inheritance could be achieved by registering a conversion (using for example a classmethod ) for each subclass in __init_subclass__ method (or a metaclass), or by using __subclasses__ ; see example Generic conversions \u00b6 Generic conversions are supported out of the box. from typing import Generic , TypeVar import pytest from apischema import ValidationError , deserialize , serialize from apischema.conversions import deserializer , serializer from apischema.json_schema import deserialization_schema , serialization_schema T = TypeVar ( \"T\" ) class Wrapper ( Generic [ T ]): def __init__ ( self , wrapped : T ): self . wrapped = wrapped @serializer def unwrap ( self ) -> T : return self . wrapped # Wrapper constructor can be used as a function too (so deserializer could work as decorator) deserializer ( Wrapper ) assert deserialize ( Wrapper [ list [ int ]], [ 0 , 1 ]) . wrapped == [ 0 , 1 ] with pytest . raises ( ValidationError ): deserialize ( Wrapper [ int ], \"wrapped\" ) assert serialize ( Wrapper [ str ], Wrapper ( \"wrapped\" )) == \"wrapped\" assert ( deserialization_schema ( Wrapper [ int ]) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"type\" : \"integer\" } == serialization_schema ( Wrapper [ int ]) ) However, you're not allowed to register a conversion of a specialized generic type, like Foo[int] . Conversion object \u00b6 In the previous example, conversions were registered using only converter functions. However, it can also be done by passing a apischema.conversions.Conversion instance. It allows specifying additional metadata to conversion (see next sections for examples) and precise converter source/target when annotations are not available. from base64 import b64decode from apischema import deserialize , deserializer from apischema.conversions import Conversion deserializer ( Conversion ( b64decode , source = str , target = bytes )) # Roughly equivalent to: # def decode_bytes(source: str) -> bytes: # return b64decode(source) # but saving a function call assert deserialize ( bytes , \"Zm9v\" ) == b \"foo\" Dynamic conversions \u2014 select conversions at runtime \u00b6 Whether or not a conversion is registered for a given type, conversions can also be provided at runtime, using the conversion parameter of deserialize / serialize / deserialization_schema / serialization_schema . import os import time from dataclasses import dataclass from datetime import datetime from typing import Annotated from apischema import deserialize , serialize from apischema.metadata import conversion # Set UTC timezone for example os . environ [ \"TZ\" ] = \"UTC\" time . tzset () def datetime_from_timestamp ( timestamp : int ) -> datetime : return datetime . fromtimestamp ( timestamp ) date = datetime ( 2017 , 9 , 2 ) assert deserialize ( datetime , 1504310400 , conversion = datetime_from_timestamp ) == date @dataclass class Foo : bar : int baz : int def sum ( self ) -> int : return self . bar + self . baz @property def diff ( self ) -> int : return int ( self . bar - self . baz ) assert serialize ( Foo , Foo ( 0 , 1 )) == { \"bar\" : 0 , \"baz\" : 1 } assert serialize ( Foo , Foo ( 0 , 1 ), conversion = Foo . sum ) == 1 assert serialize ( Foo , Foo ( 0 , 1 ), conversion = Foo . diff ) == - 1 # conversions can be specified using Annotated assert serialize ( Annotated [ Foo , conversion ( serialization = Foo . sum )], Foo ( 0 , 1 )) == 1 Note For definitions_schema , conversions can be added with types by using a tuple instead, for example definitions_schema(serializations=[(list[Foo], foo_to_bar)]) . The conversion parameter can also take a tuple of conversions, when you have a Union , a tuple or when you want to have several deserializations for the same type. Dynamic conversions are local \u00b6 Dynamic conversions are discarded after having been applied (or after class without conversion having been encountered). For example, you can't apply directly a dynamic conversion to a dataclass field when calling serialize on an instance of this dataclass. Reasons for this design are detailed in the FAQ . import os import time from dataclasses import dataclass from datetime import datetime from apischema import serialize # Set UTC timezone for example os . environ [ \"TZ\" ] = \"UTC\" time . tzset () def to_timestamp ( d : datetime ) -> int : return int ( d . timestamp ()) @dataclass class Foo : bar : datetime # timestamp conversion is not applied on Foo field because it's discarded # when encountering Foo assert serialize ( Foo , Foo ( datetime ( 2019 , 10 , 13 )), conversion = to_timestamp ) == { \"bar\" : \"2019-10-13T00:00:00\" } # timestamp conversion is applied on every member of list assert serialize ( list [ datetime ], [ datetime ( 1970 , 1 , 1 )], conversion = to_timestamp ) == [ 0 ] Note Dynamic conversion is not discarded when the encountered type is a container ( list , dict , Collection , etc. or Union ) or a registered conversion from/to a container; the dynamic conversion can then apply to the container elements Dynamic conversions interact with type_name \u00b6 Dynamic conversions are applied before looking for a ref registered with type_name from dataclasses import dataclass from apischema import type_name from apischema.json_schema import serialization_schema @dataclass class Foo : pass @dataclass class Bar : pass def foo_to_bar ( _ : Foo ) -> Bar : return Bar () type_name ( \"Bars\" )( list [ Bar ]) assert serialization_schema ( list [ Foo ], conversion = foo_to_bar , all_refs = True ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"$ref\" : \"#/$defs/Bars\" , \"$defs\" : { # Bars is present because `list[Foo]` is dynamically converted to `list[Bar]` \"Bars\" : { \"type\" : \"array\" , \"items\" : { \"$ref\" : \"#/$defs/Bar\" }}, \"Bar\" : { \"type\" : \"object\" , \"additionalProperties\" : False }, }, } Bypass registered conversion \u00b6 Using apischema.identity as a dynamic conversion allows you to bypass a registered conversion, i.e. to (de)serialize the given type as it would be without conversion registered. from dataclasses import dataclass from apischema import identity , serialize , serializer from apischema.conversions import Conversion @dataclass class RGB : red : int green : int blue : int @serializer @property def hexa ( self ) -> str : return f \"# { self . red : 02x }{ self . green : 02x }{ self . blue : 02x } \" assert serialize ( RGB , RGB ( 0 , 0 , 0 )) == \"#000000\" # dynamic conversion used to bypass the registered one assert serialize ( RGB , RGB ( 0 , 0 , 0 ), conversion = identity ) == { \"red\" : 0 , \"green\" : 0 , \"blue\" : 0 , } # Expended bypass form assert serialize ( RGB , RGB ( 0 , 0 , 0 ), conversion = Conversion ( identity , source = RGB , target = RGB ) ) == { \"red\" : 0 , \"green\" : 0 , \"blue\" : 0 } Note For a more precise selection of bypassed conversion, for tuple or Union member for example, it's possible to pass the concerned class as the source and the target of conversion with identity converter, as shown in the example. Liskov substitution principle \u00b6 LSP is taken into account when applying dynamic conversion: the serializer source can be a subclass of the actual class and the deserializer target can be a superclass of the actual class. from dataclasses import dataclass from apischema import deserialize , serialize @dataclass class Foo : field : int @dataclass class Bar ( Foo ): other : str def foo_to_int ( foo : Foo ) -> int : return foo . field def bar_from_int ( i : int ) -> Bar : return Bar ( i , str ( i )) assert serialize ( Bar , Bar ( 0 , \"\" ), conversion = foo_to_int ) == 0 assert deserialize ( Foo , 0 , conversion = bar_from_int ) == Bar ( 0 , \"0\" ) Generic dynamic conversions \u00b6 Generic dynamic conversions are supported out of the box. Also, contrary to registered conversions, partially specialized generics are allowed. from collections.abc import Mapping , Sequence from operator import itemgetter from typing import TypeVar from apischema import serialize from apischema.json_schema import serialization_schema T = TypeVar ( \"T\" ) Priority = int def sort_by_priority ( values_with_priority : Mapping [ T , Priority ]) -> Sequence [ T ]: return [ k for k , _ in sorted ( values_with_priority . items (), key = itemgetter ( 1 ))] assert serialize ( dict [ str , Priority ], { \"a\" : 1 , \"b\" : 0 }, conversion = sort_by_priority ) == [ \"b\" , \"a\" ] assert serialization_schema ( dict [ str , Priority ], conversion = sort_by_priority ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" }, } Field conversions \u00b6 It is possible to register a conversion for a particular dataclass field using conversion metadata. import os import time from dataclasses import dataclass , field from datetime import datetime from apischema import deserialize , serialize from apischema.conversions import Conversion from apischema.metadata import conversion # Set UTC timezone for example os . environ [ \"TZ\" ] = \"UTC\" time . tzset () from_timestamp = Conversion ( datetime . fromtimestamp , source = int , target = datetime ) def to_timestamp ( d : datetime ) -> int : return int ( d . timestamp ()) @dataclass class Foo : some_date : datetime = field ( metadata = conversion ( from_timestamp , to_timestamp )) other_date : datetime assert deserialize ( Foo , { \"some_date\" : 0 , \"other_date\" : \"2019-10-13\" }) == Foo ( datetime ( 1970 , 1 , 1 ), datetime ( 2019 , 10 , 13 ) ) assert serialize ( Foo , Foo ( datetime ( 1970 , 1 , 1 ), datetime ( 2019 , 10 , 13 ))) == { \"some_date\" : 0 , \"other_date\" : \"2019-10-13T00:00:00\" , } Note It's possible to pass a conversion only for deserialization or only for serialization Serialized method conversions \u00b6 Serialized methods can also have dedicated conversions for their return import os import time from dataclasses import dataclass from datetime import datetime from apischema import serialize , serialized # Set UTC timezone for example os . environ [ \"TZ\" ] = \"UTC\" time . tzset () def to_timestamp ( d : datetime ) -> int : return int ( d . timestamp ()) @dataclass class Foo : @serialized ( conversion = to_timestamp ) def some_date ( self ) -> datetime : return datetime ( 1970 , 1 , 1 ) assert serialize ( Foo , Foo ()) == { \"some_date\" : 0 } Default conversions \u00b6 As with almost every default behavior in apischema , default conversions can be configured using apischema.settings.deserialization.default_conversion / apischema.settings.serialization.default_conversion . The initial value of these settings are the function which retrieved conversions registered with deserializer / serializer . You can for example support attrs classes with this feature: from typing import Sequence import attrs from apischema import deserialize , serialize , settings from apischema.json_schema import deserialization_schema from apischema.objects import ObjectField prev_default_object_fields = settings . default_object_fields def attrs_fields ( cls : type ) -> Sequence [ ObjectField ] | None : if hasattr ( cls , \"__attrs_attrs__\" ): return [ ObjectField ( a . name , a . type , required = a . default == attrs . NOTHING , default = a . default ) for a in getattr ( cls , \"__attrs_attrs__\" ) ] else : return prev_default_object_fields ( cls ) settings . default_object_fields = attrs_fields @attrs . define class Foo : bar : int assert deserialize ( Foo , { \"bar\" : 0 }) == Foo ( 0 ) assert serialize ( Foo , Foo ( 0 )) == { \"bar\" : 0 } assert deserialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : \"integer\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , } apischema functions ( deserialize / serialize / deserialization_schema / serialization_schema / definitions_schema ) also have a default_conversion parameter to dynamically modify default conversions. See FAQ for the difference between conversion and default_conversion parameters. Sub-conversions \u00b6 Sub-conversions are dynamic conversions applied on the result of a conversion. from dataclasses import dataclass from typing import Generic , TypeVar from apischema.conversions import Conversion from apischema.json_schema import serialization_schema T = TypeVar ( \"T\" ) class Query ( Generic [ T ]): ... def query_to_list ( q : Query [ T ]) -> list [ T ]: ... def query_to_scalar ( q : Query [ T ]) -> T | None : ... @dataclass class FooModel : bar : int class Foo : def serialize ( self ) -> FooModel : ... assert serialization_schema ( Query [ Foo ], conversion = Conversion ( query_to_list , sub_conversion = Foo . serialize ) ) == { # We get an array of Foo \"type\" : \"array\" , \"items\" : { \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : \"integer\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , }, \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , } Sub-conversions can also be used to bypass registered conversions or to define recursive conversions . Lazy/recursive conversions \u00b6 Conversions can be defined lazily, i.e. using a function returning Conversion (single, or a tuple of it); this function must be wrapped into a apischema.conversions.LazyConversion instance. It allows creating recursive conversions or using a conversion object which can be modified after its definition (for example a conversion for a base class modified by __init_subclass__ ) It is used by apischema itself for the generated JSON schema. It is indeed a recursive data, and the different versions are handled by a conversion with a lazy recursive sub-conversion. from dataclasses import dataclass from apischema import serialize from apischema.conversions import Conversion , LazyConversion @dataclass class Foo : elements : list [ \"int | Foo\" ] def foo_elements ( foo : Foo ) -> list [ int | Foo ]: return foo . elements # Recursive conversion pattern tmp = None conversion = Conversion ( foo_elements , sub_conversion = LazyConversion ( lambda : tmp )) tmp = conversion assert serialize ( Foo , Foo ([ 0 , Foo ([ 1 ])]), conversion = conversion ) == [ 0 , [ 1 ]] # Without the recursive sub-conversion, it would have been: assert serialize ( Foo , Foo ([ 0 , Foo ([ 1 ])]), conversion = foo_elements ) == [ 0 , { \"elements\" : [ 1 ]}, ] Lazy registered conversions \u00b6 Lazy conversions can also be registered, but the deserialization target/serialization source has to be passed too. from dataclasses import dataclass from apischema import deserialize , deserializer , serialize , serializer from apischema.conversions import Conversion @dataclass class Foo : bar : int deserializer ( lazy = lambda : Conversion ( lambda bar : Foo ( bar ), source = int , target = Foo ), target = Foo ) serializer ( lazy = lambda : Conversion ( lambda foo : foo . bar , source = Foo , target = int ), source = Foo ) assert deserialize ( Foo , 0 ) == Foo ( 0 ) assert serialize ( Foo , Foo ( 0 )) == 0 Conversion helpers \u00b6 String conversions \u00b6 A common pattern of conversion concerns classes that have a string constructor and a __str__ method, for example standard types uuid.UUID , pathlib.Path , or ipaddress.IPv4Address . Using apischema.conversions.as_str will register a string-deserializer from the constructor and a string-serializer from the __str__ method. ValueError raised by the constructor is caught and converted to ValidationError . import bson import pytest from apischema import Unsupported , deserialize , serialize from apischema.conversions import as_str with pytest . raises ( Unsupported ): deserialize ( bson . ObjectId , \"0123456789ab0123456789ab\" ) with pytest . raises ( Unsupported ): serialize ( bson . ObjectId , bson . ObjectId ( \"0123456789ab0123456789ab\" )) as_str ( bson . ObjectId ) assert deserialize ( bson . ObjectId , \"0123456789ab0123456789ab\" ) == bson . ObjectId ( \"0123456789ab0123456789ab\" ) assert ( serialize ( bson . ObjectId , bson . ObjectId ( \"0123456789ab0123456789ab\" )) == \"0123456789ab0123456789ab\" ) Note Previously mentioned standard types are handled by apischema using as_str . ValueErrorCatching \u00b6 Converters can be wrapped with apischema.conversions.catch_value_error in order to catch ValueError and reraise it as a ValidationError . It's notably used but as_str and other standard types. Note This wrapper is in fact inlined in deserialization, so it has better performance than writing the try-catch in the code. Use Enum names \u00b6 Enum subclasses are (de)serialized using values. However, you may want to use enumeration names instead, that's why apischema provides apischema.conversion.as_names to decorate Enum subclasses. from enum import Enum from apischema import deserialize , serialize from apischema.conversions import as_names from apischema.json_schema import deserialization_schema , serialization_schema @as_names class MyEnum ( Enum ): FOO = object () BAR = object () assert deserialize ( MyEnum , \"FOO\" ) == MyEnum . FOO assert serialize ( MyEnum , MyEnum . FOO ) == \"FOO\" assert ( deserialization_schema ( MyEnum ) == serialization_schema ( MyEnum ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"type\" : \"string\" , \"enum\" : [ \"FOO\" , \"BAR\" ], } ) Class as union of its subclasses \u00b6 Object deserialization \u2014 transform function into a dataclass deserializer \u00b6 apischema.objects.object_deserialization can convert a function into a new function taking a unique parameter, a dataclass whose fields are mapped from the original function parameters. It can be used for example to build a deserialization conversion from an alternative constructor. from apischema import deserialize , deserializer , type_name from apischema.json_schema import deserialization_schema from apischema.objects import object_deserialization def create_range ( start : int , stop : int , step : int = 1 ) -> range : return range ( start , stop , step ) range_conv = object_deserialization ( create_range , type_name ( \"Range\" )) # Conversion can be registered deserializer ( range_conv ) assert deserialize ( range , { \"start\" : 0 , \"stop\" : 10 }) == range ( 0 , 10 ) assert deserialization_schema ( range ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"type\" : \"object\" , \"properties\" : { \"start\" : { \"type\" : \"integer\" }, \"stop\" : { \"type\" : \"integer\" }, \"step\" : { \"type\" : \"integer\" , \"default\" : 1 }, }, \"required\" : [ \"start\" , \"stop\" ], \"additionalProperties\" : False , } Note Parameters metadata can be specified using typing.Annotated , or be passed with parameters_metadata parameter, which is a mapping of parameter names as key and mapped metadata as value. Object serialization \u2014 select only a subset of fields \u00b6 apischema.objects.object_serialization can be used to serialize only a subset of an object fields and methods. from dataclasses import dataclass from typing import Any from apischema import alias , serialize , type_name from apischema.json_schema import JsonSchemaVersion , definitions_schema from apischema.objects import get_field , object_serialization @dataclass class Data : id : int content : str @property def size ( self ) -> int : return len ( self . content ) def get_details ( self ) -> Any : ... # Serialization fields can be a str/field or a function/method/property size_only = object_serialization ( Data , [ get_field ( Data ) . id , Data . size ], type_name ( \"DataSize\" ) ) # [\"id\", Data.size] would also work def complete_data (): return [ ... , # shortcut to include all the fields Data . size , ( Data . get_details , alias ( \"details\" )), # add/override metadata using tuple ] # Serialization fields computation can be deferred in a function # The serialization name will then be defaulted to the function name complete = object_serialization ( Data , complete_data ) data = Data ( 0 , \"data\" ) assert serialize ( Data , data , conversion = size_only ) == { \"id\" : 0 , \"size\" : 4 } assert serialize ( Data , data , conversion = complete ) == { \"id\" : 0 , \"content\" : \"data\" , \"size\" : 4 , \"details\" : None , # because get_details return None in this example } assert definitions_schema ( serialization = [( Data , size_only ), ( Data , complete )], version = JsonSchemaVersion . OPEN_API_3_0 , ) == { \"DataSize\" : { \"type\" : \"object\" , \"properties\" : { \"id\" : { \"type\" : \"integer\" }, \"size\" : { \"type\" : \"integer\" }}, \"required\" : [ \"id\" , \"size\" ], \"additionalProperties\" : False , }, \"CompleteData\" : { \"type\" : \"object\" , \"properties\" : { \"id\" : { \"type\" : \"integer\" }, \"content\" : { \"type\" : \"string\" }, \"size\" : { \"type\" : \"integer\" }, \"details\" : {}, }, \"required\" : [ \"id\" , \"content\" , \"size\" , \"details\" ], \"additionalProperties\" : False , }, } FAQ \u00b6 What's the difference between conversion and default_conversion parameters? \u00b6 Dynamic conversions ( conversion parameter) exists to ensure consistency and reuse of subschemas referenced (with a $ref ) in the JSON/OpenAPI schema. In fact, different global conversions ( default_conversion parameter) could lead to having a field with different schemas depending on global conversions, so a class would not be able to be referenced consistently. Because dynamic conversions are local, they cannot mess with an object field schema. Schema generation uses the same default conversions for all definitions (which can have associated dynamic conversion). default_conversion parameter allows having different (de)serialization contexts, for example to map date to string between frontend and backend, and to timestamp between backend services.","title":"Conversions"},{"location":"conversions/#conversions-deserialization-customization","text":"apischema covers the majority of standard data types, but of course that's not enough, which is why it enables you to add support for all your classes and the libraries you use. Actually, apischema itself uses this conversion feature to provide a basic support for standard library data types like UUID/datetime/etc. (see std_types.py ) ORM support can easily be achieved with this feature (see SQLAlchemy example ). In fact, you can even add support for competitor libraries like Pydantic (see Pydantic compatibility example )","title":"Conversions \u2013 (de)serialization customization"},{"location":"conversions/#principle-apischema-conversions","text":"An apischema conversion is composed of a source type, let's call it Source , a target type Target and a converter function with signature (Source) -> Target . When a class (actually, a non-builtin class, so not int / list /etc.) is deserialized, apischema will check if there is a conversion where this type is the target. If found, the source type of conversion will be deserialized, then the converter will be applied to get an object of the expected type. Serialization works the same way but inverted: look for a conversion with type as source, apply then converter, and get the target type. Conversions are also handled in schema generation: for a deserialization schema, source schema is merged to target schema, while target schema is merged to source schema for a serialization schema.","title":"Principle - apischema conversions"},{"location":"conversions/#register-a-conversion","text":"Conversion is registered using apischema.deserializer / apischema.serializer for deserialization/serialization respectively. When used as function decorator, the Source / Target types are directly extracted from the conversion function signature. serializer can be called on methods/properties, in which case Source type is inferred to be the owning type. from dataclasses import dataclass from apischema import deserialize , schema , serialize from apischema.conversions import deserializer , serializer from apischema.json_schema import deserialization_schema , serialization_schema @schema ( pattern = r \"^#[0-9a-fA-F] {6} $\" ) @dataclass class RGB : red : int green : int blue : int @serializer @property def hexa ( self ) -> str : return f \"# { self . red : 02x }{ self . green : 02x }{ self . blue : 02x } \" # serializer can also be called with methods/properties outside of the class # For example, `serializer(RGB.hexa)` would have the same effect as the decorator above @deserializer def from_hexa ( hexa : str ) -> RGB : return RGB ( int ( hexa [ 1 : 3 ], 16 ), int ( hexa [ 3 : 5 ], 16 ), int ( hexa [ 5 : 7 ], 16 )) assert deserialize ( RGB , \"#000000\" ) == RGB ( 0 , 0 , 0 ) assert serialize ( RGB , RGB ( 0 , 0 , 42 )) == \"#00002a\" assert ( deserialization_schema ( RGB ) == serialization_schema ( RGB ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"type\" : \"string\" , \"pattern\" : \"^#[0-9a-fA-F] {6} $\" , } ) Warning (De)serializer methods cannot be used with typing.NamedTuple ; in fact, apischema uses the __set_name__ magic method but it is not called on NamedTuple subclass fields.","title":"Register a conversion"},{"location":"conversions/#multiple-deserializers","text":"Sometimes, you want to have several possibilities to deserialize a type. If it's possible to register a deserializer with a Union param, it's not very practical. That's why apischema make it possible to register several deserializers for the same type. They will be handled with a Union source type (ordered by deserializers registration), with the right serializer selected according to the matching alternative. from dataclasses import dataclass from apischema import deserialize , deserializer from apischema.json_schema import deserialization_schema @dataclass class Expression : value : int @deserializer def evaluate_expression ( expr : str ) -> Expression : return Expression ( int ( eval ( expr ))) # Could be shorten into deserializer(Expression), because class is callable too @deserializer def expression_from_value ( value : int ) -> Expression : return Expression ( value ) assert deserialization_schema ( Expression ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"type\" : [ \"string\" , \"integer\" ], } assert deserialize ( Expression , 0 ) == deserialize ( Expression , \"1 - 1\" ) == Expression ( 0 ) On the other hand, serializer registration overwrites the previous registration if any. apischema.conversions.reset_deserializers / apischema.conversions.reset_serializers can be used to reset (de)serializers (even those of the standard types embedded in apischema )","title":"Multiple deserializers"},{"location":"conversions/#inheritance","text":"All serializers are naturally inherited. In fact, with a conversion function (Source) -> Target , you can always pass a subtype of Source and get a Target in return. Moreover, when serializer is a method/property, overriding this method/property in a subclass will override the inherited serializer. from apischema import serialize , serializer class Foo : pass @serializer def serialize_foo ( foo : Foo ) -> int : return 0 class Foo2 ( Foo ): pass # Deserializer is inherited assert serialize ( Foo , Foo ()) == serialize ( Foo2 , Foo2 ()) == 0 class Bar : @serializer def serialize ( self ) -> int : return 0 class Bar2 ( Bar ): def serialize ( self ) -> int : return 1 # Deserializer is inherited and overridden assert serialize ( Bar , Bar ()) == 0 != serialize ( Bar2 , Bar2 ()) == 1 Note Inheritance can also be toggled off in specific cases, like in the Class as union of its subclasses example On the other hand, deserializers cannot be inherited, because the same Source passed to a conversion function (Source) -> Target will always give the same Target (not ensured to be the desired subtype). Note Pseudo-inheritance could be achieved by registering a conversion (using for example a classmethod ) for each subclass in __init_subclass__ method (or a metaclass), or by using __subclasses__ ; see example","title":"Inheritance"},{"location":"conversions/#generic-conversions","text":"Generic conversions are supported out of the box. from typing import Generic , TypeVar import pytest from apischema import ValidationError , deserialize , serialize from apischema.conversions import deserializer , serializer from apischema.json_schema import deserialization_schema , serialization_schema T = TypeVar ( \"T\" ) class Wrapper ( Generic [ T ]): def __init__ ( self , wrapped : T ): self . wrapped = wrapped @serializer def unwrap ( self ) -> T : return self . wrapped # Wrapper constructor can be used as a function too (so deserializer could work as decorator) deserializer ( Wrapper ) assert deserialize ( Wrapper [ list [ int ]], [ 0 , 1 ]) . wrapped == [ 0 , 1 ] with pytest . raises ( ValidationError ): deserialize ( Wrapper [ int ], \"wrapped\" ) assert serialize ( Wrapper [ str ], Wrapper ( \"wrapped\" )) == \"wrapped\" assert ( deserialization_schema ( Wrapper [ int ]) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"type\" : \"integer\" } == serialization_schema ( Wrapper [ int ]) ) However, you're not allowed to register a conversion of a specialized generic type, like Foo[int] .","title":"Generic conversions"},{"location":"conversions/#conversion-object","text":"In the previous example, conversions were registered using only converter functions. However, it can also be done by passing a apischema.conversions.Conversion instance. It allows specifying additional metadata to conversion (see next sections for examples) and precise converter source/target when annotations are not available. from base64 import b64decode from apischema import deserialize , deserializer from apischema.conversions import Conversion deserializer ( Conversion ( b64decode , source = str , target = bytes )) # Roughly equivalent to: # def decode_bytes(source: str) -> bytes: # return b64decode(source) # but saving a function call assert deserialize ( bytes , \"Zm9v\" ) == b \"foo\"","title":"Conversion object"},{"location":"conversions/#dynamic-conversions-select-conversions-at-runtime","text":"Whether or not a conversion is registered for a given type, conversions can also be provided at runtime, using the conversion parameter of deserialize / serialize / deserialization_schema / serialization_schema . import os import time from dataclasses import dataclass from datetime import datetime from typing import Annotated from apischema import deserialize , serialize from apischema.metadata import conversion # Set UTC timezone for example os . environ [ \"TZ\" ] = \"UTC\" time . tzset () def datetime_from_timestamp ( timestamp : int ) -> datetime : return datetime . fromtimestamp ( timestamp ) date = datetime ( 2017 , 9 , 2 ) assert deserialize ( datetime , 1504310400 , conversion = datetime_from_timestamp ) == date @dataclass class Foo : bar : int baz : int def sum ( self ) -> int : return self . bar + self . baz @property def diff ( self ) -> int : return int ( self . bar - self . baz ) assert serialize ( Foo , Foo ( 0 , 1 )) == { \"bar\" : 0 , \"baz\" : 1 } assert serialize ( Foo , Foo ( 0 , 1 ), conversion = Foo . sum ) == 1 assert serialize ( Foo , Foo ( 0 , 1 ), conversion = Foo . diff ) == - 1 # conversions can be specified using Annotated assert serialize ( Annotated [ Foo , conversion ( serialization = Foo . sum )], Foo ( 0 , 1 )) == 1 Note For definitions_schema , conversions can be added with types by using a tuple instead, for example definitions_schema(serializations=[(list[Foo], foo_to_bar)]) . The conversion parameter can also take a tuple of conversions, when you have a Union , a tuple or when you want to have several deserializations for the same type.","title":"Dynamic conversions \u2014 select conversions at runtime"},{"location":"conversions/#dynamic-conversions-are-local","text":"Dynamic conversions are discarded after having been applied (or after class without conversion having been encountered). For example, you can't apply directly a dynamic conversion to a dataclass field when calling serialize on an instance of this dataclass. Reasons for this design are detailed in the FAQ . import os import time from dataclasses import dataclass from datetime import datetime from apischema import serialize # Set UTC timezone for example os . environ [ \"TZ\" ] = \"UTC\" time . tzset () def to_timestamp ( d : datetime ) -> int : return int ( d . timestamp ()) @dataclass class Foo : bar : datetime # timestamp conversion is not applied on Foo field because it's discarded # when encountering Foo assert serialize ( Foo , Foo ( datetime ( 2019 , 10 , 13 )), conversion = to_timestamp ) == { \"bar\" : \"2019-10-13T00:00:00\" } # timestamp conversion is applied on every member of list assert serialize ( list [ datetime ], [ datetime ( 1970 , 1 , 1 )], conversion = to_timestamp ) == [ 0 ] Note Dynamic conversion is not discarded when the encountered type is a container ( list , dict , Collection , etc. or Union ) or a registered conversion from/to a container; the dynamic conversion can then apply to the container elements","title":"Dynamic conversions are local"},{"location":"conversions/#dynamic-conversions-interact-with-type_name","text":"Dynamic conversions are applied before looking for a ref registered with type_name from dataclasses import dataclass from apischema import type_name from apischema.json_schema import serialization_schema @dataclass class Foo : pass @dataclass class Bar : pass def foo_to_bar ( _ : Foo ) -> Bar : return Bar () type_name ( \"Bars\" )( list [ Bar ]) assert serialization_schema ( list [ Foo ], conversion = foo_to_bar , all_refs = True ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"$ref\" : \"#/$defs/Bars\" , \"$defs\" : { # Bars is present because `list[Foo]` is dynamically converted to `list[Bar]` \"Bars\" : { \"type\" : \"array\" , \"items\" : { \"$ref\" : \"#/$defs/Bar\" }}, \"Bar\" : { \"type\" : \"object\" , \"additionalProperties\" : False }, }, }","title":"Dynamic conversions interact with type_name"},{"location":"conversions/#bypass-registered-conversion","text":"Using apischema.identity as a dynamic conversion allows you to bypass a registered conversion, i.e. to (de)serialize the given type as it would be without conversion registered. from dataclasses import dataclass from apischema import identity , serialize , serializer from apischema.conversions import Conversion @dataclass class RGB : red : int green : int blue : int @serializer @property def hexa ( self ) -> str : return f \"# { self . red : 02x }{ self . green : 02x }{ self . blue : 02x } \" assert serialize ( RGB , RGB ( 0 , 0 , 0 )) == \"#000000\" # dynamic conversion used to bypass the registered one assert serialize ( RGB , RGB ( 0 , 0 , 0 ), conversion = identity ) == { \"red\" : 0 , \"green\" : 0 , \"blue\" : 0 , } # Expended bypass form assert serialize ( RGB , RGB ( 0 , 0 , 0 ), conversion = Conversion ( identity , source = RGB , target = RGB ) ) == { \"red\" : 0 , \"green\" : 0 , \"blue\" : 0 } Note For a more precise selection of bypassed conversion, for tuple or Union member for example, it's possible to pass the concerned class as the source and the target of conversion with identity converter, as shown in the example.","title":"Bypass registered conversion"},{"location":"conversions/#liskov-substitution-principle","text":"LSP is taken into account when applying dynamic conversion: the serializer source can be a subclass of the actual class and the deserializer target can be a superclass of the actual class. from dataclasses import dataclass from apischema import deserialize , serialize @dataclass class Foo : field : int @dataclass class Bar ( Foo ): other : str def foo_to_int ( foo : Foo ) -> int : return foo . field def bar_from_int ( i : int ) -> Bar : return Bar ( i , str ( i )) assert serialize ( Bar , Bar ( 0 , \"\" ), conversion = foo_to_int ) == 0 assert deserialize ( Foo , 0 , conversion = bar_from_int ) == Bar ( 0 , \"0\" )","title":"Liskov substitution principle"},{"location":"conversions/#generic-dynamic-conversions","text":"Generic dynamic conversions are supported out of the box. Also, contrary to registered conversions, partially specialized generics are allowed. from collections.abc import Mapping , Sequence from operator import itemgetter from typing import TypeVar from apischema import serialize from apischema.json_schema import serialization_schema T = TypeVar ( \"T\" ) Priority = int def sort_by_priority ( values_with_priority : Mapping [ T , Priority ]) -> Sequence [ T ]: return [ k for k , _ in sorted ( values_with_priority . items (), key = itemgetter ( 1 ))] assert serialize ( dict [ str , Priority ], { \"a\" : 1 , \"b\" : 0 }, conversion = sort_by_priority ) == [ \"b\" , \"a\" ] assert serialization_schema ( dict [ str , Priority ], conversion = sort_by_priority ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" }, }","title":"Generic dynamic conversions"},{"location":"conversions/#field-conversions","text":"It is possible to register a conversion for a particular dataclass field using conversion metadata. import os import time from dataclasses import dataclass , field from datetime import datetime from apischema import deserialize , serialize from apischema.conversions import Conversion from apischema.metadata import conversion # Set UTC timezone for example os . environ [ \"TZ\" ] = \"UTC\" time . tzset () from_timestamp = Conversion ( datetime . fromtimestamp , source = int , target = datetime ) def to_timestamp ( d : datetime ) -> int : return int ( d . timestamp ()) @dataclass class Foo : some_date : datetime = field ( metadata = conversion ( from_timestamp , to_timestamp )) other_date : datetime assert deserialize ( Foo , { \"some_date\" : 0 , \"other_date\" : \"2019-10-13\" }) == Foo ( datetime ( 1970 , 1 , 1 ), datetime ( 2019 , 10 , 13 ) ) assert serialize ( Foo , Foo ( datetime ( 1970 , 1 , 1 ), datetime ( 2019 , 10 , 13 ))) == { \"some_date\" : 0 , \"other_date\" : \"2019-10-13T00:00:00\" , } Note It's possible to pass a conversion only for deserialization or only for serialization","title":"Field conversions"},{"location":"conversions/#serialized-method-conversions","text":"Serialized methods can also have dedicated conversions for their return import os import time from dataclasses import dataclass from datetime import datetime from apischema import serialize , serialized # Set UTC timezone for example os . environ [ \"TZ\" ] = \"UTC\" time . tzset () def to_timestamp ( d : datetime ) -> int : return int ( d . timestamp ()) @dataclass class Foo : @serialized ( conversion = to_timestamp ) def some_date ( self ) -> datetime : return datetime ( 1970 , 1 , 1 ) assert serialize ( Foo , Foo ()) == { \"some_date\" : 0 }","title":"Serialized method conversions"},{"location":"conversions/#default-conversions","text":"As with almost every default behavior in apischema , default conversions can be configured using apischema.settings.deserialization.default_conversion / apischema.settings.serialization.default_conversion . The initial value of these settings are the function which retrieved conversions registered with deserializer / serializer . You can for example support attrs classes with this feature: from typing import Sequence import attrs from apischema import deserialize , serialize , settings from apischema.json_schema import deserialization_schema from apischema.objects import ObjectField prev_default_object_fields = settings . default_object_fields def attrs_fields ( cls : type ) -> Sequence [ ObjectField ] | None : if hasattr ( cls , \"__attrs_attrs__\" ): return [ ObjectField ( a . name , a . type , required = a . default == attrs . NOTHING , default = a . default ) for a in getattr ( cls , \"__attrs_attrs__\" ) ] else : return prev_default_object_fields ( cls ) settings . default_object_fields = attrs_fields @attrs . define class Foo : bar : int assert deserialize ( Foo , { \"bar\" : 0 }) == Foo ( 0 ) assert serialize ( Foo , Foo ( 0 )) == { \"bar\" : 0 } assert deserialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : \"integer\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , } apischema functions ( deserialize / serialize / deserialization_schema / serialization_schema / definitions_schema ) also have a default_conversion parameter to dynamically modify default conversions. See FAQ for the difference between conversion and default_conversion parameters.","title":"Default conversions"},{"location":"conversions/#sub-conversions","text":"Sub-conversions are dynamic conversions applied on the result of a conversion. from dataclasses import dataclass from typing import Generic , TypeVar from apischema.conversions import Conversion from apischema.json_schema import serialization_schema T = TypeVar ( \"T\" ) class Query ( Generic [ T ]): ... def query_to_list ( q : Query [ T ]) -> list [ T ]: ... def query_to_scalar ( q : Query [ T ]) -> T | None : ... @dataclass class FooModel : bar : int class Foo : def serialize ( self ) -> FooModel : ... assert serialization_schema ( Query [ Foo ], conversion = Conversion ( query_to_list , sub_conversion = Foo . serialize ) ) == { # We get an array of Foo \"type\" : \"array\" , \"items\" : { \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : \"integer\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , }, \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , } Sub-conversions can also be used to bypass registered conversions or to define recursive conversions .","title":"Sub-conversions"},{"location":"conversions/#lazyrecursive-conversions","text":"Conversions can be defined lazily, i.e. using a function returning Conversion (single, or a tuple of it); this function must be wrapped into a apischema.conversions.LazyConversion instance. It allows creating recursive conversions or using a conversion object which can be modified after its definition (for example a conversion for a base class modified by __init_subclass__ ) It is used by apischema itself for the generated JSON schema. It is indeed a recursive data, and the different versions are handled by a conversion with a lazy recursive sub-conversion. from dataclasses import dataclass from apischema import serialize from apischema.conversions import Conversion , LazyConversion @dataclass class Foo : elements : list [ \"int | Foo\" ] def foo_elements ( foo : Foo ) -> list [ int | Foo ]: return foo . elements # Recursive conversion pattern tmp = None conversion = Conversion ( foo_elements , sub_conversion = LazyConversion ( lambda : tmp )) tmp = conversion assert serialize ( Foo , Foo ([ 0 , Foo ([ 1 ])]), conversion = conversion ) == [ 0 , [ 1 ]] # Without the recursive sub-conversion, it would have been: assert serialize ( Foo , Foo ([ 0 , Foo ([ 1 ])]), conversion = foo_elements ) == [ 0 , { \"elements\" : [ 1 ]}, ]","title":"Lazy/recursive conversions"},{"location":"conversions/#lazy-registered-conversions","text":"Lazy conversions can also be registered, but the deserialization target/serialization source has to be passed too. from dataclasses import dataclass from apischema import deserialize , deserializer , serialize , serializer from apischema.conversions import Conversion @dataclass class Foo : bar : int deserializer ( lazy = lambda : Conversion ( lambda bar : Foo ( bar ), source = int , target = Foo ), target = Foo ) serializer ( lazy = lambda : Conversion ( lambda foo : foo . bar , source = Foo , target = int ), source = Foo ) assert deserialize ( Foo , 0 ) == Foo ( 0 ) assert serialize ( Foo , Foo ( 0 )) == 0","title":"Lazy registered conversions"},{"location":"conversions/#conversion-helpers","text":"","title":"Conversion helpers"},{"location":"conversions/#string-conversions","text":"A common pattern of conversion concerns classes that have a string constructor and a __str__ method, for example standard types uuid.UUID , pathlib.Path , or ipaddress.IPv4Address . Using apischema.conversions.as_str will register a string-deserializer from the constructor and a string-serializer from the __str__ method. ValueError raised by the constructor is caught and converted to ValidationError . import bson import pytest from apischema import Unsupported , deserialize , serialize from apischema.conversions import as_str with pytest . raises ( Unsupported ): deserialize ( bson . ObjectId , \"0123456789ab0123456789ab\" ) with pytest . raises ( Unsupported ): serialize ( bson . ObjectId , bson . ObjectId ( \"0123456789ab0123456789ab\" )) as_str ( bson . ObjectId ) assert deserialize ( bson . ObjectId , \"0123456789ab0123456789ab\" ) == bson . ObjectId ( \"0123456789ab0123456789ab\" ) assert ( serialize ( bson . ObjectId , bson . ObjectId ( \"0123456789ab0123456789ab\" )) == \"0123456789ab0123456789ab\" ) Note Previously mentioned standard types are handled by apischema using as_str .","title":"String conversions"},{"location":"conversions/#valueerrorcatching","text":"Converters can be wrapped with apischema.conversions.catch_value_error in order to catch ValueError and reraise it as a ValidationError . It's notably used but as_str and other standard types. Note This wrapper is in fact inlined in deserialization, so it has better performance than writing the try-catch in the code.","title":"ValueErrorCatching"},{"location":"conversions/#use-enum-names","text":"Enum subclasses are (de)serialized using values. However, you may want to use enumeration names instead, that's why apischema provides apischema.conversion.as_names to decorate Enum subclasses. from enum import Enum from apischema import deserialize , serialize from apischema.conversions import as_names from apischema.json_schema import deserialization_schema , serialization_schema @as_names class MyEnum ( Enum ): FOO = object () BAR = object () assert deserialize ( MyEnum , \"FOO\" ) == MyEnum . FOO assert serialize ( MyEnum , MyEnum . FOO ) == \"FOO\" assert ( deserialization_schema ( MyEnum ) == serialization_schema ( MyEnum ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"type\" : \"string\" , \"enum\" : [ \"FOO\" , \"BAR\" ], } )","title":"Use Enum names"},{"location":"conversions/#class-as-union-of-its-subclasses","text":"","title":"Class as union of its subclasses"},{"location":"conversions/#object-deserialization-transform-function-into-a-dataclass-deserializer","text":"apischema.objects.object_deserialization can convert a function into a new function taking a unique parameter, a dataclass whose fields are mapped from the original function parameters. It can be used for example to build a deserialization conversion from an alternative constructor. from apischema import deserialize , deserializer , type_name from apischema.json_schema import deserialization_schema from apischema.objects import object_deserialization def create_range ( start : int , stop : int , step : int = 1 ) -> range : return range ( start , stop , step ) range_conv = object_deserialization ( create_range , type_name ( \"Range\" )) # Conversion can be registered deserializer ( range_conv ) assert deserialize ( range , { \"start\" : 0 , \"stop\" : 10 }) == range ( 0 , 10 ) assert deserialization_schema ( range ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"type\" : \"object\" , \"properties\" : { \"start\" : { \"type\" : \"integer\" }, \"stop\" : { \"type\" : \"integer\" }, \"step\" : { \"type\" : \"integer\" , \"default\" : 1 }, }, \"required\" : [ \"start\" , \"stop\" ], \"additionalProperties\" : False , } Note Parameters metadata can be specified using typing.Annotated , or be passed with parameters_metadata parameter, which is a mapping of parameter names as key and mapped metadata as value.","title":"Object deserialization \u2014 transform function into a dataclass deserializer"},{"location":"conversions/#object-serialization-select-only-a-subset-of-fields","text":"apischema.objects.object_serialization can be used to serialize only a subset of an object fields and methods. from dataclasses import dataclass from typing import Any from apischema import alias , serialize , type_name from apischema.json_schema import JsonSchemaVersion , definitions_schema from apischema.objects import get_field , object_serialization @dataclass class Data : id : int content : str @property def size ( self ) -> int : return len ( self . content ) def get_details ( self ) -> Any : ... # Serialization fields can be a str/field or a function/method/property size_only = object_serialization ( Data , [ get_field ( Data ) . id , Data . size ], type_name ( \"DataSize\" ) ) # [\"id\", Data.size] would also work def complete_data (): return [ ... , # shortcut to include all the fields Data . size , ( Data . get_details , alias ( \"details\" )), # add/override metadata using tuple ] # Serialization fields computation can be deferred in a function # The serialization name will then be defaulted to the function name complete = object_serialization ( Data , complete_data ) data = Data ( 0 , \"data\" ) assert serialize ( Data , data , conversion = size_only ) == { \"id\" : 0 , \"size\" : 4 } assert serialize ( Data , data , conversion = complete ) == { \"id\" : 0 , \"content\" : \"data\" , \"size\" : 4 , \"details\" : None , # because get_details return None in this example } assert definitions_schema ( serialization = [( Data , size_only ), ( Data , complete )], version = JsonSchemaVersion . OPEN_API_3_0 , ) == { \"DataSize\" : { \"type\" : \"object\" , \"properties\" : { \"id\" : { \"type\" : \"integer\" }, \"size\" : { \"type\" : \"integer\" }}, \"required\" : [ \"id\" , \"size\" ], \"additionalProperties\" : False , }, \"CompleteData\" : { \"type\" : \"object\" , \"properties\" : { \"id\" : { \"type\" : \"integer\" }, \"content\" : { \"type\" : \"string\" }, \"size\" : { \"type\" : \"integer\" }, \"details\" : {}, }, \"required\" : [ \"id\" , \"content\" , \"size\" , \"details\" ], \"additionalProperties\" : False , }, }","title":"Object serialization \u2014 select only a subset of fields"},{"location":"conversions/#faq","text":"","title":"FAQ"},{"location":"conversions/#whats-the-difference-between-conversion-and-default_conversion-parameters","text":"Dynamic conversions ( conversion parameter) exists to ensure consistency and reuse of subschemas referenced (with a $ref ) in the JSON/OpenAPI schema. In fact, different global conversions ( default_conversion parameter) could lead to having a field with different schemas depending on global conversions, so a class would not be able to be referenced consistently. Because dynamic conversions are local, they cannot mess with an object field schema. Schema generation uses the same default conversions for all definitions (which can have associated dynamic conversion). default_conversion parameter allows having different (de)serialization contexts, for example to map date to string between frontend and backend, and to timestamp between backend services.","title":"What's the difference between conversion and default_conversion parameters?"},{"location":"data_model/","text":"Data model \u00b6 apischema handles every class/type you need. By the way, it's done in an additive way, meaning that it doesn't affect your types. PEP 585 \u00b6 With Python 3.9 and PEP 585 , typing is substantially shaken up; all container types of typing module are now deprecated. apischema fully support 3.9 and PEP 585, as shown in the different examples. However, typing containers can still be used, especially/necessarily when using an older version. Dataclasses \u00b6 Because the library aims to bring the minimum boilerplate, it's built on the top of standard library. Dataclasses are thus the core structure of the data model. Dataclasses bring the possibility of field customization, with more than just a default value. In addition to the common parameters of dataclasses.field , customization is done with the metadata parameter; metadata can also be passed using PEP 593 typing.Annotated . With some teasing of features presented later: from dataclasses import dataclass , field from typing import Annotated from apischema import alias , schema from apischema.metadata import required @dataclass class Foo : bar : int = field ( default = 0 , metadata = alias ( \"foo_bar\" ) | schema ( title = \"foo! bar!\" , min = 0 , max = 42 ) | required , ) baz : Annotated [ int , alias ( \"foo_baz\" ), schema ( title = \"foo! baz!\" , min = 0 , max = 32 ), required ] = 0 # pipe `|` operator can also be used in Annotated Note Field's metadata are just an ordinary dict ; apischema provides some functions to enrich these metadata with its own keys ( alias(\"foo_bar\") is roughly equivalent to `{\"_apischema_alias\": \"foo_bar\"}) and use them when the time comes, but metadata are not reserved to apischema and other keys can be added. Because PEP 584 is painfully missing before Python 3.9, apischema metadata use their own subclass of dict just to add | operator for convenience in all Python versions. Dataclasses __post_init__ and field(init=False) are fully supported. Implications of this feature usage are documented in the relative sections. Warning Before 3.8, InitVar is doing type erasure , which is why it's not possible for apischema to retrieve type information of init variables. To fix this behavior, a field metadata init_var can be used to put back the type of the field ( init_var also accepts stringified type annotations). Dataclass-like types ( attrs / SQLAlchemy /etc.) can also be supported with a few lines of code, see next section Standard library types \u00b6 apischema natively handles most of the types provided by the standard library. They are sorted in the following categories: Primitive \u00b6 str , int , float , bool , None , subclasses of them They correspond to JSON primitive types. Collection \u00b6 collection.abc.Collection ( typing.Collection ) collection.abc.Sequence ( typing.Sequence ) tuple ( typing.Tuple ) collection.abc.MutableSequence ( typing.MutableSequence ) list ( typing.List ) collection.abc.Set ( typing.AbstractSet ) collection.abc.MutableSet ( typing.MutableSet ) frozenset ( typing.FrozenSet ) set ( typing.Set ) They correspond to JSON array and are serialized to list . Mapping \u00b6 collection.abc.Mapping ( typing.Mapping ) collection.abc.MutableMapping ( typing.MutableMapping ) dict ( typing.Dict ) They correpond to JSON object and are serialized to dict . Enumeration \u00b6 enum.Enum subclasses, typing.Literal Warning Enum subclasses are (de)serialized using values , not names. apischema also provides a conversion to use names instead. Typing facilities \u00b6 typing.Optional / typing.Union ( Optional[T] is strictly equivalent to Union[T, None] ) : Deserialization select the first matching alternative; unsupported alternatives are ignored tuple ( typing.Tuple ) : Can be used as collection as well as true tuple, like tuple[str, int] typing.NewType : Serialized according to its base type typing.NamedTuple : Handled as an object type, roughly like a dataclass; fields metadata can be passed using Annotated typing.TypedDict : Handled as an object type, but with a dictionary shape; fields metadata can be passed using Annotated typing.Any : Untouched by deserialization, serialized according to the object runtime class typing.LiteralString : Handled as str Other standard library types \u00b6 bytes : with str (de)serialization using base64 encoding datetime.datetime datetime.date datetime.time : Supported only in 3.7+ with fromisoformat / isoformat Decimal : With float (de)serialization ipaddress.IPv4Address ipaddress.IPv4Interface ipaddress.IPv4Network ipaddress.IPv6Address ipaddress.IPv6Interface ipaddress.IPv6Network pathlib.Path re.Pattern ( typing.Pattern ) uuid.UUID : With str (de)serialization Generic \u00b6 typing.Generic can be used out of the box like in the following example: from dataclasses import dataclass from typing import Generic , TypeVar import pytest from apischema import ValidationError , deserialize T = TypeVar ( \"T\" ) @dataclass class Box ( Generic [ T ]): content : T assert deserialize ( Box [ str ], { \"content\" : \"void\" }) == Box ( \"void\" ) with pytest . raises ( ValidationError ): deserialize ( Box [ str ], { \"content\" : 42 }) Warning Generic types don't have default type name (used in JSON/GraphQL schema) \u2014 should Group[Foo] be named GroupFoo / FooGroup /something else? \u2014 so they require by-class or default type_name assignment . Recursive types, string annotations and PEP 563 \u00b6 Recursive classes can be typed as they usually do, with or without PEP 563 . Here with string annotations: from dataclasses import dataclass from typing import Optional from apischema import deserialize @dataclass class Node : value : int child : Optional [ \"Node\" ] = None assert deserialize ( Node , { \"value\" : 0 , \"child\" : { \"value\" : 1 }}) == Node ( 0 , Node ( 1 )) Here with PEP 563 (requires 3.7+) from __future__ import annotations from dataclasses import dataclass from apischema import deserialize @dataclass class Node : value : int child : Node | None = None assert deserialize ( Node , { \"value\" : 0 , \"child\" : { \"value\" : 1 }}) == Node ( 0 , Node ( 1 )) Warning To resolve annotations, apischema uses typing.get_type_hints ; this doesn't work really well when used on objects defined outside of global scope. Warning (minor) Currently, PEP 585 can have surprising behavior when used outside the box, see bpo-41370 null vs. undefined \u00b6 Contrary to Javascript, Python doesn't have an undefined equivalent (if we consider None to be the equivalent of null ). But it can be useful to distinguish (especially when thinking about HTTP PATCH method) between a null field and an undefined /absent field. That's why apischema provides an Undefined constant (a single instance of UndefinedType class) which can be used as a default value everywhere where this distinction is needed. In fact, default values are used when field are absent, thus a default Undefined will mark the field as absent. Dataclass/ NamedTuple fields are ignored by serialization when Undefined . from dataclasses import dataclass from apischema import Undefined , UndefinedType , deserialize , serialize from apischema.json_schema import deserialization_schema @dataclass class Foo : bar : int | UndefinedType = Undefined baz : int | UndefinedType | None = Undefined assert deserialize ( Foo , { \"bar\" : 0 , \"baz\" : None }) == Foo ( 0 , None ) assert deserialize ( Foo , {}) == Foo ( Undefined , Undefined ) assert serialize ( Foo , Foo ( Undefined , 42 )) == { \"baz\" : 42 } # Foo.bar and Foo.baz are not required assert deserialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : \"integer\" }, \"baz\" : { \"type\" : [ \"integer\" , \"null\" ]}}, \"additionalProperties\" : False , } Note UndefinedType must only be used inside an Union , as it has no sense as a standalone type. By the way, no suitable name was found to shorten Union[T, UndefinedType] but propositions are welcomed. Note Undefined is a falsy constant, i.e. bool(Undefined) is False . Use None as if it was Undefined \u00b6 Using None can be more convenient than Undefined as a placeholder for missing value, but Optional types are translated to nullable fields. That's why apischema provides none_as_undefined metadata, allowing None to be handled as if it was Undefined : type will not be nullable and field not serialized if its value is None . from dataclasses import dataclass , field import pytest from apischema import ValidationError , deserialize , serialize from apischema.json_schema import deserialization_schema , serialization_schema from apischema.metadata import none_as_undefined @dataclass class Foo : bar : str | None = field ( default = None , metadata = none_as_undefined ) assert ( deserialization_schema ( Foo ) == serialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : \"string\" }}, \"additionalProperties\" : False , } ) with pytest . raises ( ValidationError ): deserialize ( Foo , { \"bar\" : None }) assert serialize ( Foo , Foo ( None )) == {} Annotated - PEP 593 \u00b6 PEP 593 is fully supported; annotations stranger to apischema are simply ignored. Custom types \u00b6 apischema can support almost all of your custom types in a few lines of code, using the conversion feature . However, it also provides a simple and direct way to support dataclass-like types, as presented below . Otherwise, when apischema encounters a type that it doesn't support, apischema.Unsupported exception will be raised. Note In the rare case when a union member should be ignored by apischema, it's possible to use mark it as unsupported using Union[Foo, Annotated[Bar, Unsupported]] . Dataclass-like types, aka object types \u00b6 Internally, apischema handle standard object types \u2014 dataclasses, named tuple and typed dictionary \u2014 the same way by mapping them to a set of apischema.objects.ObjectField , which has the following definition: @dataclass ( frozen = True ) class ObjectField : name : str # field's name type : Any # field's type required : bool = True # if the field is required metadata : Mapping [ str , Any ] = field ( default_factory = dict ) # field's metadata default : InitVar [ Any ] = ... # field's default value default_factory : Optional [ Callable [[], Any ]] = None # field's default factory kind : FieldKind = FieldKind . NORMAL # NORMAL/READ_ONLY/WRITE_ONLY Thus, support of dataclass-like types ( attrs , SQLAlchemy traditional mappers, etc.) can be achieved by mapping the concerned class to its own list of ObjectField s; this is done using apischema.objects.set_object_fields . from apischema import deserialize , serialize from apischema.json_schema import deserialization_schema from apischema.objects import ObjectField , set_object_fields class Foo : def __init__ ( self , bar ): self . bar = bar set_object_fields ( Foo , [ ObjectField ( \"bar\" , int )]) # Fields can also be passed in a factory set_object_fields ( Foo , lambda : [ ObjectField ( \"bar\" , int )]) foo = deserialize ( Foo , { \"bar\" : 0 }) assert isinstance ( foo , Foo ) and foo . bar == 0 assert serialize ( Foo , Foo ( 0 )) == { \"bar\" : 0 } assert deserialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : \"integer\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , } Another way to set object fields is to directly modify apischema default behavior, using apischema.settings.default_object_fields . Note set_object_fields / settings.default_object_fields can be used to override existing fields. Current fields can be retrieved using apischema.objects.object_fields . from collections.abc import Sequence from typing import Optional from apischema import settings from apischema.objects import ObjectField previous_default_object_fields = settings . default_object_field def default_object_fields ( cls ) -> Optional [ Sequence [ ObjectField ]]: return [ ... ] if ... else previous_default_object_fields ( cls ) settings . default_object_fields = default_object_fields Note Almost every default behavior of apischema can be customized using apischema.settings . Examples of SQLAlchemy support and attrs support illustrate both methods (which could also be combined). Skip field \u00b6 Dataclass fields can be excluded from apischema processing by using apischema.metadata.skip in the field metadata. It can be parametrized with deserialization / serialization boolean parameters to skip a field only for the given operations. from dataclasses import dataclass , field from typing import Any from apischema.json_schema import deserialization_schema , serialization_schema from apischema.metadata import skip @dataclass class Foo : bar : Any deserialization_only : Any = field ( metadata = skip ( serialization = True )) serialization_only : Any = field ( default = None , metadata = skip ( deserialization = True )) baz : Any = field ( default = None , metadata = skip ) assert deserialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"type\" : \"object\" , \"properties\" : { \"bar\" : {}, \"deserialization_only\" : {}}, \"required\" : [ \"bar\" , \"deserialization_only\" ], \"additionalProperties\" : False , } assert serialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"type\" : \"object\" , \"properties\" : { \"bar\" : {}, \"serialization_only\" : {}}, \"required\" : [ \"bar\" , \"serialization_only\" ], \"additionalProperties\" : False , } Note Fields skipped in deserialization should have a default value if deserialized, because deserialization of the class could raise otherwise. Skip field serialization depending on condition \u00b6 Field can also be skipped when serializing, depending on the condition given by serialization_if , or when the field value is equal to its default value with serialization_default=True . from dataclasses import dataclass , field from typing import Any from apischema import serialize from apischema.metadata import skip @dataclass class Foo : bar : Any = field ( metadata = skip ( serialization_if = lambda x : not x )) baz : Any = field ( default_factory = list , metadata = skip ( serialization_default = True )) assert serialize ( Foo ( False , [])) == {} Composition over inheritance - composed dataclasses flattening \u00b6 Dataclass fields which are themselves dataclass can be \"flattened\" into the owning one by using flatten metadata. Then, when the class is (de)serialized, \"flattened\" fields will be (de)serialized at the same level as the owning class. from dataclasses import dataclass , field from apischema import Undefined , UndefinedType , alias , deserialize , serialize from apischema.fields import with_fields_set from apischema.json_schema import deserialization_schema from apischema.metadata import flatten @dataclass class JsonSchema : title : str | UndefinedType = Undefined description : str | UndefinedType = Undefined format : str | UndefinedType = Undefined ... @with_fields_set @dataclass class RootJsonSchema : schema : str | UndefinedType = field ( default = Undefined , metadata = alias ( \"$schema\" )) defs : list [ JsonSchema ] = field ( default_factory = list , metadata = alias ( \"$defs\" )) # This field schema is flattened inside the owning one json_schema : JsonSchema = field ( default_factory = JsonSchema , metadata = flatten ) data = { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"title\" : \"flattened example\" , } root_schema = RootJsonSchema ( schema = \"http://json-schema.org/draft/2020-12/schema#\" , json_schema = JsonSchema ( title = \"flattened example\" ), ) assert deserialize ( RootJsonSchema , data ) == root_schema assert serialize ( RootJsonSchema , root_schema ) == data assert deserialization_schema ( RootJsonSchema ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"$defs\" : { \"JsonSchema\" : { \"type\" : \"object\" , \"properties\" : { \"title\" : { \"type\" : \"string\" }, \"description\" : { \"type\" : \"string\" }, \"format\" : { \"type\" : \"string\" }, }, \"additionalProperties\" : False , } }, # It results in allOf + unevaluatedProperties=False \"allOf\" : [ # RootJsonSchema (without JsonSchema) { \"type\" : \"object\" , \"properties\" : { \"$schema\" : { \"type\" : \"string\" }, \"$defs\" : { \"type\" : \"array\" , \"items\" : { \"$ref\" : \"#/$defs/JsonSchema\" }, \"default\" : [], }, }, \"additionalProperties\" : False , }, # JonsSchema { \"$ref\" : \"#/$defs/JsonSchema\" }, ], \"unevaluatedProperties\" : False , } Note Generated JSON schema use unevaluatedProperties keyword . This feature is very convenient for building model by composing smaller components. If some kind of reuse could also be achieved with inheritance, it can be less practical when it comes to use it in code, because there is no easy way to build an inherited class when you have an instance of the super class; you have to copy all the fields by hand. On the other hand, using composition (of flattened fields), it's easy to instantiate the class when the smaller component is just a field of it. FAQ \u00b6 Why isn't Iterable handled with other collection types? \u00b6 Iterable could be handled (actually, it was at the beginning), however, this doesn't really make sense from a data point of view. Iterables are computation objects, they can be infinite, etc. They don't correspond to a serialized data; Collection is way more appropriate in this context. What happens if I override dataclass __init__ ? \u00b6 apischema always assumes that dataclass __init__ can be called with all its fields as kwargs parameters. If that's no longer the case after a modification of __init__ (what means if an exception is thrown when the constructor is called because of bad parameters), apischema treats then the class as not supported .","title":"Data model"},{"location":"data_model/#data-model","text":"apischema handles every class/type you need. By the way, it's done in an additive way, meaning that it doesn't affect your types.","title":"Data model"},{"location":"data_model/#pep-585","text":"With Python 3.9 and PEP 585 , typing is substantially shaken up; all container types of typing module are now deprecated. apischema fully support 3.9 and PEP 585, as shown in the different examples. However, typing containers can still be used, especially/necessarily when using an older version.","title":"PEP 585"},{"location":"data_model/#dataclasses","text":"Because the library aims to bring the minimum boilerplate, it's built on the top of standard library. Dataclasses are thus the core structure of the data model. Dataclasses bring the possibility of field customization, with more than just a default value. In addition to the common parameters of dataclasses.field , customization is done with the metadata parameter; metadata can also be passed using PEP 593 typing.Annotated . With some teasing of features presented later: from dataclasses import dataclass , field from typing import Annotated from apischema import alias , schema from apischema.metadata import required @dataclass class Foo : bar : int = field ( default = 0 , metadata = alias ( \"foo_bar\" ) | schema ( title = \"foo! bar!\" , min = 0 , max = 42 ) | required , ) baz : Annotated [ int , alias ( \"foo_baz\" ), schema ( title = \"foo! baz!\" , min = 0 , max = 32 ), required ] = 0 # pipe `|` operator can also be used in Annotated Note Field's metadata are just an ordinary dict ; apischema provides some functions to enrich these metadata with its own keys ( alias(\"foo_bar\") is roughly equivalent to `{\"_apischema_alias\": \"foo_bar\"}) and use them when the time comes, but metadata are not reserved to apischema and other keys can be added. Because PEP 584 is painfully missing before Python 3.9, apischema metadata use their own subclass of dict just to add | operator for convenience in all Python versions. Dataclasses __post_init__ and field(init=False) are fully supported. Implications of this feature usage are documented in the relative sections. Warning Before 3.8, InitVar is doing type erasure , which is why it's not possible for apischema to retrieve type information of init variables. To fix this behavior, a field metadata init_var can be used to put back the type of the field ( init_var also accepts stringified type annotations). Dataclass-like types ( attrs / SQLAlchemy /etc.) can also be supported with a few lines of code, see next section","title":"Dataclasses"},{"location":"data_model/#standard-library-types","text":"apischema natively handles most of the types provided by the standard library. They are sorted in the following categories:","title":"Standard library types"},{"location":"data_model/#primitive","text":"str , int , float , bool , None , subclasses of them They correspond to JSON primitive types.","title":"Primitive"},{"location":"data_model/#collection","text":"collection.abc.Collection ( typing.Collection ) collection.abc.Sequence ( typing.Sequence ) tuple ( typing.Tuple ) collection.abc.MutableSequence ( typing.MutableSequence ) list ( typing.List ) collection.abc.Set ( typing.AbstractSet ) collection.abc.MutableSet ( typing.MutableSet ) frozenset ( typing.FrozenSet ) set ( typing.Set ) They correspond to JSON array and are serialized to list .","title":"Collection"},{"location":"data_model/#mapping","text":"collection.abc.Mapping ( typing.Mapping ) collection.abc.MutableMapping ( typing.MutableMapping ) dict ( typing.Dict ) They correpond to JSON object and are serialized to dict .","title":"Mapping"},{"location":"data_model/#enumeration","text":"enum.Enum subclasses, typing.Literal Warning Enum subclasses are (de)serialized using values , not names. apischema also provides a conversion to use names instead.","title":"Enumeration"},{"location":"data_model/#typing-facilities","text":"typing.Optional / typing.Union ( Optional[T] is strictly equivalent to Union[T, None] ) : Deserialization select the first matching alternative; unsupported alternatives are ignored tuple ( typing.Tuple ) : Can be used as collection as well as true tuple, like tuple[str, int] typing.NewType : Serialized according to its base type typing.NamedTuple : Handled as an object type, roughly like a dataclass; fields metadata can be passed using Annotated typing.TypedDict : Handled as an object type, but with a dictionary shape; fields metadata can be passed using Annotated typing.Any : Untouched by deserialization, serialized according to the object runtime class typing.LiteralString : Handled as str","title":"Typing facilities"},{"location":"data_model/#other-standard-library-types","text":"bytes : with str (de)serialization using base64 encoding datetime.datetime datetime.date datetime.time : Supported only in 3.7+ with fromisoformat / isoformat Decimal : With float (de)serialization ipaddress.IPv4Address ipaddress.IPv4Interface ipaddress.IPv4Network ipaddress.IPv6Address ipaddress.IPv6Interface ipaddress.IPv6Network pathlib.Path re.Pattern ( typing.Pattern ) uuid.UUID : With str (de)serialization","title":"Other standard library types"},{"location":"data_model/#generic","text":"typing.Generic can be used out of the box like in the following example: from dataclasses import dataclass from typing import Generic , TypeVar import pytest from apischema import ValidationError , deserialize T = TypeVar ( \"T\" ) @dataclass class Box ( Generic [ T ]): content : T assert deserialize ( Box [ str ], { \"content\" : \"void\" }) == Box ( \"void\" ) with pytest . raises ( ValidationError ): deserialize ( Box [ str ], { \"content\" : 42 }) Warning Generic types don't have default type name (used in JSON/GraphQL schema) \u2014 should Group[Foo] be named GroupFoo / FooGroup /something else? \u2014 so they require by-class or default type_name assignment .","title":"Generic"},{"location":"data_model/#recursive-types-string-annotations-and-pep-563","text":"Recursive classes can be typed as they usually do, with or without PEP 563 . Here with string annotations: from dataclasses import dataclass from typing import Optional from apischema import deserialize @dataclass class Node : value : int child : Optional [ \"Node\" ] = None assert deserialize ( Node , { \"value\" : 0 , \"child\" : { \"value\" : 1 }}) == Node ( 0 , Node ( 1 )) Here with PEP 563 (requires 3.7+) from __future__ import annotations from dataclasses import dataclass from apischema import deserialize @dataclass class Node : value : int child : Node | None = None assert deserialize ( Node , { \"value\" : 0 , \"child\" : { \"value\" : 1 }}) == Node ( 0 , Node ( 1 )) Warning To resolve annotations, apischema uses typing.get_type_hints ; this doesn't work really well when used on objects defined outside of global scope. Warning (minor) Currently, PEP 585 can have surprising behavior when used outside the box, see bpo-41370","title":"Recursive types, string annotations and PEP 563"},{"location":"data_model/#null-vs-undefined","text":"Contrary to Javascript, Python doesn't have an undefined equivalent (if we consider None to be the equivalent of null ). But it can be useful to distinguish (especially when thinking about HTTP PATCH method) between a null field and an undefined /absent field. That's why apischema provides an Undefined constant (a single instance of UndefinedType class) which can be used as a default value everywhere where this distinction is needed. In fact, default values are used when field are absent, thus a default Undefined will mark the field as absent. Dataclass/ NamedTuple fields are ignored by serialization when Undefined . from dataclasses import dataclass from apischema import Undefined , UndefinedType , deserialize , serialize from apischema.json_schema import deserialization_schema @dataclass class Foo : bar : int | UndefinedType = Undefined baz : int | UndefinedType | None = Undefined assert deserialize ( Foo , { \"bar\" : 0 , \"baz\" : None }) == Foo ( 0 , None ) assert deserialize ( Foo , {}) == Foo ( Undefined , Undefined ) assert serialize ( Foo , Foo ( Undefined , 42 )) == { \"baz\" : 42 } # Foo.bar and Foo.baz are not required assert deserialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : \"integer\" }, \"baz\" : { \"type\" : [ \"integer\" , \"null\" ]}}, \"additionalProperties\" : False , } Note UndefinedType must only be used inside an Union , as it has no sense as a standalone type. By the way, no suitable name was found to shorten Union[T, UndefinedType] but propositions are welcomed. Note Undefined is a falsy constant, i.e. bool(Undefined) is False .","title":"null vs. undefined"},{"location":"data_model/#use-none-as-if-it-was-undefined","text":"Using None can be more convenient than Undefined as a placeholder for missing value, but Optional types are translated to nullable fields. That's why apischema provides none_as_undefined metadata, allowing None to be handled as if it was Undefined : type will not be nullable and field not serialized if its value is None . from dataclasses import dataclass , field import pytest from apischema import ValidationError , deserialize , serialize from apischema.json_schema import deserialization_schema , serialization_schema from apischema.metadata import none_as_undefined @dataclass class Foo : bar : str | None = field ( default = None , metadata = none_as_undefined ) assert ( deserialization_schema ( Foo ) == serialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : \"string\" }}, \"additionalProperties\" : False , } ) with pytest . raises ( ValidationError ): deserialize ( Foo , { \"bar\" : None }) assert serialize ( Foo , Foo ( None )) == {}","title":"Use None as if it was Undefined"},{"location":"data_model/#annotated-pep-593","text":"PEP 593 is fully supported; annotations stranger to apischema are simply ignored.","title":"Annotated - PEP 593"},{"location":"data_model/#custom-types","text":"apischema can support almost all of your custom types in a few lines of code, using the conversion feature . However, it also provides a simple and direct way to support dataclass-like types, as presented below . Otherwise, when apischema encounters a type that it doesn't support, apischema.Unsupported exception will be raised. Note In the rare case when a union member should be ignored by apischema, it's possible to use mark it as unsupported using Union[Foo, Annotated[Bar, Unsupported]] .","title":"Custom types"},{"location":"data_model/#dataclass-like-types-aka-object-types","text":"Internally, apischema handle standard object types \u2014 dataclasses, named tuple and typed dictionary \u2014 the same way by mapping them to a set of apischema.objects.ObjectField , which has the following definition: @dataclass ( frozen = True ) class ObjectField : name : str # field's name type : Any # field's type required : bool = True # if the field is required metadata : Mapping [ str , Any ] = field ( default_factory = dict ) # field's metadata default : InitVar [ Any ] = ... # field's default value default_factory : Optional [ Callable [[], Any ]] = None # field's default factory kind : FieldKind = FieldKind . NORMAL # NORMAL/READ_ONLY/WRITE_ONLY Thus, support of dataclass-like types ( attrs , SQLAlchemy traditional mappers, etc.) can be achieved by mapping the concerned class to its own list of ObjectField s; this is done using apischema.objects.set_object_fields . from apischema import deserialize , serialize from apischema.json_schema import deserialization_schema from apischema.objects import ObjectField , set_object_fields class Foo : def __init__ ( self , bar ): self . bar = bar set_object_fields ( Foo , [ ObjectField ( \"bar\" , int )]) # Fields can also be passed in a factory set_object_fields ( Foo , lambda : [ ObjectField ( \"bar\" , int )]) foo = deserialize ( Foo , { \"bar\" : 0 }) assert isinstance ( foo , Foo ) and foo . bar == 0 assert serialize ( Foo , Foo ( 0 )) == { \"bar\" : 0 } assert deserialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : \"integer\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , } Another way to set object fields is to directly modify apischema default behavior, using apischema.settings.default_object_fields . Note set_object_fields / settings.default_object_fields can be used to override existing fields. Current fields can be retrieved using apischema.objects.object_fields . from collections.abc import Sequence from typing import Optional from apischema import settings from apischema.objects import ObjectField previous_default_object_fields = settings . default_object_field def default_object_fields ( cls ) -> Optional [ Sequence [ ObjectField ]]: return [ ... ] if ... else previous_default_object_fields ( cls ) settings . default_object_fields = default_object_fields Note Almost every default behavior of apischema can be customized using apischema.settings . Examples of SQLAlchemy support and attrs support illustrate both methods (which could also be combined).","title":"Dataclass-like types, aka object types"},{"location":"data_model/#skip-field","text":"Dataclass fields can be excluded from apischema processing by using apischema.metadata.skip in the field metadata. It can be parametrized with deserialization / serialization boolean parameters to skip a field only for the given operations. from dataclasses import dataclass , field from typing import Any from apischema.json_schema import deserialization_schema , serialization_schema from apischema.metadata import skip @dataclass class Foo : bar : Any deserialization_only : Any = field ( metadata = skip ( serialization = True )) serialization_only : Any = field ( default = None , metadata = skip ( deserialization = True )) baz : Any = field ( default = None , metadata = skip ) assert deserialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"type\" : \"object\" , \"properties\" : { \"bar\" : {}, \"deserialization_only\" : {}}, \"required\" : [ \"bar\" , \"deserialization_only\" ], \"additionalProperties\" : False , } assert serialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"type\" : \"object\" , \"properties\" : { \"bar\" : {}, \"serialization_only\" : {}}, \"required\" : [ \"bar\" , \"serialization_only\" ], \"additionalProperties\" : False , } Note Fields skipped in deserialization should have a default value if deserialized, because deserialization of the class could raise otherwise.","title":"Skip field"},{"location":"data_model/#skip-field-serialization-depending-on-condition","text":"Field can also be skipped when serializing, depending on the condition given by serialization_if , or when the field value is equal to its default value with serialization_default=True . from dataclasses import dataclass , field from typing import Any from apischema import serialize from apischema.metadata import skip @dataclass class Foo : bar : Any = field ( metadata = skip ( serialization_if = lambda x : not x )) baz : Any = field ( default_factory = list , metadata = skip ( serialization_default = True )) assert serialize ( Foo ( False , [])) == {}","title":"Skip field serialization depending on condition"},{"location":"data_model/#composition-over-inheritance-composed-dataclasses-flattening","text":"Dataclass fields which are themselves dataclass can be \"flattened\" into the owning one by using flatten metadata. Then, when the class is (de)serialized, \"flattened\" fields will be (de)serialized at the same level as the owning class. from dataclasses import dataclass , field from apischema import Undefined , UndefinedType , alias , deserialize , serialize from apischema.fields import with_fields_set from apischema.json_schema import deserialization_schema from apischema.metadata import flatten @dataclass class JsonSchema : title : str | UndefinedType = Undefined description : str | UndefinedType = Undefined format : str | UndefinedType = Undefined ... @with_fields_set @dataclass class RootJsonSchema : schema : str | UndefinedType = field ( default = Undefined , metadata = alias ( \"$schema\" )) defs : list [ JsonSchema ] = field ( default_factory = list , metadata = alias ( \"$defs\" )) # This field schema is flattened inside the owning one json_schema : JsonSchema = field ( default_factory = JsonSchema , metadata = flatten ) data = { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"title\" : \"flattened example\" , } root_schema = RootJsonSchema ( schema = \"http://json-schema.org/draft/2020-12/schema#\" , json_schema = JsonSchema ( title = \"flattened example\" ), ) assert deserialize ( RootJsonSchema , data ) == root_schema assert serialize ( RootJsonSchema , root_schema ) == data assert deserialization_schema ( RootJsonSchema ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"$defs\" : { \"JsonSchema\" : { \"type\" : \"object\" , \"properties\" : { \"title\" : { \"type\" : \"string\" }, \"description\" : { \"type\" : \"string\" }, \"format\" : { \"type\" : \"string\" }, }, \"additionalProperties\" : False , } }, # It results in allOf + unevaluatedProperties=False \"allOf\" : [ # RootJsonSchema (without JsonSchema) { \"type\" : \"object\" , \"properties\" : { \"$schema\" : { \"type\" : \"string\" }, \"$defs\" : { \"type\" : \"array\" , \"items\" : { \"$ref\" : \"#/$defs/JsonSchema\" }, \"default\" : [], }, }, \"additionalProperties\" : False , }, # JonsSchema { \"$ref\" : \"#/$defs/JsonSchema\" }, ], \"unevaluatedProperties\" : False , } Note Generated JSON schema use unevaluatedProperties keyword . This feature is very convenient for building model by composing smaller components. If some kind of reuse could also be achieved with inheritance, it can be less practical when it comes to use it in code, because there is no easy way to build an inherited class when you have an instance of the super class; you have to copy all the fields by hand. On the other hand, using composition (of flattened fields), it's easy to instantiate the class when the smaller component is just a field of it.","title":"Composition over inheritance - composed dataclasses flattening"},{"location":"data_model/#faq","text":"","title":"FAQ"},{"location":"data_model/#why-isnt-iterable-handled-with-other-collection-types","text":"Iterable could be handled (actually, it was at the beginning), however, this doesn't really make sense from a data point of view. Iterables are computation objects, they can be infinite, etc. They don't correspond to a serialized data; Collection is way more appropriate in this context.","title":"Why isn't Iterable handled with other collection types?"},{"location":"data_model/#what-happens-if-i-override-dataclass-__init__","text":"apischema always assumes that dataclass __init__ can be called with all its fields as kwargs parameters. If that's no longer the case after a modification of __init__ (what means if an exception is thrown when the constructor is called because of bad parameters), apischema treats then the class as not supported .","title":"What happens if I override dataclass __init__?"},{"location":"de_serialization/","text":"(De)serialization \u00b6 apischema aims to help with deserialization/serialization of API data, mostly JSON. Let's start again with the overview example from collections.abc import Collection from dataclasses import dataclass , field from uuid import UUID , uuid4 import pytest from graphql import print_schema from apischema import ValidationError , deserialize , serialize from apischema.graphql import graphql_schema from apischema.json_schema import deserialization_schema # Define a schema with standard dataclasses @dataclass class Resource : id : UUID name : str tags : set [ str ] = field ( default_factory = set ) # Get some data uuid = uuid4 () data = { \"id\" : str ( uuid ), \"name\" : \"wyfo\" , \"tags\" : [ \"some_tag\" ]} # Deserialize data resource = deserialize ( Resource , data ) assert resource == Resource ( uuid , \"wyfo\" , { \"some_tag\" }) # Serialize objects assert serialize ( Resource , resource ) == data # Validate during deserialization with pytest . raises ( ValidationError ) as err : # pytest checks exception is raised deserialize ( Resource , { \"id\" : \"42\" , \"name\" : \"wyfo\" }) assert err . value . errors == [ { \"loc\" : [ \"id\" ], \"err\" : \"badly formed hexadecimal UUID string\" } ] # Generate JSON Schema assert deserialization_schema ( Resource ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"type\" : \"object\" , \"properties\" : { \"id\" : { \"type\" : \"string\" , \"format\" : \"uuid\" }, \"name\" : { \"type\" : \"string\" }, \"tags\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" }, \"uniqueItems\" : True , \"default\" : [], }, }, \"required\" : [ \"id\" , \"name\" ], \"additionalProperties\" : False , } # Define GraphQL operations def resources ( tags : Collection [ str ] | None = None ) -> Collection [ Resource ] | None : ... # Generate GraphQL schema schema = graphql_schema ( query = [ resources ], id_types = { UUID }) schema_str = \"\"\" \\ type Query { resources(tags: [String!]): [Resource!] } type Resource { id: ID! name: String! tags: [String!]! }\"\"\" assert print_schema ( schema ) == schema_str Deserialization \u00b6 apischema.deserialize deserializes Python types from JSON-like data: dict / list / str / int / float / bool / None \u2014 in short, what you get when you execute json.loads . Types can be dataclasses as well as list[int] , NewType s, or whatever you want (see conversions to extend deserialization support to every type you want). from collections.abc import Collection , Mapping from dataclasses import dataclass from typing import NewType from apischema import deserialize @dataclass class Foo : bar : str MyInt = NewType ( \"MyInt\" , int ) assert deserialize ( Foo , { \"bar\" : \"bar\" }) == Foo ( \"bar\" ) assert deserialize ( MyInt , 0 ) == MyInt ( 0 ) == 0 assert deserialize ( Mapping [ str , Collection [ Foo ]], { \"key\" : [{ \"bar\" : \"42\" }]}) == { \"key\" : [ Foo ( \"42\" )] } Deserialization performs a validation of data, based on typing annotations and other information (see schema and validation ). Deserialization passthrough \u00b6 In some case, e.g. MessagePack loading with raw bytes inside, some data will have other type than JSON primitive ones. These types can be allowed using pass_through parameter; it must be collection of classes, or a predicate. Behavior can also be set globally using apischema.settings.deserialization.pass_through . Only non JSON primitive classes can be allowed, because apischema relies on a type check with isinstance to skip deserialization. That exclude NewType but also TypeDict . from datetime import datetime , timedelta from apischema import deserialize start , end = datetime . now (), datetime . now () + timedelta ( 1 ) assert deserialize ( tuple [ datetime , datetime ], [ start , end ], pass_through = { datetime } ) == ( start , end ) # Passing through types can also be deserialized normally from JSON types assert deserialize ( tuple [ datetime , datetime ], [ start . isoformat (), end . isoformat ()], pass_through = { datetime }, ) == ( start , end ) Note Equivalent serialization feature is presented in optimizations documentation . Strictness \u00b6 Coercion \u00b6 apischema is strict by default. You ask for an integer, you have to receive an integer. However, in some cases, data has to be be coerced, for example when parsing a configuration file. That can be done using coerce parameter; when set to True , all primitive types will be coerced to the expected type of the data model like the following: import pytest from apischema import ValidationError , deserialize with pytest . raises ( ValidationError ): deserialize ( bool , \"ok\" ) assert deserialize ( bool , \"ok\" , coerce = True ) bool can be coerced from str with the following case-insensitive mapping: False True 0 1 f t n y no yes false true off on ko ok The coerce parameter can also receive a coercion function which will then be used instead of default one. from typing import TypeVar , cast import pytest from apischema import ValidationError , deserialize T = TypeVar ( \"T\" ) def coerce ( cls : type [ T ], data ) -> T : \"\"\"Only coerce int to bool\"\"\" if cls is bool and isinstance ( data , int ): return cast ( T , bool ( data )) else : return data with pytest . raises ( ValidationError ): deserialize ( bool , 0 ) with pytest . raises ( ValidationError ): assert deserialize ( bool , \"ok\" , coerce = coerce ) assert deserialize ( bool , 1 , coerce = coerce ) Note If coercer result is not an instance of class passed in argument, a ValidationError will be raised with an appropriate error message Warning Coercer first argument is a primitive json type str / bool / int / float / list / dict / type(None) ; it can be type(None) , so returning cls(data) will fail in this case. Additional properties \u00b6 apischema is strict too about the number of fields received for an object . In JSON schema terms, apischema put \"additionalProperties\": false by default (this can be configured by class with properties field ). This behavior can be controlled by additional_properties parameter. When set to True , it prevents the rejection of unexpected properties. from dataclasses import dataclass import pytest from apischema import ValidationError , deserialize @dataclass class Foo : bar : str data = { \"bar\" : \"bar\" , \"other\" : 42 } with pytest . raises ( ValidationError ): deserialize ( Foo , data ) assert deserialize ( Foo , data , additional_properties = True ) == Foo ( \"bar\" ) Fall back on default \u00b6 Validation errors can happen when deserializing an ill-formed field. However, if this field has a default value/factory, deserialization can fall back on this default; this is enabled by fall_back_on_default parameter. This behavior can also be configured for each field using metadata. from dataclasses import dataclass , field import pytest from apischema import ValidationError , deserialize from apischema.metadata import fall_back_on_default @dataclass class Foo : bar : str = \"bar\" baz : str = field ( default = \"baz\" , metadata = fall_back_on_default ) with pytest . raises ( ValidationError ): deserialize ( Foo , { \"bar\" : 0 }) assert deserialize ( Foo , { \"bar\" : 0 }, fall_back_on_default = True ) == Foo () assert deserialize ( Foo , { \"baz\" : 0 }) == Foo () Strictness configuration \u00b6 apischema global configuration is managed through apischema.settings object. It has, among other, three global variables settings.additional_properties , settings.deserialization.coerce and settings.deserialization.fall_back_on_default whose values are used as default parameter values for the deserialize ; by default, additional_properties=False , coerce=False and fall_back_on_default=False . Note additional_properties settings is in settings.deserialization because it's also used in serialization . Global coercion function can be set with settings.coercer following this example: import json from apischema import ValidationError , settings prev_coercer = settings . coercer def coercer ( cls , data ): \"\"\"In case of coercion failures, try to deserialize json data\"\"\" try : return prev_coercer ( cls , data ) except ValidationError as err : if not isinstance ( data , str ): raise try : return json . loads ( data ) except json . JSONDecodeError : raise err settings . coercer = coercer Fields set \u00b6 Sometimes, it can be useful to know which field has been set by the deserialization, for example in the case of PATCH requests, to know which field has been updated. Moreover, it is also used in serialization to limit the fields serialized (see next section ) Because apischema use vanilla dataclasses, this feature is not enabled by default and must be set explicitly on a per-class basis. apischema provides a simple API to get/set this metadata. from dataclasses import dataclass from apischema import deserialize from apischema.fields import ( fields_set , is_set , set_fields , unset_fields , with_fields_set , ) # This decorator enable the feature @with_fields_set @dataclass class Foo : bar : int baz : str | None = None # Retrieve fields set foo1 = Foo ( 0 , None ) assert fields_set ( foo1 ) == { \"bar\" , \"baz\" } foo2 = Foo ( 0 ) assert fields_set ( foo2 ) == { \"bar\" } # Test fields individually (with autocompletion and refactoring) assert is_set ( foo1 ) . baz assert not is_set ( foo2 ) . baz # Mark fields as set/unset set_fields ( foo2 , \"baz\" ) assert fields_set ( foo2 ) == { \"bar\" , \"baz\" } unset_fields ( foo2 , \"baz\" ) assert fields_set ( foo2 ) == { \"bar\" } set_fields ( foo2 , \"baz\" , overwrite = True ) assert fields_set ( foo2 ) == { \"baz\" } # Fields modification are taken in account foo2 . bar = 0 assert fields_set ( foo2 ) == { \"bar\" , \"baz\" } # Because deserialization use normal constructor, it works with the feature foo3 = deserialize ( Foo , { \"bar\" : 0 }) assert fields_set ( foo3 ) == { \"bar\" } Warning The with_fields_set decorator MUST be put above dataclass one. This is because both of them modify __init__ method, but only the first is built to take the second in account. Warning dataclasses.replace works by setting all the fields of the replaced object. Because of this issue, apischema provides a little wrapper apischema.dataclasses.replace . Serialization \u00b6 apischema.serialize is used to serialize Python objects to JSON-like data. Contrary to apischema.deserialize , Python type can be omitted; in this case, the object will be serialized with an typing.Any type, i.e. the class of the serialized object will be used. from dataclasses import dataclass from typing import Any from apischema import serialize @dataclass class Foo : bar : str assert serialize ( Foo , Foo ( \"baz\" )) == { \"bar\" : \"baz\" } assert serialize ( tuple [ int , int ], ( 0 , 1 )) == [ 0 , 1 ] assert ( serialize ( Any , { \"key\" : ( \"value\" , 42 )}) == serialize ({ \"key\" : ( \"value\" , 42 )}) == { \"key\" : [ \"value\" , 42 ]} ) assert serialize ( Foo ( \"baz\" )) == { \"bar\" : \"baz\" } Note Omitting type with serialize can have unwanted side effects, as it makes loose any type annotations of the serialized object. In fact, generic specialization as well as PEP 593 annotations cannot be retrieved from an object instance; conversions can also be impacted That's why it's advisable to pass the type when it is available. Type checking \u00b6 Serialization can be configured using check_type (default to False ) and fall_back_on_any (default to False ) parameters. If check_type is True , the serialized object type will be checked to match the serialized type. If it doesn't, fall_back_on_any allows bypassing the serialized type to use typing.Any instead, i.e. to use the serialized object class. The default values of these parameters can be modified through apischema.settings.serialization.check_type and apischema.settings.serialization.fall_back_on_any . Note apischema relies on typing annotations, and assumes that the code is well statically type-checked. That's why it doesn't add the overhead of type checking by default (it's more than 10% performance impact). Serialized methods/properties \u00b6 apischema can execute methods/properties during serialization and add the computed values with the other fields values; just put apischema.serialized decorator on top of methods/properties you want to be serialized. The function name is used unless an alias is given in decorator argument. from dataclasses import dataclass from apischema import serialize , serialized from apischema.json_schema import serialization_schema @dataclass class Foo : @serialized @property def bar ( self ) -> int : return 0 # Serialized method can have default argument @serialized def baz ( self , some_arg_with_default : int = 1 ) -> int : return some_arg_with_default @serialized ( \"aliased\" ) @property def with_alias ( self ) -> int : return 2 # Serialized method can also be defined outside class, # but first parameter must be annotated @serialized def function ( foo : Foo ) -> int : return 3 assert serialize ( Foo , Foo ()) == { \"bar\" : 0 , \"baz\" : 1 , \"aliased\" : 2 , \"function\" : 3 } assert serialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"type\" : \"object\" , \"properties\" : { \"aliased\" : { \"type\" : \"integer\" }, \"bar\" : { \"type\" : \"integer\" }, \"baz\" : { \"type\" : \"integer\" }, \"function\" : { \"type\" : \"integer\" }, }, \"required\" : [ \"bar\" , \"baz\" , \"aliased\" , \"function\" ], \"additionalProperties\" : False , } Note Serialized methods must not have parameters without default, as apischema needs to execute them without arguments Note Overriding of a serialized method in a subclass will also override the serialization of the subclass. Error handling \u00b6 Errors occurring in serialized methods can be caught in a dedicated error handler registered with error_handler parameter. This function takes in parameters the exception, the object and the alias of the serialized method; it can return a new value or raise the current or another exception \u2014 it can for example be used to log errors without throwing the complete serialization. The resulting serialization type will be a Union of the normal type and the error handling type; if the error handler always raises, use typing.NoReturn annotation. error_handler=None correspond to a default handler which only return None \u2014 exception is thus discarded and serialization type becomes Optional . The error handler is only executed by apischema serialization process, it's not added to the function, so this one can be executed normally and raise an exception in the rest of your code. from dataclasses import dataclass from logging import getLogger from typing import Any from apischema import serialize , serialized from apischema.json_schema import serialization_schema logger = getLogger ( __name__ ) def log_error ( error : Exception , obj : Any , alias : str ) -> None : logger . error ( \"Serialization error in %s . %s \" , type ( obj ) . __name__ , alias , exc_info = error ) return None @dataclass class Foo : @serialized ( error_handler = log_error ) def bar ( self ) -> int : raise RuntimeError ( \"Some error\" ) assert serialize ( Foo , Foo ()) == { \"bar\" : None } # Logs \"Serialization error in Foo.bar\" assert serialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : [ \"integer\" , \"null\" ]}}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , } Non-required serialized methods \u00b6 Serialized methods (or their error handler) can return apischema.Undefined , in which case the property will not be included into the serialization; accordingly, the property loses the required qualification in the JSON schema. from dataclasses import dataclass from apischema import Undefined , UndefinedType , serialize , serialized from apischema.json_schema import serialization_schema @dataclass class Foo : @serialized def bar ( self ) -> int | UndefinedType : return Undefined assert serialize ( Foo , Foo ()) == {} assert serialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : \"integer\" }}, \"additionalProperties\" : False , } Generic serialized methods \u00b6 Serialized methods of generic classes get the right type when their owning class is specialized. from dataclasses import dataclass from typing import Generic , TypeVar from apischema import serialized from apischema.json_schema import serialization_schema T = TypeVar ( \"T\" ) U = TypeVar ( \"U\" ) @dataclass class Foo ( Generic [ T ]): @serialized def bar ( self ) -> T : ... @serialized def baz ( foo : Foo [ U ]) -> U : ... @dataclass class FooInt ( Foo [ int ]): ... assert ( serialization_schema ( Foo [ int ]) == serialization_schema ( FooInt ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : \"integer\" }, \"baz\" : { \"type\" : \"integer\" }}, \"required\" : [ \"bar\" , \"baz\" ], \"additionalProperties\" : False , } ) Exclude unset fields \u00b6 When a class has a lot of optional fields, it can be convenient to not include all of them, to avoid a bunch of useless fields in your serialized data. Using the previous feature of fields set tracking , serialize can exclude unset fields using its exclude_unset parameter or settings.serialization.exclude_unset (default is True ). from dataclasses import dataclass from apischema import serialize from apischema.fields import with_fields_set # Decorator needed to benefit from the feature @with_fields_set @dataclass class Foo : bar : int baz : str | None = None assert serialize ( Foo , Foo ( 0 )) == { \"bar\" : 0 } assert serialize ( Foo , Foo ( 0 ), exclude_unset = False ) == { \"bar\" : 0 , \"baz\" : None } Note As written in comment in the example, with_fields_set is necessary to benefit from the feature. If the dataclass don't use it, the feature will have no effect. Sometimes, some fields must be serialized, even with their default value; this behavior can be enforced using field metadata. With it, a field will be marked as set even if its default value is used at initialization. from dataclasses import dataclass , field from apischema import serialize from apischema.fields import with_fields_set from apischema.metadata import default_as_set # Decorator needed to benefit from the feature @with_fields_set @dataclass class Foo : bar : int | None = field ( default = None , metadata = default_as_set ) assert serialize ( Foo , Foo ()) == { \"bar\" : None } assert serialize ( Foo , Foo ( 0 )) == { \"bar\" : 0 } Note This metadata has effect only in combination with with_fields_set decorator. Exclude fields with default value or None \u00b6 Fields metadata apischema.skip already allows skipping fields serialization depending on a condition, for example if the field is None or equal to its default value. However, it must be added on each concerned fields, and that can be tedious when you want to set that behavior globally. That's why apischema provides the two following settings: settings.serialization.exclude_defaults : whether fields which are equal to their default values should be excluded from serialization; default False settings.serialization.exclude_none : whether fields which are equal to None should be excluded from serialization; default False These settings can also be set directly using serialize parameters, like in the following example: from dataclasses import dataclass from apischema import serialize @dataclass class Foo : bar : int = 0 baz : str | None = None assert serialize ( Foo , Foo (), exclude_defaults = True ) == {} assert serialize ( Foo , Foo (), exclude_none = True ) == { \"bar\" : 0 } Field ordering \u00b6 Usually, JSON object properties are unordered, but sometimes, order does matter. By default, fields, are ordered according to their declaration; serialized methods are appended after the fields. However, it's possible to change the ordering using apischema.order . Class-level ordering \u00b6 order can be used to decorate a class with the field ordered as expected: import json from dataclasses import dataclass from apischema import order , serialize @order ([ \"baz\" , \"bar\" , \"biz\" ]) @dataclass class Foo : bar : int baz : int biz : str assert json . dumps ( serialize ( Foo , Foo ( 0 , 0 , \"\" ))) == '{\"baz\": 0, \"bar\": 0, \"biz\": \"\"}' Field-level ordering \u00b6 Each field has an order \"value\" (0 by default), and ordering is done by sorting fields using this value; if several fields have the same order value, they are sorted by their declaration order. For instance, assigning -1 to a field will put it before every other fields, and 999 will surely put it at the end. This order value is set using order , this time as a field metadata (or passed to order argument of serialized methods/properties ). It has the following overloaded signature: order(value: int, /) : set the order value of the field order(*, after) : ignore the order value and put the field after the given field/method/property order(*, before) : ignore the order value and put the field before the given field/method/property Note after and before can be raw strings, but also dataclass fields, methods or properties. Also, order can again be used as class decorator to override ordering metadata, by passing this time a mapping of field with their overridden order. import json from dataclasses import dataclass , field from datetime import date from apischema import order , serialize , serialized @order ({ \"trigram\" : order ( - 1 )}) @dataclass class User : firstname : str lastname : str address : str = field ( metadata = order ( after = \"birthdate\" )) birthdate : date = field () @serialized @property def trigram ( self ) -> str : return ( self . firstname [ 0 ] + self . lastname [ 0 ] + self . lastname [ - 1 ]) . lower () @serialized ( order = order ( before = birthdate )) @property def age ( self ) -> int : age = date . today () . year - self . birthdate . year if age > 0 and ( date . today () . month , date . today () . day ) < ( self . birthdate . month , self . birthdate . day , ): age -= 1 return age user = User ( \"Harry\" , \"Potter\" , \"London\" , date ( 1980 , 7 , 31 )) dump = f \"\"\" {{ \"trigram\": \"hpr\", \"firstname\": \"Harry\", \"lastname\": \"Potter\", \"age\": { user . age } , \"birthdate\": \"1980-07-31\", \"address\": \"London\" }} \"\"\" assert json . dumps ( serialize ( User , user ), indent = 4 ) == dump TypedDict additional properties \u00b6 TypedDict can contain additional keys, which are not serialized by default. Setting additional_properties parameter to True (or apischema.settings.additional_properties ) will toggle on their serialization (without aliasing). FAQ \u00b6 Why isn't coercion the default behavior? \u00b6 Because ill-formed data can be symptomatic of deeper issues, it has been decided that highlighting them would be better than hiding them. By the way, this is easily globally configurable. Why isn't with_fields_set enabled by default? \u00b6 It's true that this feature has the little cost of adding a decorator everywhere. However, keeping dataclass decorator allows IDEs/linters/type checkers/etc. to handle the class as such, so there is no need to develop a plugin for them. Standard compliance can be worth the additional decorator. (And little overhead can be avoided when not useful) Why isn't serialization type checking enabled by default? \u00b6 Type checking has a runtime cost, which means poorer performance. Moreover, as explained in performances section , it prevents \"passthrough\" optimization. At last, code is supposed to be statically verified, and thus types already checked. (If some silly things are done and leads to have unsupported types passed to the JSON library, an error will be raised anyway). Runtime type checking is more a development feature, which could for example be with apischema.settings.serialization.check_type = __debug__ . Why not use json library default fallback parameter for serialization? \u00b6 Some apischema features like conversions can simply not be implemented with default fallback. By the way, apischema can perform surprisingly better than using default . However, default can be used in combination with passthrough optimization when needed to improve performance.","title":"(De)serialization"},{"location":"de_serialization/#deserialization","text":"apischema aims to help with deserialization/serialization of API data, mostly JSON. Let's start again with the overview example from collections.abc import Collection from dataclasses import dataclass , field from uuid import UUID , uuid4 import pytest from graphql import print_schema from apischema import ValidationError , deserialize , serialize from apischema.graphql import graphql_schema from apischema.json_schema import deserialization_schema # Define a schema with standard dataclasses @dataclass class Resource : id : UUID name : str tags : set [ str ] = field ( default_factory = set ) # Get some data uuid = uuid4 () data = { \"id\" : str ( uuid ), \"name\" : \"wyfo\" , \"tags\" : [ \"some_tag\" ]} # Deserialize data resource = deserialize ( Resource , data ) assert resource == Resource ( uuid , \"wyfo\" , { \"some_tag\" }) # Serialize objects assert serialize ( Resource , resource ) == data # Validate during deserialization with pytest . raises ( ValidationError ) as err : # pytest checks exception is raised deserialize ( Resource , { \"id\" : \"42\" , \"name\" : \"wyfo\" }) assert err . value . errors == [ { \"loc\" : [ \"id\" ], \"err\" : \"badly formed hexadecimal UUID string\" } ] # Generate JSON Schema assert deserialization_schema ( Resource ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"type\" : \"object\" , \"properties\" : { \"id\" : { \"type\" : \"string\" , \"format\" : \"uuid\" }, \"name\" : { \"type\" : \"string\" }, \"tags\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" }, \"uniqueItems\" : True , \"default\" : [], }, }, \"required\" : [ \"id\" , \"name\" ], \"additionalProperties\" : False , } # Define GraphQL operations def resources ( tags : Collection [ str ] | None = None ) -> Collection [ Resource ] | None : ... # Generate GraphQL schema schema = graphql_schema ( query = [ resources ], id_types = { UUID }) schema_str = \"\"\" \\ type Query { resources(tags: [String!]): [Resource!] } type Resource { id: ID! name: String! tags: [String!]! }\"\"\" assert print_schema ( schema ) == schema_str","title":"(De)serialization"},{"location":"de_serialization/#deserialization_1","text":"apischema.deserialize deserializes Python types from JSON-like data: dict / list / str / int / float / bool / None \u2014 in short, what you get when you execute json.loads . Types can be dataclasses as well as list[int] , NewType s, or whatever you want (see conversions to extend deserialization support to every type you want). from collections.abc import Collection , Mapping from dataclasses import dataclass from typing import NewType from apischema import deserialize @dataclass class Foo : bar : str MyInt = NewType ( \"MyInt\" , int ) assert deserialize ( Foo , { \"bar\" : \"bar\" }) == Foo ( \"bar\" ) assert deserialize ( MyInt , 0 ) == MyInt ( 0 ) == 0 assert deserialize ( Mapping [ str , Collection [ Foo ]], { \"key\" : [{ \"bar\" : \"42\" }]}) == { \"key\" : [ Foo ( \"42\" )] } Deserialization performs a validation of data, based on typing annotations and other information (see schema and validation ).","title":"Deserialization"},{"location":"de_serialization/#deserialization-passthrough","text":"In some case, e.g. MessagePack loading with raw bytes inside, some data will have other type than JSON primitive ones. These types can be allowed using pass_through parameter; it must be collection of classes, or a predicate. Behavior can also be set globally using apischema.settings.deserialization.pass_through . Only non JSON primitive classes can be allowed, because apischema relies on a type check with isinstance to skip deserialization. That exclude NewType but also TypeDict . from datetime import datetime , timedelta from apischema import deserialize start , end = datetime . now (), datetime . now () + timedelta ( 1 ) assert deserialize ( tuple [ datetime , datetime ], [ start , end ], pass_through = { datetime } ) == ( start , end ) # Passing through types can also be deserialized normally from JSON types assert deserialize ( tuple [ datetime , datetime ], [ start . isoformat (), end . isoformat ()], pass_through = { datetime }, ) == ( start , end ) Note Equivalent serialization feature is presented in optimizations documentation .","title":"Deserialization passthrough"},{"location":"de_serialization/#strictness","text":"","title":"Strictness"},{"location":"de_serialization/#coercion","text":"apischema is strict by default. You ask for an integer, you have to receive an integer. However, in some cases, data has to be be coerced, for example when parsing a configuration file. That can be done using coerce parameter; when set to True , all primitive types will be coerced to the expected type of the data model like the following: import pytest from apischema import ValidationError , deserialize with pytest . raises ( ValidationError ): deserialize ( bool , \"ok\" ) assert deserialize ( bool , \"ok\" , coerce = True ) bool can be coerced from str with the following case-insensitive mapping: False True 0 1 f t n y no yes false true off on ko ok The coerce parameter can also receive a coercion function which will then be used instead of default one. from typing import TypeVar , cast import pytest from apischema import ValidationError , deserialize T = TypeVar ( \"T\" ) def coerce ( cls : type [ T ], data ) -> T : \"\"\"Only coerce int to bool\"\"\" if cls is bool and isinstance ( data , int ): return cast ( T , bool ( data )) else : return data with pytest . raises ( ValidationError ): deserialize ( bool , 0 ) with pytest . raises ( ValidationError ): assert deserialize ( bool , \"ok\" , coerce = coerce ) assert deserialize ( bool , 1 , coerce = coerce ) Note If coercer result is not an instance of class passed in argument, a ValidationError will be raised with an appropriate error message Warning Coercer first argument is a primitive json type str / bool / int / float / list / dict / type(None) ; it can be type(None) , so returning cls(data) will fail in this case.","title":"Coercion"},{"location":"de_serialization/#additional-properties","text":"apischema is strict too about the number of fields received for an object . In JSON schema terms, apischema put \"additionalProperties\": false by default (this can be configured by class with properties field ). This behavior can be controlled by additional_properties parameter. When set to True , it prevents the rejection of unexpected properties. from dataclasses import dataclass import pytest from apischema import ValidationError , deserialize @dataclass class Foo : bar : str data = { \"bar\" : \"bar\" , \"other\" : 42 } with pytest . raises ( ValidationError ): deserialize ( Foo , data ) assert deserialize ( Foo , data , additional_properties = True ) == Foo ( \"bar\" )","title":"Additional properties"},{"location":"de_serialization/#fall-back-on-default","text":"Validation errors can happen when deserializing an ill-formed field. However, if this field has a default value/factory, deserialization can fall back on this default; this is enabled by fall_back_on_default parameter. This behavior can also be configured for each field using metadata. from dataclasses import dataclass , field import pytest from apischema import ValidationError , deserialize from apischema.metadata import fall_back_on_default @dataclass class Foo : bar : str = \"bar\" baz : str = field ( default = \"baz\" , metadata = fall_back_on_default ) with pytest . raises ( ValidationError ): deserialize ( Foo , { \"bar\" : 0 }) assert deserialize ( Foo , { \"bar\" : 0 }, fall_back_on_default = True ) == Foo () assert deserialize ( Foo , { \"baz\" : 0 }) == Foo ()","title":"Fall back on default"},{"location":"de_serialization/#strictness-configuration","text":"apischema global configuration is managed through apischema.settings object. It has, among other, three global variables settings.additional_properties , settings.deserialization.coerce and settings.deserialization.fall_back_on_default whose values are used as default parameter values for the deserialize ; by default, additional_properties=False , coerce=False and fall_back_on_default=False . Note additional_properties settings is in settings.deserialization because it's also used in serialization . Global coercion function can be set with settings.coercer following this example: import json from apischema import ValidationError , settings prev_coercer = settings . coercer def coercer ( cls , data ): \"\"\"In case of coercion failures, try to deserialize json data\"\"\" try : return prev_coercer ( cls , data ) except ValidationError as err : if not isinstance ( data , str ): raise try : return json . loads ( data ) except json . JSONDecodeError : raise err settings . coercer = coercer","title":"Strictness configuration"},{"location":"de_serialization/#fields-set","text":"Sometimes, it can be useful to know which field has been set by the deserialization, for example in the case of PATCH requests, to know which field has been updated. Moreover, it is also used in serialization to limit the fields serialized (see next section ) Because apischema use vanilla dataclasses, this feature is not enabled by default and must be set explicitly on a per-class basis. apischema provides a simple API to get/set this metadata. from dataclasses import dataclass from apischema import deserialize from apischema.fields import ( fields_set , is_set , set_fields , unset_fields , with_fields_set , ) # This decorator enable the feature @with_fields_set @dataclass class Foo : bar : int baz : str | None = None # Retrieve fields set foo1 = Foo ( 0 , None ) assert fields_set ( foo1 ) == { \"bar\" , \"baz\" } foo2 = Foo ( 0 ) assert fields_set ( foo2 ) == { \"bar\" } # Test fields individually (with autocompletion and refactoring) assert is_set ( foo1 ) . baz assert not is_set ( foo2 ) . baz # Mark fields as set/unset set_fields ( foo2 , \"baz\" ) assert fields_set ( foo2 ) == { \"bar\" , \"baz\" } unset_fields ( foo2 , \"baz\" ) assert fields_set ( foo2 ) == { \"bar\" } set_fields ( foo2 , \"baz\" , overwrite = True ) assert fields_set ( foo2 ) == { \"baz\" } # Fields modification are taken in account foo2 . bar = 0 assert fields_set ( foo2 ) == { \"bar\" , \"baz\" } # Because deserialization use normal constructor, it works with the feature foo3 = deserialize ( Foo , { \"bar\" : 0 }) assert fields_set ( foo3 ) == { \"bar\" } Warning The with_fields_set decorator MUST be put above dataclass one. This is because both of them modify __init__ method, but only the first is built to take the second in account. Warning dataclasses.replace works by setting all the fields of the replaced object. Because of this issue, apischema provides a little wrapper apischema.dataclasses.replace .","title":"Fields set"},{"location":"de_serialization/#serialization","text":"apischema.serialize is used to serialize Python objects to JSON-like data. Contrary to apischema.deserialize , Python type can be omitted; in this case, the object will be serialized with an typing.Any type, i.e. the class of the serialized object will be used. from dataclasses import dataclass from typing import Any from apischema import serialize @dataclass class Foo : bar : str assert serialize ( Foo , Foo ( \"baz\" )) == { \"bar\" : \"baz\" } assert serialize ( tuple [ int , int ], ( 0 , 1 )) == [ 0 , 1 ] assert ( serialize ( Any , { \"key\" : ( \"value\" , 42 )}) == serialize ({ \"key\" : ( \"value\" , 42 )}) == { \"key\" : [ \"value\" , 42 ]} ) assert serialize ( Foo ( \"baz\" )) == { \"bar\" : \"baz\" } Note Omitting type with serialize can have unwanted side effects, as it makes loose any type annotations of the serialized object. In fact, generic specialization as well as PEP 593 annotations cannot be retrieved from an object instance; conversions can also be impacted That's why it's advisable to pass the type when it is available.","title":"Serialization"},{"location":"de_serialization/#type-checking","text":"Serialization can be configured using check_type (default to False ) and fall_back_on_any (default to False ) parameters. If check_type is True , the serialized object type will be checked to match the serialized type. If it doesn't, fall_back_on_any allows bypassing the serialized type to use typing.Any instead, i.e. to use the serialized object class. The default values of these parameters can be modified through apischema.settings.serialization.check_type and apischema.settings.serialization.fall_back_on_any . Note apischema relies on typing annotations, and assumes that the code is well statically type-checked. That's why it doesn't add the overhead of type checking by default (it's more than 10% performance impact).","title":"Type checking"},{"location":"de_serialization/#serialized-methodsproperties","text":"apischema can execute methods/properties during serialization and add the computed values with the other fields values; just put apischema.serialized decorator on top of methods/properties you want to be serialized. The function name is used unless an alias is given in decorator argument. from dataclasses import dataclass from apischema import serialize , serialized from apischema.json_schema import serialization_schema @dataclass class Foo : @serialized @property def bar ( self ) -> int : return 0 # Serialized method can have default argument @serialized def baz ( self , some_arg_with_default : int = 1 ) -> int : return some_arg_with_default @serialized ( \"aliased\" ) @property def with_alias ( self ) -> int : return 2 # Serialized method can also be defined outside class, # but first parameter must be annotated @serialized def function ( foo : Foo ) -> int : return 3 assert serialize ( Foo , Foo ()) == { \"bar\" : 0 , \"baz\" : 1 , \"aliased\" : 2 , \"function\" : 3 } assert serialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"type\" : \"object\" , \"properties\" : { \"aliased\" : { \"type\" : \"integer\" }, \"bar\" : { \"type\" : \"integer\" }, \"baz\" : { \"type\" : \"integer\" }, \"function\" : { \"type\" : \"integer\" }, }, \"required\" : [ \"bar\" , \"baz\" , \"aliased\" , \"function\" ], \"additionalProperties\" : False , } Note Serialized methods must not have parameters without default, as apischema needs to execute them without arguments Note Overriding of a serialized method in a subclass will also override the serialization of the subclass.","title":"Serialized methods/properties"},{"location":"de_serialization/#error-handling","text":"Errors occurring in serialized methods can be caught in a dedicated error handler registered with error_handler parameter. This function takes in parameters the exception, the object and the alias of the serialized method; it can return a new value or raise the current or another exception \u2014 it can for example be used to log errors without throwing the complete serialization. The resulting serialization type will be a Union of the normal type and the error handling type; if the error handler always raises, use typing.NoReturn annotation. error_handler=None correspond to a default handler which only return None \u2014 exception is thus discarded and serialization type becomes Optional . The error handler is only executed by apischema serialization process, it's not added to the function, so this one can be executed normally and raise an exception in the rest of your code. from dataclasses import dataclass from logging import getLogger from typing import Any from apischema import serialize , serialized from apischema.json_schema import serialization_schema logger = getLogger ( __name__ ) def log_error ( error : Exception , obj : Any , alias : str ) -> None : logger . error ( \"Serialization error in %s . %s \" , type ( obj ) . __name__ , alias , exc_info = error ) return None @dataclass class Foo : @serialized ( error_handler = log_error ) def bar ( self ) -> int : raise RuntimeError ( \"Some error\" ) assert serialize ( Foo , Foo ()) == { \"bar\" : None } # Logs \"Serialization error in Foo.bar\" assert serialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : [ \"integer\" , \"null\" ]}}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , }","title":"Error handling"},{"location":"de_serialization/#non-required-serialized-methods","text":"Serialized methods (or their error handler) can return apischema.Undefined , in which case the property will not be included into the serialization; accordingly, the property loses the required qualification in the JSON schema. from dataclasses import dataclass from apischema import Undefined , UndefinedType , serialize , serialized from apischema.json_schema import serialization_schema @dataclass class Foo : @serialized def bar ( self ) -> int | UndefinedType : return Undefined assert serialize ( Foo , Foo ()) == {} assert serialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : \"integer\" }}, \"additionalProperties\" : False , }","title":"Non-required serialized methods"},{"location":"de_serialization/#generic-serialized-methods","text":"Serialized methods of generic classes get the right type when their owning class is specialized. from dataclasses import dataclass from typing import Generic , TypeVar from apischema import serialized from apischema.json_schema import serialization_schema T = TypeVar ( \"T\" ) U = TypeVar ( \"U\" ) @dataclass class Foo ( Generic [ T ]): @serialized def bar ( self ) -> T : ... @serialized def baz ( foo : Foo [ U ]) -> U : ... @dataclass class FooInt ( Foo [ int ]): ... assert ( serialization_schema ( Foo [ int ]) == serialization_schema ( FooInt ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : \"integer\" }, \"baz\" : { \"type\" : \"integer\" }}, \"required\" : [ \"bar\" , \"baz\" ], \"additionalProperties\" : False , } )","title":"Generic serialized methods"},{"location":"de_serialization/#exclude-unset-fields","text":"When a class has a lot of optional fields, it can be convenient to not include all of them, to avoid a bunch of useless fields in your serialized data. Using the previous feature of fields set tracking , serialize can exclude unset fields using its exclude_unset parameter or settings.serialization.exclude_unset (default is True ). from dataclasses import dataclass from apischema import serialize from apischema.fields import with_fields_set # Decorator needed to benefit from the feature @with_fields_set @dataclass class Foo : bar : int baz : str | None = None assert serialize ( Foo , Foo ( 0 )) == { \"bar\" : 0 } assert serialize ( Foo , Foo ( 0 ), exclude_unset = False ) == { \"bar\" : 0 , \"baz\" : None } Note As written in comment in the example, with_fields_set is necessary to benefit from the feature. If the dataclass don't use it, the feature will have no effect. Sometimes, some fields must be serialized, even with their default value; this behavior can be enforced using field metadata. With it, a field will be marked as set even if its default value is used at initialization. from dataclasses import dataclass , field from apischema import serialize from apischema.fields import with_fields_set from apischema.metadata import default_as_set # Decorator needed to benefit from the feature @with_fields_set @dataclass class Foo : bar : int | None = field ( default = None , metadata = default_as_set ) assert serialize ( Foo , Foo ()) == { \"bar\" : None } assert serialize ( Foo , Foo ( 0 )) == { \"bar\" : 0 } Note This metadata has effect only in combination with with_fields_set decorator.","title":"Exclude unset fields"},{"location":"de_serialization/#exclude-fields-with-default-value-or-none","text":"Fields metadata apischema.skip already allows skipping fields serialization depending on a condition, for example if the field is None or equal to its default value. However, it must be added on each concerned fields, and that can be tedious when you want to set that behavior globally. That's why apischema provides the two following settings: settings.serialization.exclude_defaults : whether fields which are equal to their default values should be excluded from serialization; default False settings.serialization.exclude_none : whether fields which are equal to None should be excluded from serialization; default False These settings can also be set directly using serialize parameters, like in the following example: from dataclasses import dataclass from apischema import serialize @dataclass class Foo : bar : int = 0 baz : str | None = None assert serialize ( Foo , Foo (), exclude_defaults = True ) == {} assert serialize ( Foo , Foo (), exclude_none = True ) == { \"bar\" : 0 }","title":"Exclude fields with default value or None"},{"location":"de_serialization/#field-ordering","text":"Usually, JSON object properties are unordered, but sometimes, order does matter. By default, fields, are ordered according to their declaration; serialized methods are appended after the fields. However, it's possible to change the ordering using apischema.order .","title":"Field ordering"},{"location":"de_serialization/#class-level-ordering","text":"order can be used to decorate a class with the field ordered as expected: import json from dataclasses import dataclass from apischema import order , serialize @order ([ \"baz\" , \"bar\" , \"biz\" ]) @dataclass class Foo : bar : int baz : int biz : str assert json . dumps ( serialize ( Foo , Foo ( 0 , 0 , \"\" ))) == '{\"baz\": 0, \"bar\": 0, \"biz\": \"\"}'","title":"Class-level ordering"},{"location":"de_serialization/#field-level-ordering","text":"Each field has an order \"value\" (0 by default), and ordering is done by sorting fields using this value; if several fields have the same order value, they are sorted by their declaration order. For instance, assigning -1 to a field will put it before every other fields, and 999 will surely put it at the end. This order value is set using order , this time as a field metadata (or passed to order argument of serialized methods/properties ). It has the following overloaded signature: order(value: int, /) : set the order value of the field order(*, after) : ignore the order value and put the field after the given field/method/property order(*, before) : ignore the order value and put the field before the given field/method/property Note after and before can be raw strings, but also dataclass fields, methods or properties. Also, order can again be used as class decorator to override ordering metadata, by passing this time a mapping of field with their overridden order. import json from dataclasses import dataclass , field from datetime import date from apischema import order , serialize , serialized @order ({ \"trigram\" : order ( - 1 )}) @dataclass class User : firstname : str lastname : str address : str = field ( metadata = order ( after = \"birthdate\" )) birthdate : date = field () @serialized @property def trigram ( self ) -> str : return ( self . firstname [ 0 ] + self . lastname [ 0 ] + self . lastname [ - 1 ]) . lower () @serialized ( order = order ( before = birthdate )) @property def age ( self ) -> int : age = date . today () . year - self . birthdate . year if age > 0 and ( date . today () . month , date . today () . day ) < ( self . birthdate . month , self . birthdate . day , ): age -= 1 return age user = User ( \"Harry\" , \"Potter\" , \"London\" , date ( 1980 , 7 , 31 )) dump = f \"\"\" {{ \"trigram\": \"hpr\", \"firstname\": \"Harry\", \"lastname\": \"Potter\", \"age\": { user . age } , \"birthdate\": \"1980-07-31\", \"address\": \"London\" }} \"\"\" assert json . dumps ( serialize ( User , user ), indent = 4 ) == dump","title":"Field-level ordering"},{"location":"de_serialization/#typeddict-additional-properties","text":"TypedDict can contain additional keys, which are not serialized by default. Setting additional_properties parameter to True (or apischema.settings.additional_properties ) will toggle on their serialization (without aliasing).","title":"TypedDict additional properties"},{"location":"de_serialization/#faq","text":"","title":"FAQ"},{"location":"de_serialization/#why-isnt-coercion-the-default-behavior","text":"Because ill-formed data can be symptomatic of deeper issues, it has been decided that highlighting them would be better than hiding them. By the way, this is easily globally configurable.","title":"Why isn't coercion the default behavior?"},{"location":"de_serialization/#why-isnt-with_fields_set-enabled-by-default","text":"It's true that this feature has the little cost of adding a decorator everywhere. However, keeping dataclass decorator allows IDEs/linters/type checkers/etc. to handle the class as such, so there is no need to develop a plugin for them. Standard compliance can be worth the additional decorator. (And little overhead can be avoided when not useful)","title":"Why isn't with_fields_set enabled by default?"},{"location":"de_serialization/#why-isnt-serialization-type-checking-enabled-by-default","text":"Type checking has a runtime cost, which means poorer performance. Moreover, as explained in performances section , it prevents \"passthrough\" optimization. At last, code is supposed to be statically verified, and thus types already checked. (If some silly things are done and leads to have unsupported types passed to the JSON library, an error will be raised anyway). Runtime type checking is more a development feature, which could for example be with apischema.settings.serialization.check_type = __debug__ .","title":"Why isn't serialization type checking enabled by default?"},{"location":"de_serialization/#why-not-use-json-library-default-fallback-parameter-for-serialization","text":"Some apischema features like conversions can simply not be implemented with default fallback. By the way, apischema can perform surprisingly better than using default . However, default can be used in combination with passthrough optimization when needed to improve performance.","title":"Why not use json library default fallback parameter for serialization?"},{"location":"difference_with_pydantic/","text":"Difference with pydantic \u00b6 As the question is often asked, it is answered in a dedicated section. Here are some the key differences between apischema and pydantic : apischema is (a lot) faster \u00b6 According to benchmark , apischema is a lot faster than pydantic , especially for serialization. Both use Cython to optimize the code, but even without compilation (running only Python modules), apischema is still faster than Cythonized pydantic . Better performance, but not at the cost of fewer functionalities; that's rather the opposite: dynamic aliasing , conversions , flattened fields , etc. apischema can generate GraphQL schema from your resolvers \u00b6 Not just a simple printable schema but a complete graphql.GraphQLSchema (using graphql-core library) which can be used to execute your queries/mutations/subscriptions through your resolvers, powered by apischema (de)serialization and conversions features. Types and resolvers can be used both in traditional JSON-oriented API and GraphQL API. apischema uses standard dataclasses and types \u00b6 pydantic uses its own BaseModel class, or its own pseudo- dataclass , so you are forced to tie all your code to the library, and you cannot easily reuse code written in a more standard way or in external libraries. By the way, Pydantic use expressions in typing annotations ( conint , etc.), while it's not recommended and treated as an error by tools like Mypy apischema doesn't require external plugins for editors, linters, etc. \u00b6 pydantic requires a plugin to allow Mypy to type check BaseModel and other pydantic singularities (and to not raise errors on it); plugins are also needed for editors. apischema doesn't mix up (de)serialization with your code \u00b6 While pydantic mixes up model constructor with deserializer, apischema uses dedicated functions for its features, meaning your dataclasses are instantiated normally with type checking. In your code, you manipulate objects; (de)serialization is for input/output. apischema also doesn't mix up validation of external data with your statically checked code; there is no runtime validation in constructors. apischema truly works out-of-the-box with forward type references (especially for recursive model) \u00b6 pydantic requires calling update_forward_refs method on recursive types, while apischema \"just works\". apischema supports Generic without requiring additional stuff \u00b6 pydantic BaseModel cannot be used with generic model, you have to use GenericModel . With apischema , you just write your generic classes normally. apischema conversions feature allows to support any type defined in your code, but also in external libraries \u00b6 pydantic doesn't make it easy to support external types, like bson.ObjectId ; see this issue on the subject. You could dynamically add a __get_validators__ method to foreign classes, but that doesn't work with builtin types like collection.deque and other types written in C. Serialization customization is harder, with definition of encoding function by model; it cannot be done at the same place as deserialization. There is also no correlation done between (de)serialization customization and model JSON schema; you could have to overwrite the generated schema if you don't want to get an inconsistency. apischema only requires a few lines of code to support any type you want, from bson.ObjectId to SQLAlchemy models by way of builtin and generic like collection.deque , and even pydantic . Conversions are also integrated in JSON schema this one is generated according to the source/target of the conversion Here is a comparison of a custom type support: import re from typing import NamedTuple , NewType import pydantic.validators import apischema # Serialization can only be customized into the enclosing models class RGB ( NamedTuple ): red : int green : int blue : int # If you don't put this method, RGB schema will be: # {'title': 'Rgb', 'type': 'array', 'items': {}} @classmethod def __modify_schema__ ( cls , field_schema ) -> None : field_schema . update ({ \"type\" : \"string\" , \"pattern\" : r \"#[0-9A-Fa-f] {6} \" }) field_schema . pop ( \"items\" , ... ) @classmethod def __get_validators__ ( cls ): yield pydantic . validators . str_validator yield cls . validate @classmethod def validate ( cls , value ) -> \"RGB\" : if ( not isinstance ( value , str ) or re . fullmatch ( r \"#[0-9A-Fa-f] {6} \" , value ) is None ): raise ValueError ( \"Invalid RGB\" ) return RGB ( red = int ( value [ 1 : 3 ], 16 ), green = int ( value [ 3 : 5 ], 16 ), blue = int ( value [ 5 : 7 ], 16 ) ) # Simpler with apischema class RGB ( NamedTuple ): red : int green : int blue : int # NewType can be used to add schema to conversion source/target # but Annotated[str, apischema.schema(pattern=r\"#[0-9A-Fa-f]{6}\")] would have worked too HexaRGB = NewType ( \"HexaRGB\" , str ) # pattern is used in JSON schema and in deserialization validation apischema . schema ( pattern = r \"#[0-9A-Fa-f] {6} \" )( HexaRGB ) @apischema . deserializer # could be declared as a staticmethod of RGB class def from_hexa ( hexa : HexaRGB ) -> RGB : return RGB ( int ( hexa [ 1 : 3 ], 16 ), int ( hexa [ 3 : 5 ], 16 ), int ( hexa [ 5 : 7 ], 16 )) @apischema . serializer # could be declared as a method/property of RGB class def to_hexa ( rgb : RGB ) -> HexaRGB : return HexaRGB ( f \"# { rgb . red : 02x }{ rgb . green : 02x }{ rgb . blue : 02x } \" ) assert ( # schema is inherited from deserialized type apischema . json_schema . deserialization_schema ( RGB ) == apischema . json_schema . deserialization_schema ( HexaRGB ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"type\" : \"string\" , \"pattern\" : \"#[0-9A-Fa-f] {6} \" , } ) apischema can also customize serialization with computed fields \u00b6 Serialized methods/properties are regular methods/properties which are included in serialization effortlessly. apischema allows you to use composition over inheritance \u00b6 Flattened fields is a distinctive apischema feature that is very handy to build complex model from smaller fragments; you don't have to merge the fields of your fragments in a complex class with a lot of fields yourself, apischema deal with it for you, and your code is kept simple. apischema has a functional approach, pydantic has an object one \u00b6 pydantic features are based on BaseModel methods. You have to have a BaseModel instance to do anything, even if you manipulate only an integer. Complex pydantic stuff like __root__ model or deserialization customization come from this approach. apischema is functional, it doesn't use method but simple functions, which works with all types. You can also register conversions for any types similarly you would implement a type class in a functional language. And your class namespace don't mix up with a mandatory base class' one. apischema can use both camelCase and snake_case with the same types \u00b6 While pydantic field aliases are fixed at model creation, apischema lets you choose which aliasing you want at (de)serialization time. It can be convenient if you need to juggle with cases for the same models between frontends and other backend services for example. apischema doesn't coerce by default \u00b6 Your API respects its schema. It can also coerce, for example to parse configuration file, and coercion can be adjusted (for example coercing list from comma-separated string). apischema has a better integration of JSON schema/OpenAPI \u00b6 With pydantic , if you want to have a nullable field in the generated schema, you have to put nullable into schema extra keywords. apischema is bound to the last JSON schema version but offers conversion to other version like OpenAPI 3.0 and nullable is added for Optional types. apischema also supports more advanced features like dependentRequired or unevaluatedProperties . Reference handling is also more flexible apischema can add validators and JSON schema to NewType \u00b6 So it will be used in deserialization validation. You can use NewType everywhere, to gain a better type checking, self-documented code. apischema validators are regular methods with automatic dependencies management \u00b6 Using regular methods allows benefiting of type checking of fields, where pydantic validators use dynamic stuff (name of the fields as strings) and are not type-checked or have to get redundant type annotations. apischema validators also have automatic dependency management. And apischema directly supports JSON schema property dependencies . Comparison is simple with an example (validator is taken from pydantic documentation : from dataclasses import dataclass import pydantic import apischema class UserModel ( pydantic . BaseModel ): username : str password1 : str password2 : str @pydantic . root_validator def check_passwords_match ( cls , values ): # This is a classmethod (it needs a plugin to not raise a warning in your IDE) # What is the type of of values? of values['password1']? # If you rename password1 field, validator will hardly be updated # You also have to test yourself that values are provided pw1 , pw2 = values . get ( \"password1\" ), values . get ( \"password2\" ) if pw1 is not None and pw2 is not None and pw1 != pw2 : raise ValueError ( \"passwords do not match\" ) return values @dataclass class LoginForm : username : str password1 : str password2 : str @apischema . validator def check_password_match ( self ): # Typed checked, simpler, and not executed if password1 or password2 # are missing/invalid if self . password1 != self . password2 : raise ValueError ( \"passwords do not match\" ) apischema supports pydantic \u00b6 It's not a feature, is just the result of 30 lines of code .","title":"Difference with pydantic"},{"location":"difference_with_pydantic/#difference-with-pydantic","text":"As the question is often asked, it is answered in a dedicated section. Here are some the key differences between apischema and pydantic :","title":"Difference with pydantic"},{"location":"difference_with_pydantic/#apischema-is-a-lot-faster","text":"According to benchmark , apischema is a lot faster than pydantic , especially for serialization. Both use Cython to optimize the code, but even without compilation (running only Python modules), apischema is still faster than Cythonized pydantic . Better performance, but not at the cost of fewer functionalities; that's rather the opposite: dynamic aliasing , conversions , flattened fields , etc.","title":"apischema is (a lot) faster"},{"location":"difference_with_pydantic/#apischema-can-generate-graphql-schema-from-your-resolvers","text":"Not just a simple printable schema but a complete graphql.GraphQLSchema (using graphql-core library) which can be used to execute your queries/mutations/subscriptions through your resolvers, powered by apischema (de)serialization and conversions features. Types and resolvers can be used both in traditional JSON-oriented API and GraphQL API.","title":"apischema can generate GraphQL schema from your resolvers"},{"location":"difference_with_pydantic/#apischema-uses-standard-dataclasses-and-types","text":"pydantic uses its own BaseModel class, or its own pseudo- dataclass , so you are forced to tie all your code to the library, and you cannot easily reuse code written in a more standard way or in external libraries. By the way, Pydantic use expressions in typing annotations ( conint , etc.), while it's not recommended and treated as an error by tools like Mypy","title":"apischema uses standard dataclasses and types"},{"location":"difference_with_pydantic/#apischema-doesnt-require-external-plugins-for-editors-linters-etc","text":"pydantic requires a plugin to allow Mypy to type check BaseModel and other pydantic singularities (and to not raise errors on it); plugins are also needed for editors.","title":"apischema doesn't require external plugins for editors, linters, etc."},{"location":"difference_with_pydantic/#apischema-doesnt-mix-up-deserialization-with-your-code","text":"While pydantic mixes up model constructor with deserializer, apischema uses dedicated functions for its features, meaning your dataclasses are instantiated normally with type checking. In your code, you manipulate objects; (de)serialization is for input/output. apischema also doesn't mix up validation of external data with your statically checked code; there is no runtime validation in constructors.","title":"apischema doesn't mix up (de)serialization with your code"},{"location":"difference_with_pydantic/#apischema-truly-works-out-of-the-box-with-forward-type-references-especially-for-recursive-model","text":"pydantic requires calling update_forward_refs method on recursive types, while apischema \"just works\".","title":"apischema truly works out-of-the-box with forward type references (especially for recursive model)"},{"location":"difference_with_pydantic/#apischema-supports-generic-without-requiring-additional-stuff","text":"pydantic BaseModel cannot be used with generic model, you have to use GenericModel . With apischema , you just write your generic classes normally.","title":"apischema supports Generic without requiring additional stuff"},{"location":"difference_with_pydantic/#apischema-conversions-feature-allows-to-support-any-type-defined-in-your-code-but-also-in-external-libraries","text":"pydantic doesn't make it easy to support external types, like bson.ObjectId ; see this issue on the subject. You could dynamically add a __get_validators__ method to foreign classes, but that doesn't work with builtin types like collection.deque and other types written in C. Serialization customization is harder, with definition of encoding function by model; it cannot be done at the same place as deserialization. There is also no correlation done between (de)serialization customization and model JSON schema; you could have to overwrite the generated schema if you don't want to get an inconsistency. apischema only requires a few lines of code to support any type you want, from bson.ObjectId to SQLAlchemy models by way of builtin and generic like collection.deque , and even pydantic . Conversions are also integrated in JSON schema this one is generated according to the source/target of the conversion Here is a comparison of a custom type support: import re from typing import NamedTuple , NewType import pydantic.validators import apischema # Serialization can only be customized into the enclosing models class RGB ( NamedTuple ): red : int green : int blue : int # If you don't put this method, RGB schema will be: # {'title': 'Rgb', 'type': 'array', 'items': {}} @classmethod def __modify_schema__ ( cls , field_schema ) -> None : field_schema . update ({ \"type\" : \"string\" , \"pattern\" : r \"#[0-9A-Fa-f] {6} \" }) field_schema . pop ( \"items\" , ... ) @classmethod def __get_validators__ ( cls ): yield pydantic . validators . str_validator yield cls . validate @classmethod def validate ( cls , value ) -> \"RGB\" : if ( not isinstance ( value , str ) or re . fullmatch ( r \"#[0-9A-Fa-f] {6} \" , value ) is None ): raise ValueError ( \"Invalid RGB\" ) return RGB ( red = int ( value [ 1 : 3 ], 16 ), green = int ( value [ 3 : 5 ], 16 ), blue = int ( value [ 5 : 7 ], 16 ) ) # Simpler with apischema class RGB ( NamedTuple ): red : int green : int blue : int # NewType can be used to add schema to conversion source/target # but Annotated[str, apischema.schema(pattern=r\"#[0-9A-Fa-f]{6}\")] would have worked too HexaRGB = NewType ( \"HexaRGB\" , str ) # pattern is used in JSON schema and in deserialization validation apischema . schema ( pattern = r \"#[0-9A-Fa-f] {6} \" )( HexaRGB ) @apischema . deserializer # could be declared as a staticmethod of RGB class def from_hexa ( hexa : HexaRGB ) -> RGB : return RGB ( int ( hexa [ 1 : 3 ], 16 ), int ( hexa [ 3 : 5 ], 16 ), int ( hexa [ 5 : 7 ], 16 )) @apischema . serializer # could be declared as a method/property of RGB class def to_hexa ( rgb : RGB ) -> HexaRGB : return HexaRGB ( f \"# { rgb . red : 02x }{ rgb . green : 02x }{ rgb . blue : 02x } \" ) assert ( # schema is inherited from deserialized type apischema . json_schema . deserialization_schema ( RGB ) == apischema . json_schema . deserialization_schema ( HexaRGB ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"type\" : \"string\" , \"pattern\" : \"#[0-9A-Fa-f] {6} \" , } )","title":"apischema conversions feature allows to support any type defined in your code, but also in external libraries"},{"location":"difference_with_pydantic/#apischema-can-also-customize-serialization-with-computed-fields","text":"Serialized methods/properties are regular methods/properties which are included in serialization effortlessly.","title":"apischema can also customize serialization with computed fields"},{"location":"difference_with_pydantic/#apischema-allows-you-to-use-composition-over-inheritance","text":"Flattened fields is a distinctive apischema feature that is very handy to build complex model from smaller fragments; you don't have to merge the fields of your fragments in a complex class with a lot of fields yourself, apischema deal with it for you, and your code is kept simple.","title":"apischema allows you to use composition over inheritance"},{"location":"difference_with_pydantic/#apischema-has-a-functional-approach-pydantic-has-an-object-one","text":"pydantic features are based on BaseModel methods. You have to have a BaseModel instance to do anything, even if you manipulate only an integer. Complex pydantic stuff like __root__ model or deserialization customization come from this approach. apischema is functional, it doesn't use method but simple functions, which works with all types. You can also register conversions for any types similarly you would implement a type class in a functional language. And your class namespace don't mix up with a mandatory base class' one.","title":"apischema has a functional approach, pydantic has an object one"},{"location":"difference_with_pydantic/#apischema-can-use-both-camelcase-and-snake_case-with-the-same-types","text":"While pydantic field aliases are fixed at model creation, apischema lets you choose which aliasing you want at (de)serialization time. It can be convenient if you need to juggle with cases for the same models between frontends and other backend services for example.","title":"apischema can use both camelCase and snake_case with the same types"},{"location":"difference_with_pydantic/#apischema-doesnt-coerce-by-default","text":"Your API respects its schema. It can also coerce, for example to parse configuration file, and coercion can be adjusted (for example coercing list from comma-separated string).","title":"apischema doesn't coerce by default"},{"location":"difference_with_pydantic/#apischema-has-a-better-integration-of-json-schemaopenapi","text":"With pydantic , if you want to have a nullable field in the generated schema, you have to put nullable into schema extra keywords. apischema is bound to the last JSON schema version but offers conversion to other version like OpenAPI 3.0 and nullable is added for Optional types. apischema also supports more advanced features like dependentRequired or unevaluatedProperties . Reference handling is also more flexible","title":"apischema has a better integration of JSON schema/OpenAPI"},{"location":"difference_with_pydantic/#apischema-can-add-validators-and-json-schema-to-newtype","text":"So it will be used in deserialization validation. You can use NewType everywhere, to gain a better type checking, self-documented code.","title":"apischema can add validators and JSON schema to NewType"},{"location":"difference_with_pydantic/#apischema-validators-are-regular-methods-with-automatic-dependencies-management","text":"Using regular methods allows benefiting of type checking of fields, where pydantic validators use dynamic stuff (name of the fields as strings) and are not type-checked or have to get redundant type annotations. apischema validators also have automatic dependency management. And apischema directly supports JSON schema property dependencies . Comparison is simple with an example (validator is taken from pydantic documentation : from dataclasses import dataclass import pydantic import apischema class UserModel ( pydantic . BaseModel ): username : str password1 : str password2 : str @pydantic . root_validator def check_passwords_match ( cls , values ): # This is a classmethod (it needs a plugin to not raise a warning in your IDE) # What is the type of of values? of values['password1']? # If you rename password1 field, validator will hardly be updated # You also have to test yourself that values are provided pw1 , pw2 = values . get ( \"password1\" ), values . get ( \"password2\" ) if pw1 is not None and pw2 is not None and pw1 != pw2 : raise ValueError ( \"passwords do not match\" ) return values @dataclass class LoginForm : username : str password1 : str password2 : str @apischema . validator def check_password_match ( self ): # Typed checked, simpler, and not executed if password1 or password2 # are missing/invalid if self . password1 != self . password2 : raise ValueError ( \"passwords do not match\" )","title":"apischema validators are regular methods with automatic dependencies management"},{"location":"difference_with_pydantic/#apischema-supports-pydantic","text":"It's not a feature, is just the result of 30 lines of code .","title":"apischema supports pydantic"},{"location":"json_schema/","text":"JSON schema \u00b6 JSON schema generation \u00b6 JSON schema can be generated from data model. However, because of all possible customizations , the schema can differ between deserilialization and serialization. In common cases, deserialization_schema and serialization_schema will give the same result. from dataclasses import dataclass from apischema.json_schema import deserialization_schema , serialization_schema @dataclass class Foo : bar : str assert deserialization_schema ( Foo ) == serialization_schema ( Foo ) assert deserialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"additionalProperties\" : False , \"properties\" : { \"bar\" : { \"type\" : \"string\" }}, \"required\" : [ \"bar\" ], \"type\" : \"object\" , } Field alias \u00b6 Sometimes dataclass field names can clash with a language keyword, sometimes the property name is not convenient. Hopefully, field can define an alias which will be used in schema and deserialization/serialization. from dataclasses import dataclass , field from apischema import alias , deserialize , serialize from apischema.json_schema import deserialization_schema @dataclass class Foo : class_ : str = field ( metadata = alias ( \"class\" )) assert deserialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"additionalProperties\" : False , \"properties\" : { \"class\" : { \"type\" : \"string\" }}, \"required\" : [ \"class\" ], \"type\" : \"object\" , } assert deserialize ( Foo , { \"class\" : \"bar\" }) == Foo ( \"bar\" ) assert serialize ( Foo , Foo ( \"bar\" )) == { \"class\" : \"bar\" } Alias all fields \u00b6 Field aliasing can also be done at class level by specifying an aliasing function. This aliaser is applied to field alias if defined or field name, or not applied if override=False is specified. from dataclasses import dataclass , field from typing import Any from apischema import alias from apischema.json_schema import deserialization_schema @alias ( lambda s : f \"foo_ { s } \" ) @dataclass class Foo : field1 : Any field2 : Any = field ( metadata = alias ( override = False )) field3 : Any = field ( metadata = alias ( \"field03\" )) field4 : Any = field ( metadata = alias ( \"field04\" , override = False )) assert deserialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"additionalProperties\" : False , \"properties\" : { \"foo_field1\" : {}, \"field2\" : {}, \"foo_field03\" : {}, \"field04\" : {}}, \"required\" : [ \"foo_field1\" , \"field2\" , \"foo_field03\" , \"field04\" ], \"type\" : \"object\" , } Class-level aliasing can be used to define a camelCase API. Dynamic aliasing and default aliaser \u00b6 apischema operations deserialize / serialize / deserialization_schema / serialization_schema provide an aliaser parameter which will be applied on every fields being processed in this operation. Similar to strictness configuration , this parameter has a default value controlled by apischema.settings.aliaser . It can be used for example to make all an application use camelCase . Actually, there is a shortcut for that: Otherwise, it's used the same way than settings.coercer . from apischema import settings settings . camel_case = True Note Dynamic aliaser ignores override=False Schema annotations \u00b6 Type annotations are not enough to express a complete schema, but apischema has a function for that; schema can be used both as type decorator or field metadata. from dataclasses import dataclass , field from typing import NewType from apischema import schema from apischema.json_schema import deserialization_schema Tag = NewType ( \"Tag\" , str ) schema ( min_len = 3 , pattern = r \"^\\w*$\" , examples = [ \"available\" , \"EMEA\" ])( Tag ) @dataclass class Resource : id : int tags : list [ Tag ] = field ( default_factory = list , metadata = schema ( description = \"regroup multiple resources\" , max_items = 3 , unique = True ), ) assert deserialization_schema ( Resource ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"additionalProperties\" : False , \"properties\" : { \"id\" : { \"type\" : \"integer\" }, \"tags\" : { \"description\" : \"regroup multiple resources\" , \"items\" : { \"examples\" : [ \"available\" , \"EMEA\" ], \"minLength\" : 3 , \"pattern\" : \"^ \\\\ w*$\" , \"type\" : \"string\" , }, \"maxItems\" : 3 , \"type\" : \"array\" , \"uniqueItems\" : True , \"default\" : [], }, }, \"required\" : [ \"id\" ], \"type\" : \"object\" , } Note Schema are particularly useful with NewType . For example, if you use prefixed ids, you can use a NewType with a pattern schema to validate them, and benefit of more precise type checking. The following keys are available (they are sometimes shorten compared to JSON schema original for code concision and snake_case): Key JSON schema keyword type restriction title / / description / / default / / examples / / min minimum int max maximum int exc_min exclusiveMinimum int exc_max exclusiveMaximum int mult_of multipleOf int format / str media_type contentMediaType str encoding contentEncoding str min_len minLength str max_len maxLength str pattern / str min_items minItems list max_items maxItems list unique / list min_props minProperties dict max_props maxProperties dict Note In case of field schema, field default value will be serialized (if possible) to add default keyword to the schema. Constraints validation \u00b6 JSON schema constrains the data deserialized; these constraints are naturally used for validation. from dataclasses import dataclass , field from typing import NewType import pytest from apischema import ValidationError , deserialize , schema Tag = NewType ( \"Tag\" , str ) schema ( min_len = 3 , pattern = r \"^\\w*$\" , examples = [ \"available\" , \"EMEA\" ])( Tag ) @dataclass class Resource : id : int tags : list [ Tag ] = field ( default_factory = list , metadata = schema ( description = \"regroup multiple resources\" , max_items = 3 , unique = True ), ) with pytest . raises ( ValidationError ) as err : # pytest check exception is raised deserialize ( Resource , { \"id\" : 42 , \"tags\" : [ \"tag\" , \"duplicate\" , \"duplicate\" , \"bad&\" , \"_\" ]} ) assert err . value . errors == [ { \"loc\" : [ \"tags\" ], \"err\" : \"item count greater than 3 (maxItems)\" }, { \"loc\" : [ \"tags\" ], \"err\" : \"duplicate items (uniqueItems)\" }, { \"loc\" : [ \"tags\" , 3 ], \"err\" : \"not matching pattern ^ \\\\ w*$ (pattern)\" }, { \"loc\" : [ \"tags\" , 4 ], \"err\" : \"string length lower than 3 (minLength)\" }, ] Note Error message are fully customizable Extra schema \u00b6 schema has two other arguments: extra and override , which give a finer control of the JSON schema generated: extra and override . It can be used for example to build \"strict\" unions (using oneOf instead of anyOf ) from dataclasses import dataclass from typing import Annotated , Any from apischema import schema from apischema.json_schema import deserialization_schema # schema extra can be callable to modify the schema in place def to_one_of ( schema : dict [ str , Any ]): if \"anyOf\" in schema : schema [ \"oneOf\" ] = schema . pop ( \"anyOf\" ) OneOf = schema ( extra = to_one_of ) # or extra can be a dictionary which will update the schema @schema ( extra = { \"$ref\" : \"http://some-domain.org/path/to/schema.json#/$defs/Foo\" }, override = True , # override apischema generated schema, using only extra ) @dataclass class Foo : bar : int # Use Annotated with OneOf to make a \"strict\" Union assert deserialization_schema ( Annotated [ Foo | int , OneOf ]) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"oneOf\" : [ # oneOf instead of anyOf { \"$ref\" : \"http://some-domain.org/path/to/schema.json#/$defs/Foo\" }, { \"type\" : \"integer\" }, ], } Base schema \u00b6 apischema.settings.base_schema can be used to define \"base schema\" for the different kind of objects: types, object fields or (serialized) methods. from dataclasses import dataclass , field from typing import Any , Callable , get_origin import docstring_parser from apischema import schema , serialized , settings from apischema.json_schema import serialization_schema from apischema.schemas import Schema from apischema.type_names import get_type_name @dataclass class Foo : \"\"\"Foo class :var bar: bar attribute\"\"\" bar : str = field ( metadata = schema ( max_len = 10 )) @serialized @property def baz ( self ) -> int : \"\"\"baz method\"\"\" ... def type_base_schema ( tp : Any ) -> Schema | None : if not hasattr ( tp , \"__doc__\" ): return None return schema ( title = get_type_name ( tp ) . json_schema , description = docstring_parser . parse ( tp . __doc__ ) . short_description , ) def field_base_schema ( tp : Any , name : str , alias : str ) -> Schema | None : title = alias . replace ( \"_\" , \" \" ) . capitalize () tp = get_origin ( tp ) or tp # tp can be generic for meta in docstring_parser . parse ( tp . __doc__ ) . meta : if meta . args == [ \"var\" , name ]: return schema ( title = title , description = meta . description ) return schema ( title = title ) def method_base_schema ( tp : Any , method : Callable , alias : str ) -> Schema | None : return schema ( title = alias . replace ( \"_\" , \" \" ) . capitalize (), description = docstring_parser . parse ( method . __doc__ ) . short_description , ) settings . base_schema . type = type_base_schema settings . base_schema . field = field_base_schema settings . base_schema . method = method_base_schema assert serialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"additionalProperties\" : False , \"title\" : \"Foo\" , \"description\" : \"Foo class\" , \"properties\" : { \"bar\" : { \"description\" : \"bar attribute\" , \"title\" : \"Bar\" , \"type\" : \"string\" , \"maxLength\" : 10 , }, \"baz\" : { \"description\" : \"baz method\" , \"title\" : \"Baz\" , \"type\" : \"integer\" }, }, \"required\" : [ \"bar\" , \"baz\" ], \"type\" : \"object\" , } Base schema will be merged with schema defined at type/field/method level. Required field with default value \u00b6 By default, a dataclass/namedtuple field will be tagged required if it doesn't have a default value. However, you may want to have a default value for a field in order to be more convenient in your code, but still make the field required. One could think about some schema model where version is fixed but is required, for example JSON-RPC with \"jsonrpc\": \"2.0\" . That's done with field metadata required . from dataclasses import dataclass , field import pytest from apischema import ValidationError , deserialize from apischema.metadata import required @dataclass class Foo : bar : int | None = field ( default = None , metadata = required ) with pytest . raises ( ValidationError ) as err : deserialize ( Foo , {}) assert err . value . errors == [{ \"loc\" : [ \"bar\" ], \"err\" : \"missing property\" }] Additional properties / pattern properties \u00b6 With Mapping \u00b6 Schema of a Mapping / dict type is naturally translated to \"additionalProperties\": <schema of the value type> . However when the schema of the key has a pattern , it will give a \"patternProperties\": {<key pattern>: <schema of the value type>} With dataclass \u00b6 additionalProperties / patternProperties can be added to dataclasses by using fields annotated with properties metadata. Properties not mapped on regular fields will be deserialized into this fields; they must have a Mapping type, or be deserializable from a Mapping , because they are instantiated with a mapping. from collections.abc import Mapping from dataclasses import dataclass , field from typing import Annotated from apischema import deserialize , properties , schema from apischema.json_schema import deserialization_schema @dataclass class Config : active : bool = True server_options : Mapping [ str , bool ] = field ( default_factory = dict , metadata = properties ( pattern = r \"^server_\" ) ) client_options : Mapping [ Annotated [ str , schema ( pattern = r \"^client_\" )], bool # noqa: F722 ] = field ( default_factory = dict , metadata = properties ( ... )) options : Mapping [ str , bool ] = field ( default_factory = dict , metadata = properties ) assert deserialize ( Config , { \"use_lightsaber\" : True , \"server_auto_restart\" : False , \"client_timeout\" : False }, ) == Config ( True , { \"server_auto_restart\" : False }, { \"client_timeout\" : False }, { \"use_lightsaber\" : True }, ) assert deserialization_schema ( Config ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"type\" : \"object\" , \"properties\" : { \"active\" : { \"type\" : \"boolean\" , \"default\" : True }}, \"additionalProperties\" : { \"type\" : \"boolean\" }, \"patternProperties\" : { \"^server_\" : { \"type\" : \"boolean\" }, \"^client_\" : { \"type\" : \"boolean\" }, }, } Note Of course, a dataclass can only have a single properties field without pattern, because it makes no sens to have several additionalProperties . Property dependencies \u00b6 apischema supports property dependencies for dataclass through a class member. Dependencies are also used in validation. from dataclasses import dataclass , field import pytest from apischema import ( Undefined , UndefinedType , ValidationError , dependent_required , deserialize , ) from apischema.json_schema import deserialization_schema @dataclass class Billing : name : str # Fields used in dependencies MUST be declared with `field` credit_card : int | UndefinedType = field ( default = Undefined ) billing_address : str | UndefinedType = field ( default = Undefined ) dependencies = dependent_required ({ credit_card : [ billing_address ]}) # it can also be done outside the class with # dependent_required({\"credit_card\": [\"billing_address\"]}, owner=Billing) assert deserialization_schema ( Billing ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"additionalProperties\" : False , \"dependentRequired\" : { \"credit_card\" : [ \"billing_address\" ]}, \"properties\" : { \"name\" : { \"type\" : \"string\" }, \"credit_card\" : { \"type\" : \"integer\" }, \"billing_address\" : { \"type\" : \"string\" }, }, \"required\" : [ \"name\" ], \"type\" : \"object\" , } with pytest . raises ( ValidationError ) as err : deserialize ( Billing , { \"name\" : \"Anonymous\" , \"credit_card\" : 1234_5678_9012_3456 }) assert err . value . errors == [ { \"loc\" : [ \"billing_address\" ], \"err\" : \"missing property (required by ['credit_card'])\" , } ] Because bidirectional dependencies are a common idiom, apischema provides a shortcut notation; it's indeed possible to write dependent_required([credit_card, billing_adress]) . JSON schema reference \u00b6 For complex schema with type reuse, it's convenient to extract definitions of schema components in order to reuse them \u2014 it's even mandatory for recursive types; JSON schema use JSON pointers \"$ref\" to refer to the definitions. apischema handles this feature natively. from dataclasses import dataclass from typing import Optional from apischema.json_schema import deserialization_schema @dataclass class Node : value : int child : Optional [ \"Node\" ] = None assert deserialization_schema ( Node ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"$ref\" : \"#/$defs/Node\" , \"$defs\" : { \"Node\" : { \"type\" : \"object\" , \"properties\" : { \"value\" : { \"type\" : \"integer\" }, \"child\" : { \"anyOf\" : [{ \"$ref\" : \"#/$defs/Node\" }, { \"type\" : \"null\" }], \"default\" : None , }, }, \"required\" : [ \"value\" ], \"additionalProperties\" : False , } }, } Use reference only for reused types \u00b6 apischema can control the reference use through the boolean all_ref parameter of deserialization_schema / serialization_schema : all_refs=True -> all types with a reference will be put in the definitions and referenced with $ref ; all_refs=False -> only types which are reused in the schema are put in definitions all_refs default value depends on the JSON schema version : it's False for JSON schema drafts but True for OpenAPI. from dataclasses import dataclass from apischema.json_schema import deserialization_schema @dataclass class Bar : baz : str @dataclass class Foo : bar1 : Bar bar2 : Bar assert deserialization_schema ( Foo , all_refs = False ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"$defs\" : { \"Bar\" : { \"additionalProperties\" : False , \"properties\" : { \"baz\" : { \"type\" : \"string\" }}, \"required\" : [ \"baz\" ], \"type\" : \"object\" , } }, \"additionalProperties\" : False , \"properties\" : { \"bar1\" : { \"$ref\" : \"#/$defs/Bar\" }, \"bar2\" : { \"$ref\" : \"#/$defs/Bar\" }}, \"required\" : [ \"bar1\" , \"bar2\" ], \"type\" : \"object\" , } assert deserialization_schema ( Foo , all_refs = True ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"$defs\" : { \"Bar\" : { \"additionalProperties\" : False , \"properties\" : { \"baz\" : { \"type\" : \"string\" }}, \"required\" : [ \"baz\" ], \"type\" : \"object\" , }, \"Foo\" : { \"additionalProperties\" : False , \"properties\" : { \"bar1\" : { \"$ref\" : \"#/$defs/Bar\" }, \"bar2\" : { \"$ref\" : \"#/$defs/Bar\" }, }, \"required\" : [ \"bar1\" , \"bar2\" ], \"type\" : \"object\" , }, }, \"$ref\" : \"#/$defs/Foo\" , } Set reference name \u00b6 In the previous examples, types were referenced using their name. This is indeed the default behavior for every classes/ NewType s (except primitive int / str / bool / float ). It's possible to override the default reference name using apischema.type_name ; passing None instead of a string will remove the reference, making the type unable to be referenced as a separate definition in the schema. from dataclasses import dataclass from typing import Annotated from apischema import type_name from apischema.json_schema import deserialization_schema # Type name can be added as a decorator @type_name ( \"Resource\" ) @dataclass class BaseResource : id : int # or using typing.Annotated tags : Annotated [ set [ str ], type_name ( \"ResourceTags\" )] assert deserialization_schema ( BaseResource , all_refs = True ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"$defs\" : { \"Resource\" : { \"type\" : \"object\" , \"properties\" : { \"id\" : { \"type\" : \"integer\" }, \"tags\" : { \"$ref\" : \"#/$defs/ResourceTags\" }, }, \"required\" : [ \"id\" , \"tags\" ], \"additionalProperties\" : False , }, \"ResourceTags\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" }, \"uniqueItems\" : True , }, }, \"$ref\" : \"#/$defs/Resource\" , } Note Builtin collections are interchangeable when a type_name is registered. For example, if a name is registered for list[Foo] , this name will also be used for Sequence[Foo] or Collection[Foo] . Generic aliases can have a type name, but they need to be specialized; Foo[T, int] cannot have a type name but Foo[str, int] can. However, generic classes can get a dynamic type name depending on their generic argument, passing a name factory to type_name : from dataclasses import dataclass , field from typing import Generic , TypeVar from apischema import type_name from apischema.json_schema import deserialization_schema from apischema.metadata import flatten T = TypeVar ( \"T\" ) # Type name factory takes the type and its arguments as (positional) parameters @type_name ( lambda tp , arg : f \" { arg . __name__ } Resource\" ) @dataclass class Resource ( Generic [ T ]): id : int content : T = field ( metadata = flatten ) ... @dataclass class Foo : bar : str assert deserialization_schema ( Resource [ Foo ], all_refs = True ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"$ref\" : \"#/$defs/FooResource\" , \"$defs\" : { \"FooResource\" : { \"allOf\" : [ { \"type\" : \"object\" , \"properties\" : { \"id\" : { \"type\" : \"integer\" }}, \"required\" : [ \"id\" ], \"additionalProperties\" : False , }, { \"$ref\" : \"#/$defs/Foo\" }, ], \"unevaluatedProperties\" : False , }, \"Foo\" : { \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : \"string\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , }, }, } The default behavior can also be customized using apischema.settings.default_type_name : Reference factory \u00b6 In JSON schema, $ref looks like #/$defs/Foo , not just Foo . In fact, schema generation use the ref given by type_name / default_type_name and pass it to a ref_factory function (a parameter of schema generation functions) which will convert it to its final form. JSON schema version comes with its default ref_factory , for draft 2020-12, it prefixes the ref with #/$defs/ , while it prefixes with #/components/schema in case of OpenAPI. from dataclasses import dataclass from apischema.json_schema import deserialization_schema @dataclass class Foo : bar : int def ref_factory ( ref : str ) -> str : return f \"http://some-domain.org/path/to/ { ref } .json#\" assert deserialization_schema ( Foo , all_refs = True , ref_factory = ref_factory ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"$ref\" : \"http://some-domain.org/path/to/Foo.json#\" , } Note When ref_factory is passed in arguments, definitions are not added to the generated schema. That's because ref_factory would surely change definitions location, so there would be no interest to add them with a wrong location. These definitions can of course be generated separately with definitions_schema . Definitions schema \u00b6 Definitions schemas can also be extracted using apischema.json_schema.definitions_schema . It takes two lists deserialization / serialization of types (or tuple of type + dynamic conversion ) and returns a dictionary of all referenced schemas. Note This is especially useful when it comes to OpenAPI schema to generate the components section. from dataclasses import dataclass from apischema.json_schema import definitions_schema @dataclass class Bar : baz : int = 0 @dataclass class Foo : bar : Bar assert definitions_schema ( deserialization = [ list [ Foo ]], all_refs = True ) == { \"Foo\" : { \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"$ref\" : \"#/$defs/Bar\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , }, \"Bar\" : { \"type\" : \"object\" , \"properties\" : { \"baz\" : { \"type\" : \"integer\" , \"default\" : 0 }}, \"additionalProperties\" : False , }, } JSON schema / OpenAPI version \u00b6 JSON schema has several versions \u2014 OpenAPI is treated as a JSON schema version. If apischema natively use the last one: draft 2020-12, it is possible to specify a schema version which will be used for the generation. from dataclasses import dataclass from typing import Literal from apischema.json_schema import ( JsonSchemaVersion , definitions_schema , deserialization_schema , ) @dataclass class Bar : baz : int | None constant : Literal [ 0 ] = 0 @dataclass class Foo : bar : Bar assert deserialization_schema ( Foo , all_refs = True ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"$ref\" : \"#/$defs/Foo\" , \"$defs\" : { \"Foo\" : { \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"$ref\" : \"#/$defs/Bar\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , }, \"Bar\" : { \"type\" : \"object\" , \"properties\" : { \"baz\" : { \"type\" : [ \"integer\" , \"null\" ]}, \"constant\" : { \"type\" : \"integer\" , \"const\" : 0 , \"default\" : 0 }, }, \"required\" : [ \"baz\" ], \"additionalProperties\" : False , }, }, } assert deserialization_schema ( Foo , all_refs = True , version = JsonSchemaVersion . DRAFT_7 ) == { \"$schema\" : \"http://json-schema.org/draft-07/schema#\" , # $ref is isolated in allOf + draft 7 prefix \"allOf\" : [{ \"$ref\" : \"#/definitions/Foo\" }], \"definitions\" : { # not \"$defs\" \"Foo\" : { \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"$ref\" : \"#/definitions/Bar\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , }, \"Bar\" : { \"type\" : \"object\" , \"properties\" : { \"baz\" : { \"type\" : [ \"integer\" , \"null\" ]}, \"constant\" : { \"type\" : \"integer\" , \"const\" : 0 , \"default\" : 0 }, }, \"required\" : [ \"baz\" ], \"additionalProperties\" : False , }, }, } assert deserialization_schema ( Foo , version = JsonSchemaVersion . OPEN_API_3_1 ) == { # No definitions for OpenAPI, use definitions_schema for it \"$ref\" : \"#/components/schemas/Foo\" # OpenAPI prefix } assert definitions_schema ( deserialization = [ Foo ], version = JsonSchemaVersion . OPEN_API_3_1 ) == { \"Foo\" : { \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"$ref\" : \"#/components/schemas/Bar\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , }, \"Bar\" : { \"type\" : \"object\" , \"properties\" : { \"baz\" : { \"type\" : [ \"integer\" , \"null\" ]}, \"constant\" : { \"type\" : \"integer\" , \"const\" : 0 , \"default\" : 0 }, }, \"required\" : [ \"baz\" ], \"additionalProperties\" : False , }, } assert definitions_schema ( deserialization = [ Foo ], version = JsonSchemaVersion . OPEN_API_3_0 ) == { \"Foo\" : { \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"$ref\" : \"#/components/schemas/Bar\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , }, \"Bar\" : { \"type\" : \"object\" , # \"nullable\" instead of \"type\": \"null\" \"properties\" : { \"baz\" : { \"type\" : \"integer\" , \"nullable\" : True }, \"constant\" : { \"type\" : \"integer\" , \"enum\" : [ 0 ], \"default\" : 0 }, }, \"required\" : [ \"baz\" ], \"additionalProperties\" : False , }, } OpenAPI Discriminator \u00b6 OpenAPI defines a discriminator object which can be used to shortcut deserialization of union of object types. apischema provides two different ways to declare a discriminator: as an Annotated metadata of a union ; from dataclasses import dataclass from typing import Annotated import pytest from apischema import ValidationError , deserialize , discriminator , serialize from apischema.json_schema import deserialization_schema @dataclass class Cat : pass @dataclass class Dog : pass @dataclass class Lizard : pass Pet = Annotated [ Cat | Dog | Lizard , discriminator ( \"type\" , { \"dog\" : Dog })] assert deserialize ( Pet , { \"type\" : \"dog\" }) == Dog () assert deserialize ( Pet , { \"type\" : \"Cat\" }) == Cat () assert serialize ( Pet , Dog ()) == { \"type\" : \"dog\" } with pytest . raises ( ValidationError ) as err : assert deserialize ( Pet , { \"type\" : \"not a pet\" }) assert err . value . errors == [ { \"loc\" : [ \"type\" ], \"err\" : \"not one of ['dog', 'Cat', 'Lizard'] (oneOf)\" } ] assert deserialization_schema ( Pet ) == { \"oneOf\" : [ { \"$ref\" : \"#/$defs/Cat\" }, { \"$ref\" : \"#/$defs/Dog\" }, { \"$ref\" : \"#/$defs/Lizard\" }, ], \"discriminator\" : { \"propertyName\" : \"type\" , \"mapping\" : { \"dog\" : \"#/$defs/Dog\" }}, \"$defs\" : { \"Dog\" : { \"type\" : \"object\" , \"additionalProperties\" : False }, \"Cat\" : { \"type\" : \"object\" , \"additionalProperties\" : False }, \"Lizard\" : { \"type\" : \"object\" , \"additionalProperties\" : False }, }, \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , } as a decorator of base class. from dataclasses import dataclass from apischema import deserialize , discriminator , serialize from apischema.json_schema import deserialization_schema @discriminator ( \"type\" ) class Pet : pass @dataclass class Cat ( Pet ): pass @dataclass class Dog ( Pet ): pass data = { \"type\" : \"Dog\" } assert deserialize ( Pet , data ) == deserialize ( Cat | Dog , data ) == Dog () assert serialize ( Pet , Dog ()), serialize ( Cat | Dog , Dog ()) == data assert ( deserialization_schema ( Pet ) == deserialization_schema ( Cat | Dog ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"oneOf\" : [{ \"$ref\" : \"#/$defs/Cat\" }, { \"$ref\" : \"#/$defs/Dog\" }], \"$defs\" : { \"Pet\" : { \"type\" : \"object\" , \"required\" : [ \"type\" ], \"properties\" : { \"type\" : { \"type\" : \"string\" }}, \"discriminator\" : { \"propertyName\" : \"type\" }, }, \"Cat\" : { \"allOf\" : [{ \"$ref\" : \"#/$defs/Pet\" }, { \"type\" : \"object\" }]}, \"Dog\" : { \"allOf\" : [{ \"$ref\" : \"#/$defs/Pet\" }, { \"type\" : \"object\" }]}, }, } ) Note Using discriminator doesn't require to have a dedicated field (except for TypedDict ) Performance of union deserialization can be improved using discriminator. readOnly / writeOnly \u00b6 Dataclasses InitVar and field(init=False) fields will be flagged respectively with \"writeOnly\": true and \"readOnly\": true in the generated schema. In definitions schema , if a type appears both in deserialization and serialization, properties are merged and the resulting schema contains then readOnly and writeOnly properties. By the way, the required is not merged because it can't (it would mess up validation if some not-init field was required), so deserialization required is kept because it's more important as it can be used in validation (OpenAPI 3.0 semantic which allows the merge has been dropped in 3.1, so it has not been judged useful to be supported)","title":"JSON schema"},{"location":"json_schema/#json-schema","text":"","title":"JSON schema"},{"location":"json_schema/#json-schema-generation","text":"JSON schema can be generated from data model. However, because of all possible customizations , the schema can differ between deserilialization and serialization. In common cases, deserialization_schema and serialization_schema will give the same result. from dataclasses import dataclass from apischema.json_schema import deserialization_schema , serialization_schema @dataclass class Foo : bar : str assert deserialization_schema ( Foo ) == serialization_schema ( Foo ) assert deserialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"additionalProperties\" : False , \"properties\" : { \"bar\" : { \"type\" : \"string\" }}, \"required\" : [ \"bar\" ], \"type\" : \"object\" , }","title":"JSON schema generation"},{"location":"json_schema/#field-alias","text":"Sometimes dataclass field names can clash with a language keyword, sometimes the property name is not convenient. Hopefully, field can define an alias which will be used in schema and deserialization/serialization. from dataclasses import dataclass , field from apischema import alias , deserialize , serialize from apischema.json_schema import deserialization_schema @dataclass class Foo : class_ : str = field ( metadata = alias ( \"class\" )) assert deserialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"additionalProperties\" : False , \"properties\" : { \"class\" : { \"type\" : \"string\" }}, \"required\" : [ \"class\" ], \"type\" : \"object\" , } assert deserialize ( Foo , { \"class\" : \"bar\" }) == Foo ( \"bar\" ) assert serialize ( Foo , Foo ( \"bar\" )) == { \"class\" : \"bar\" }","title":"Field alias"},{"location":"json_schema/#alias-all-fields","text":"Field aliasing can also be done at class level by specifying an aliasing function. This aliaser is applied to field alias if defined or field name, or not applied if override=False is specified. from dataclasses import dataclass , field from typing import Any from apischema import alias from apischema.json_schema import deserialization_schema @alias ( lambda s : f \"foo_ { s } \" ) @dataclass class Foo : field1 : Any field2 : Any = field ( metadata = alias ( override = False )) field3 : Any = field ( metadata = alias ( \"field03\" )) field4 : Any = field ( metadata = alias ( \"field04\" , override = False )) assert deserialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"additionalProperties\" : False , \"properties\" : { \"foo_field1\" : {}, \"field2\" : {}, \"foo_field03\" : {}, \"field04\" : {}}, \"required\" : [ \"foo_field1\" , \"field2\" , \"foo_field03\" , \"field04\" ], \"type\" : \"object\" , } Class-level aliasing can be used to define a camelCase API.","title":"Alias all fields"},{"location":"json_schema/#dynamic-aliasing-and-default-aliaser","text":"apischema operations deserialize / serialize / deserialization_schema / serialization_schema provide an aliaser parameter which will be applied on every fields being processed in this operation. Similar to strictness configuration , this parameter has a default value controlled by apischema.settings.aliaser . It can be used for example to make all an application use camelCase . Actually, there is a shortcut for that: Otherwise, it's used the same way than settings.coercer . from apischema import settings settings . camel_case = True Note Dynamic aliaser ignores override=False","title":"Dynamic aliasing and default aliaser"},{"location":"json_schema/#schema-annotations","text":"Type annotations are not enough to express a complete schema, but apischema has a function for that; schema can be used both as type decorator or field metadata. from dataclasses import dataclass , field from typing import NewType from apischema import schema from apischema.json_schema import deserialization_schema Tag = NewType ( \"Tag\" , str ) schema ( min_len = 3 , pattern = r \"^\\w*$\" , examples = [ \"available\" , \"EMEA\" ])( Tag ) @dataclass class Resource : id : int tags : list [ Tag ] = field ( default_factory = list , metadata = schema ( description = \"regroup multiple resources\" , max_items = 3 , unique = True ), ) assert deserialization_schema ( Resource ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"additionalProperties\" : False , \"properties\" : { \"id\" : { \"type\" : \"integer\" }, \"tags\" : { \"description\" : \"regroup multiple resources\" , \"items\" : { \"examples\" : [ \"available\" , \"EMEA\" ], \"minLength\" : 3 , \"pattern\" : \"^ \\\\ w*$\" , \"type\" : \"string\" , }, \"maxItems\" : 3 , \"type\" : \"array\" , \"uniqueItems\" : True , \"default\" : [], }, }, \"required\" : [ \"id\" ], \"type\" : \"object\" , } Note Schema are particularly useful with NewType . For example, if you use prefixed ids, you can use a NewType with a pattern schema to validate them, and benefit of more precise type checking. The following keys are available (they are sometimes shorten compared to JSON schema original for code concision and snake_case): Key JSON schema keyword type restriction title / / description / / default / / examples / / min minimum int max maximum int exc_min exclusiveMinimum int exc_max exclusiveMaximum int mult_of multipleOf int format / str media_type contentMediaType str encoding contentEncoding str min_len minLength str max_len maxLength str pattern / str min_items minItems list max_items maxItems list unique / list min_props minProperties dict max_props maxProperties dict Note In case of field schema, field default value will be serialized (if possible) to add default keyword to the schema.","title":"Schema annotations"},{"location":"json_schema/#constraints-validation","text":"JSON schema constrains the data deserialized; these constraints are naturally used for validation. from dataclasses import dataclass , field from typing import NewType import pytest from apischema import ValidationError , deserialize , schema Tag = NewType ( \"Tag\" , str ) schema ( min_len = 3 , pattern = r \"^\\w*$\" , examples = [ \"available\" , \"EMEA\" ])( Tag ) @dataclass class Resource : id : int tags : list [ Tag ] = field ( default_factory = list , metadata = schema ( description = \"regroup multiple resources\" , max_items = 3 , unique = True ), ) with pytest . raises ( ValidationError ) as err : # pytest check exception is raised deserialize ( Resource , { \"id\" : 42 , \"tags\" : [ \"tag\" , \"duplicate\" , \"duplicate\" , \"bad&\" , \"_\" ]} ) assert err . value . errors == [ { \"loc\" : [ \"tags\" ], \"err\" : \"item count greater than 3 (maxItems)\" }, { \"loc\" : [ \"tags\" ], \"err\" : \"duplicate items (uniqueItems)\" }, { \"loc\" : [ \"tags\" , 3 ], \"err\" : \"not matching pattern ^ \\\\ w*$ (pattern)\" }, { \"loc\" : [ \"tags\" , 4 ], \"err\" : \"string length lower than 3 (minLength)\" }, ] Note Error message are fully customizable","title":"Constraints validation"},{"location":"json_schema/#extra-schema","text":"schema has two other arguments: extra and override , which give a finer control of the JSON schema generated: extra and override . It can be used for example to build \"strict\" unions (using oneOf instead of anyOf ) from dataclasses import dataclass from typing import Annotated , Any from apischema import schema from apischema.json_schema import deserialization_schema # schema extra can be callable to modify the schema in place def to_one_of ( schema : dict [ str , Any ]): if \"anyOf\" in schema : schema [ \"oneOf\" ] = schema . pop ( \"anyOf\" ) OneOf = schema ( extra = to_one_of ) # or extra can be a dictionary which will update the schema @schema ( extra = { \"$ref\" : \"http://some-domain.org/path/to/schema.json#/$defs/Foo\" }, override = True , # override apischema generated schema, using only extra ) @dataclass class Foo : bar : int # Use Annotated with OneOf to make a \"strict\" Union assert deserialization_schema ( Annotated [ Foo | int , OneOf ]) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"oneOf\" : [ # oneOf instead of anyOf { \"$ref\" : \"http://some-domain.org/path/to/schema.json#/$defs/Foo\" }, { \"type\" : \"integer\" }, ], }","title":"Extra schema"},{"location":"json_schema/#base-schema","text":"apischema.settings.base_schema can be used to define \"base schema\" for the different kind of objects: types, object fields or (serialized) methods. from dataclasses import dataclass , field from typing import Any , Callable , get_origin import docstring_parser from apischema import schema , serialized , settings from apischema.json_schema import serialization_schema from apischema.schemas import Schema from apischema.type_names import get_type_name @dataclass class Foo : \"\"\"Foo class :var bar: bar attribute\"\"\" bar : str = field ( metadata = schema ( max_len = 10 )) @serialized @property def baz ( self ) -> int : \"\"\"baz method\"\"\" ... def type_base_schema ( tp : Any ) -> Schema | None : if not hasattr ( tp , \"__doc__\" ): return None return schema ( title = get_type_name ( tp ) . json_schema , description = docstring_parser . parse ( tp . __doc__ ) . short_description , ) def field_base_schema ( tp : Any , name : str , alias : str ) -> Schema | None : title = alias . replace ( \"_\" , \" \" ) . capitalize () tp = get_origin ( tp ) or tp # tp can be generic for meta in docstring_parser . parse ( tp . __doc__ ) . meta : if meta . args == [ \"var\" , name ]: return schema ( title = title , description = meta . description ) return schema ( title = title ) def method_base_schema ( tp : Any , method : Callable , alias : str ) -> Schema | None : return schema ( title = alias . replace ( \"_\" , \" \" ) . capitalize (), description = docstring_parser . parse ( method . __doc__ ) . short_description , ) settings . base_schema . type = type_base_schema settings . base_schema . field = field_base_schema settings . base_schema . method = method_base_schema assert serialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"additionalProperties\" : False , \"title\" : \"Foo\" , \"description\" : \"Foo class\" , \"properties\" : { \"bar\" : { \"description\" : \"bar attribute\" , \"title\" : \"Bar\" , \"type\" : \"string\" , \"maxLength\" : 10 , }, \"baz\" : { \"description\" : \"baz method\" , \"title\" : \"Baz\" , \"type\" : \"integer\" }, }, \"required\" : [ \"bar\" , \"baz\" ], \"type\" : \"object\" , } Base schema will be merged with schema defined at type/field/method level.","title":"Base schema"},{"location":"json_schema/#required-field-with-default-value","text":"By default, a dataclass/namedtuple field will be tagged required if it doesn't have a default value. However, you may want to have a default value for a field in order to be more convenient in your code, but still make the field required. One could think about some schema model where version is fixed but is required, for example JSON-RPC with \"jsonrpc\": \"2.0\" . That's done with field metadata required . from dataclasses import dataclass , field import pytest from apischema import ValidationError , deserialize from apischema.metadata import required @dataclass class Foo : bar : int | None = field ( default = None , metadata = required ) with pytest . raises ( ValidationError ) as err : deserialize ( Foo , {}) assert err . value . errors == [{ \"loc\" : [ \"bar\" ], \"err\" : \"missing property\" }]","title":"Required field with default value"},{"location":"json_schema/#additional-properties-pattern-properties","text":"","title":"Additional properties / pattern properties"},{"location":"json_schema/#with-mapping","text":"Schema of a Mapping / dict type is naturally translated to \"additionalProperties\": <schema of the value type> . However when the schema of the key has a pattern , it will give a \"patternProperties\": {<key pattern>: <schema of the value type>}","title":"With Mapping"},{"location":"json_schema/#with-dataclass","text":"additionalProperties / patternProperties can be added to dataclasses by using fields annotated with properties metadata. Properties not mapped on regular fields will be deserialized into this fields; they must have a Mapping type, or be deserializable from a Mapping , because they are instantiated with a mapping. from collections.abc import Mapping from dataclasses import dataclass , field from typing import Annotated from apischema import deserialize , properties , schema from apischema.json_schema import deserialization_schema @dataclass class Config : active : bool = True server_options : Mapping [ str , bool ] = field ( default_factory = dict , metadata = properties ( pattern = r \"^server_\" ) ) client_options : Mapping [ Annotated [ str , schema ( pattern = r \"^client_\" )], bool # noqa: F722 ] = field ( default_factory = dict , metadata = properties ( ... )) options : Mapping [ str , bool ] = field ( default_factory = dict , metadata = properties ) assert deserialize ( Config , { \"use_lightsaber\" : True , \"server_auto_restart\" : False , \"client_timeout\" : False }, ) == Config ( True , { \"server_auto_restart\" : False }, { \"client_timeout\" : False }, { \"use_lightsaber\" : True }, ) assert deserialization_schema ( Config ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"type\" : \"object\" , \"properties\" : { \"active\" : { \"type\" : \"boolean\" , \"default\" : True }}, \"additionalProperties\" : { \"type\" : \"boolean\" }, \"patternProperties\" : { \"^server_\" : { \"type\" : \"boolean\" }, \"^client_\" : { \"type\" : \"boolean\" }, }, } Note Of course, a dataclass can only have a single properties field without pattern, because it makes no sens to have several additionalProperties .","title":"With dataclass"},{"location":"json_schema/#property-dependencies","text":"apischema supports property dependencies for dataclass through a class member. Dependencies are also used in validation. from dataclasses import dataclass , field import pytest from apischema import ( Undefined , UndefinedType , ValidationError , dependent_required , deserialize , ) from apischema.json_schema import deserialization_schema @dataclass class Billing : name : str # Fields used in dependencies MUST be declared with `field` credit_card : int | UndefinedType = field ( default = Undefined ) billing_address : str | UndefinedType = field ( default = Undefined ) dependencies = dependent_required ({ credit_card : [ billing_address ]}) # it can also be done outside the class with # dependent_required({\"credit_card\": [\"billing_address\"]}, owner=Billing) assert deserialization_schema ( Billing ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"additionalProperties\" : False , \"dependentRequired\" : { \"credit_card\" : [ \"billing_address\" ]}, \"properties\" : { \"name\" : { \"type\" : \"string\" }, \"credit_card\" : { \"type\" : \"integer\" }, \"billing_address\" : { \"type\" : \"string\" }, }, \"required\" : [ \"name\" ], \"type\" : \"object\" , } with pytest . raises ( ValidationError ) as err : deserialize ( Billing , { \"name\" : \"Anonymous\" , \"credit_card\" : 1234_5678_9012_3456 }) assert err . value . errors == [ { \"loc\" : [ \"billing_address\" ], \"err\" : \"missing property (required by ['credit_card'])\" , } ] Because bidirectional dependencies are a common idiom, apischema provides a shortcut notation; it's indeed possible to write dependent_required([credit_card, billing_adress]) .","title":"Property dependencies"},{"location":"json_schema/#json-schema-reference","text":"For complex schema with type reuse, it's convenient to extract definitions of schema components in order to reuse them \u2014 it's even mandatory for recursive types; JSON schema use JSON pointers \"$ref\" to refer to the definitions. apischema handles this feature natively. from dataclasses import dataclass from typing import Optional from apischema.json_schema import deserialization_schema @dataclass class Node : value : int child : Optional [ \"Node\" ] = None assert deserialization_schema ( Node ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"$ref\" : \"#/$defs/Node\" , \"$defs\" : { \"Node\" : { \"type\" : \"object\" , \"properties\" : { \"value\" : { \"type\" : \"integer\" }, \"child\" : { \"anyOf\" : [{ \"$ref\" : \"#/$defs/Node\" }, { \"type\" : \"null\" }], \"default\" : None , }, }, \"required\" : [ \"value\" ], \"additionalProperties\" : False , } }, }","title":"JSON schema reference"},{"location":"json_schema/#use-reference-only-for-reused-types","text":"apischema can control the reference use through the boolean all_ref parameter of deserialization_schema / serialization_schema : all_refs=True -> all types with a reference will be put in the definitions and referenced with $ref ; all_refs=False -> only types which are reused in the schema are put in definitions all_refs default value depends on the JSON schema version : it's False for JSON schema drafts but True for OpenAPI. from dataclasses import dataclass from apischema.json_schema import deserialization_schema @dataclass class Bar : baz : str @dataclass class Foo : bar1 : Bar bar2 : Bar assert deserialization_schema ( Foo , all_refs = False ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"$defs\" : { \"Bar\" : { \"additionalProperties\" : False , \"properties\" : { \"baz\" : { \"type\" : \"string\" }}, \"required\" : [ \"baz\" ], \"type\" : \"object\" , } }, \"additionalProperties\" : False , \"properties\" : { \"bar1\" : { \"$ref\" : \"#/$defs/Bar\" }, \"bar2\" : { \"$ref\" : \"#/$defs/Bar\" }}, \"required\" : [ \"bar1\" , \"bar2\" ], \"type\" : \"object\" , } assert deserialization_schema ( Foo , all_refs = True ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"$defs\" : { \"Bar\" : { \"additionalProperties\" : False , \"properties\" : { \"baz\" : { \"type\" : \"string\" }}, \"required\" : [ \"baz\" ], \"type\" : \"object\" , }, \"Foo\" : { \"additionalProperties\" : False , \"properties\" : { \"bar1\" : { \"$ref\" : \"#/$defs/Bar\" }, \"bar2\" : { \"$ref\" : \"#/$defs/Bar\" }, }, \"required\" : [ \"bar1\" , \"bar2\" ], \"type\" : \"object\" , }, }, \"$ref\" : \"#/$defs/Foo\" , }","title":"Use reference only for reused types"},{"location":"json_schema/#set-reference-name","text":"In the previous examples, types were referenced using their name. This is indeed the default behavior for every classes/ NewType s (except primitive int / str / bool / float ). It's possible to override the default reference name using apischema.type_name ; passing None instead of a string will remove the reference, making the type unable to be referenced as a separate definition in the schema. from dataclasses import dataclass from typing import Annotated from apischema import type_name from apischema.json_schema import deserialization_schema # Type name can be added as a decorator @type_name ( \"Resource\" ) @dataclass class BaseResource : id : int # or using typing.Annotated tags : Annotated [ set [ str ], type_name ( \"ResourceTags\" )] assert deserialization_schema ( BaseResource , all_refs = True ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"$defs\" : { \"Resource\" : { \"type\" : \"object\" , \"properties\" : { \"id\" : { \"type\" : \"integer\" }, \"tags\" : { \"$ref\" : \"#/$defs/ResourceTags\" }, }, \"required\" : [ \"id\" , \"tags\" ], \"additionalProperties\" : False , }, \"ResourceTags\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" }, \"uniqueItems\" : True , }, }, \"$ref\" : \"#/$defs/Resource\" , } Note Builtin collections are interchangeable when a type_name is registered. For example, if a name is registered for list[Foo] , this name will also be used for Sequence[Foo] or Collection[Foo] . Generic aliases can have a type name, but they need to be specialized; Foo[T, int] cannot have a type name but Foo[str, int] can. However, generic classes can get a dynamic type name depending on their generic argument, passing a name factory to type_name : from dataclasses import dataclass , field from typing import Generic , TypeVar from apischema import type_name from apischema.json_schema import deserialization_schema from apischema.metadata import flatten T = TypeVar ( \"T\" ) # Type name factory takes the type and its arguments as (positional) parameters @type_name ( lambda tp , arg : f \" { arg . __name__ } Resource\" ) @dataclass class Resource ( Generic [ T ]): id : int content : T = field ( metadata = flatten ) ... @dataclass class Foo : bar : str assert deserialization_schema ( Resource [ Foo ], all_refs = True ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"$ref\" : \"#/$defs/FooResource\" , \"$defs\" : { \"FooResource\" : { \"allOf\" : [ { \"type\" : \"object\" , \"properties\" : { \"id\" : { \"type\" : \"integer\" }}, \"required\" : [ \"id\" ], \"additionalProperties\" : False , }, { \"$ref\" : \"#/$defs/Foo\" }, ], \"unevaluatedProperties\" : False , }, \"Foo\" : { \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : \"string\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , }, }, } The default behavior can also be customized using apischema.settings.default_type_name :","title":"Set reference name"},{"location":"json_schema/#reference-factory","text":"In JSON schema, $ref looks like #/$defs/Foo , not just Foo . In fact, schema generation use the ref given by type_name / default_type_name and pass it to a ref_factory function (a parameter of schema generation functions) which will convert it to its final form. JSON schema version comes with its default ref_factory , for draft 2020-12, it prefixes the ref with #/$defs/ , while it prefixes with #/components/schema in case of OpenAPI. from dataclasses import dataclass from apischema.json_schema import deserialization_schema @dataclass class Foo : bar : int def ref_factory ( ref : str ) -> str : return f \"http://some-domain.org/path/to/ { ref } .json#\" assert deserialization_schema ( Foo , all_refs = True , ref_factory = ref_factory ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"$ref\" : \"http://some-domain.org/path/to/Foo.json#\" , } Note When ref_factory is passed in arguments, definitions are not added to the generated schema. That's because ref_factory would surely change definitions location, so there would be no interest to add them with a wrong location. These definitions can of course be generated separately with definitions_schema .","title":"Reference factory"},{"location":"json_schema/#definitions-schema","text":"Definitions schemas can also be extracted using apischema.json_schema.definitions_schema . It takes two lists deserialization / serialization of types (or tuple of type + dynamic conversion ) and returns a dictionary of all referenced schemas. Note This is especially useful when it comes to OpenAPI schema to generate the components section. from dataclasses import dataclass from apischema.json_schema import definitions_schema @dataclass class Bar : baz : int = 0 @dataclass class Foo : bar : Bar assert definitions_schema ( deserialization = [ list [ Foo ]], all_refs = True ) == { \"Foo\" : { \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"$ref\" : \"#/$defs/Bar\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , }, \"Bar\" : { \"type\" : \"object\" , \"properties\" : { \"baz\" : { \"type\" : \"integer\" , \"default\" : 0 }}, \"additionalProperties\" : False , }, }","title":"Definitions schema"},{"location":"json_schema/#json-schema-openapi-version","text":"JSON schema has several versions \u2014 OpenAPI is treated as a JSON schema version. If apischema natively use the last one: draft 2020-12, it is possible to specify a schema version which will be used for the generation. from dataclasses import dataclass from typing import Literal from apischema.json_schema import ( JsonSchemaVersion , definitions_schema , deserialization_schema , ) @dataclass class Bar : baz : int | None constant : Literal [ 0 ] = 0 @dataclass class Foo : bar : Bar assert deserialization_schema ( Foo , all_refs = True ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"$ref\" : \"#/$defs/Foo\" , \"$defs\" : { \"Foo\" : { \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"$ref\" : \"#/$defs/Bar\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , }, \"Bar\" : { \"type\" : \"object\" , \"properties\" : { \"baz\" : { \"type\" : [ \"integer\" , \"null\" ]}, \"constant\" : { \"type\" : \"integer\" , \"const\" : 0 , \"default\" : 0 }, }, \"required\" : [ \"baz\" ], \"additionalProperties\" : False , }, }, } assert deserialization_schema ( Foo , all_refs = True , version = JsonSchemaVersion . DRAFT_7 ) == { \"$schema\" : \"http://json-schema.org/draft-07/schema#\" , # $ref is isolated in allOf + draft 7 prefix \"allOf\" : [{ \"$ref\" : \"#/definitions/Foo\" }], \"definitions\" : { # not \"$defs\" \"Foo\" : { \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"$ref\" : \"#/definitions/Bar\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , }, \"Bar\" : { \"type\" : \"object\" , \"properties\" : { \"baz\" : { \"type\" : [ \"integer\" , \"null\" ]}, \"constant\" : { \"type\" : \"integer\" , \"const\" : 0 , \"default\" : 0 }, }, \"required\" : [ \"baz\" ], \"additionalProperties\" : False , }, }, } assert deserialization_schema ( Foo , version = JsonSchemaVersion . OPEN_API_3_1 ) == { # No definitions for OpenAPI, use definitions_schema for it \"$ref\" : \"#/components/schemas/Foo\" # OpenAPI prefix } assert definitions_schema ( deserialization = [ Foo ], version = JsonSchemaVersion . OPEN_API_3_1 ) == { \"Foo\" : { \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"$ref\" : \"#/components/schemas/Bar\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , }, \"Bar\" : { \"type\" : \"object\" , \"properties\" : { \"baz\" : { \"type\" : [ \"integer\" , \"null\" ]}, \"constant\" : { \"type\" : \"integer\" , \"const\" : 0 , \"default\" : 0 }, }, \"required\" : [ \"baz\" ], \"additionalProperties\" : False , }, } assert definitions_schema ( deserialization = [ Foo ], version = JsonSchemaVersion . OPEN_API_3_0 ) == { \"Foo\" : { \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"$ref\" : \"#/components/schemas/Bar\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , }, \"Bar\" : { \"type\" : \"object\" , # \"nullable\" instead of \"type\": \"null\" \"properties\" : { \"baz\" : { \"type\" : \"integer\" , \"nullable\" : True }, \"constant\" : { \"type\" : \"integer\" , \"enum\" : [ 0 ], \"default\" : 0 }, }, \"required\" : [ \"baz\" ], \"additionalProperties\" : False , }, }","title":"JSON schema / OpenAPI version"},{"location":"json_schema/#openapi-discriminator","text":"OpenAPI defines a discriminator object which can be used to shortcut deserialization of union of object types. apischema provides two different ways to declare a discriminator: as an Annotated metadata of a union ; from dataclasses import dataclass from typing import Annotated import pytest from apischema import ValidationError , deserialize , discriminator , serialize from apischema.json_schema import deserialization_schema @dataclass class Cat : pass @dataclass class Dog : pass @dataclass class Lizard : pass Pet = Annotated [ Cat | Dog | Lizard , discriminator ( \"type\" , { \"dog\" : Dog })] assert deserialize ( Pet , { \"type\" : \"dog\" }) == Dog () assert deserialize ( Pet , { \"type\" : \"Cat\" }) == Cat () assert serialize ( Pet , Dog ()) == { \"type\" : \"dog\" } with pytest . raises ( ValidationError ) as err : assert deserialize ( Pet , { \"type\" : \"not a pet\" }) assert err . value . errors == [ { \"loc\" : [ \"type\" ], \"err\" : \"not one of ['dog', 'Cat', 'Lizard'] (oneOf)\" } ] assert deserialization_schema ( Pet ) == { \"oneOf\" : [ { \"$ref\" : \"#/$defs/Cat\" }, { \"$ref\" : \"#/$defs/Dog\" }, { \"$ref\" : \"#/$defs/Lizard\" }, ], \"discriminator\" : { \"propertyName\" : \"type\" , \"mapping\" : { \"dog\" : \"#/$defs/Dog\" }}, \"$defs\" : { \"Dog\" : { \"type\" : \"object\" , \"additionalProperties\" : False }, \"Cat\" : { \"type\" : \"object\" , \"additionalProperties\" : False }, \"Lizard\" : { \"type\" : \"object\" , \"additionalProperties\" : False }, }, \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , } as a decorator of base class. from dataclasses import dataclass from apischema import deserialize , discriminator , serialize from apischema.json_schema import deserialization_schema @discriminator ( \"type\" ) class Pet : pass @dataclass class Cat ( Pet ): pass @dataclass class Dog ( Pet ): pass data = { \"type\" : \"Dog\" } assert deserialize ( Pet , data ) == deserialize ( Cat | Dog , data ) == Dog () assert serialize ( Pet , Dog ()), serialize ( Cat | Dog , Dog ()) == data assert ( deserialization_schema ( Pet ) == deserialization_schema ( Cat | Dog ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"oneOf\" : [{ \"$ref\" : \"#/$defs/Cat\" }, { \"$ref\" : \"#/$defs/Dog\" }], \"$defs\" : { \"Pet\" : { \"type\" : \"object\" , \"required\" : [ \"type\" ], \"properties\" : { \"type\" : { \"type\" : \"string\" }}, \"discriminator\" : { \"propertyName\" : \"type\" }, }, \"Cat\" : { \"allOf\" : [{ \"$ref\" : \"#/$defs/Pet\" }, { \"type\" : \"object\" }]}, \"Dog\" : { \"allOf\" : [{ \"$ref\" : \"#/$defs/Pet\" }, { \"type\" : \"object\" }]}, }, } ) Note Using discriminator doesn't require to have a dedicated field (except for TypedDict ) Performance of union deserialization can be improved using discriminator.","title":"OpenAPI Discriminator"},{"location":"json_schema/#readonly-writeonly","text":"Dataclasses InitVar and field(init=False) fields will be flagged respectively with \"writeOnly\": true and \"readOnly\": true in the generated schema. In definitions schema , if a type appears both in deserialization and serialization, properties are merged and the resulting schema contains then readOnly and writeOnly properties. By the way, the required is not merged because it can't (it would mess up validation if some not-init field was required), so deserialization required is kept because it's more important as it can be used in validation (OpenAPI 3.0 semantic which allows the merge has been dropped in 3.1, so it has not been judged useful to be supported)","title":"readOnly / writeOnly"},{"location":"optimizations_and_benchmark/","text":"Optimizations and benchmark \u00b6 apischema is (a lot) faster than its known alternatives, thanks to advanced optimizations. Note Chart is truncated to a relative performance of 20x slower. Benchmark results are detailed in the results table . Precomputed (de)serialization methods \u00b6 apischema precomputes (de)serialization methods depending on the (de)serialized type (and other parameters); type annotations processing is done in the precomputation. Methods are then cached using functools.lru_cache , so deserialize and serialize don't recompute them every time. Note The cache is automatically reset when global settings are modified, because it impacts the generated methods. However, if lru_cache is fast, using the methods directly is faster, so apischema provides apischema.deserialization_method and apischema.serialization_method . These functions share the same parameters than deserialize / serialize , except the data/object parameter to (de)serialize. Using the computed methods directly can increase performances by 10%. from dataclasses import dataclass from apischema import deserialization_method , serialization_method @dataclass class Foo : bar : int deserialize_foo = deserialization_method ( Foo ) serialize_foo = serialization_method ( Foo ) assert deserialize_foo ({ \"bar\" : 0 }) == Foo ( 0 ) assert serialize_foo ( Foo ( 0 )) == { \"bar\" : 0 } Warning Methods computed before settings modification will not be updated and use the old settings. Be careful to set your settings first. Avoid unnecessary copies \u00b6 As an example, when a list of integers is deserialized, json.load already return a list of integers. The loaded data can thus be \"reused\", and the deserialization just become a validation step. The same principle applies to serialization. It's controlled by the settings apischema.settings.deserialization.no_copy / apischema.settings.serialization.no_copy , or no_copy parameter of deserialize / serialize methods. Default behavior is to avoid these unnecessary copies, i.e. no_copy=False . from timeit import timeit from apischema import deserialize ints = list ( range ( 100 )) assert deserialize ( list [ int ], ints , no_copy = True ) is ints # default assert deserialize ( list [ int ], ints , no_copy = False ) is not ints print ( timeit ( \"deserialize(list[int], ints, no_copy=True)\" , globals = globals ())) # 8.596703557006549 print ( timeit ( \"deserialize(list[int], ints, no_copy=False)\" , globals = globals ())) # 9.365363762015477 Serialization passthrough \u00b6 JSON serialization libraries expect primitive data types ( dict / list / str /etc.). A non-negligible part of objects to be serialized are primitive. When type checking is disabled (this is default), objects annotated with primitive types doesn't need to be transformed or checked; apischema can simply \"pass through\" them, and it will result into an identity serialization method, just returning its argument. Container types like list or dict are passed through only when the contained types are passed through too (and when no_copy=True ) from apischema import serialize ints = list ( range ( 5 )) assert serialize ( list [ int ], ints ) is ints Note Enum subclasses which also inherit str / int are also passed through Passthrough options \u00b6 Some JSON serialization libraries natively support types like UUID or datetime , sometimes with a faster implementation than the apischema one \u2014 orjson , written in Rust, is a good example. To take advantage of that, apischema provides apischema.PassThroughOptions class to specify which type should be passed through, whether they are supported natively by JSON libraries (or handled in a default fallback). apischema.serialization_default can be used as default fallback in combination to PassThroughOptions . It has to be instantiated with the same kwargs parameters ( aliaser , etc.) than serialization_method . from collections.abc import Collection from uuid import UUID , uuid4 from apischema import PassThroughOptions , serialization_method uuids_method = serialization_method ( Collection [ UUID ], pass_through = PassThroughOptions ( collections = True , types = { UUID }) ) uuids = [ uuid4 () for _ in range ( 5 )] assert uuids_method ( uuids ) is uuids Important Passthrough optimization is a lot diminished with check_type=False . PassThroughOptions has the following parameters: any \u2014 pass through Any \u00b6 collections \u2014 pass through collections \u00b6 Standard collections list , tuple and dict are natively handled by JSON libraries, but set , for example, isn't. Moreover, standard abstract collections like Collection or Mapping , which are used a lot, are not guaranteed to have their runtime type supported (having a set annotated with Collection for instance). But, most of the time, collections runtime types are list / dict , so others can be handled in default fallback. Note Set-like type will not be passed through. dataclasses - pass through dataclasses \u00b6 Some JSON libraries, like orjson , support dataclasses natively. However, because apischema has a lot of specific features ( aliasing , flatten fields , conditional skipping , fields ordering , etc.), only dataclasses with none of these features, and only passed through fields, will be passed through too. enums \u2014 pass through enum.Enum subclasses \u00b6 tuple \u2014 pass through tuple \u00b6 Even if tuple is often supported by JSON serializers, if this options is not enabled, tuples will be serialized as lists. It also allows easier test writing for example. Note collections=True implies tuple=True ; types \u2014 pass through arbitrary types \u00b6 Either a collection of types, or a predicate to determine if type has to be passed through. Binary compilation using Cython \u00b6 apischema use Cython in order to compile critical parts of the code, i.e. the (de)serialization methods. However, apischema remains a pure Python library \u2014 it can work without binary modules. Cython source files ( .pyx ) are in fact generated from Python modules. It allows notably keeping the code simple, by adding switch-case optimization to replace dynamic dispatch, avoiding big chains of elif in Python code. Note Compilation is disabled when using PyPy, because it's even faster with the bare Python code. That's another interest of generating .pyx files: keeping Python source for PyPy. Override dataclass constructors \u00b6 Warning This feature is still experimental and disabled by default. Test carefully its impact on your code before enable it in production. Dataclass constructors calls is the slowest part of the deserialization, about 50% of its runtime! They are indeed pure Python functions and cannot be compiled. In case of \"normal\" dataclass (no __slots__ , __post_init__ , or __init__ / __new__ / __setattr__ overriding), apischema can override the constructor with a compilable code. This feature can be toggled on/off globally using apischema.settings.deserialization.override_dataclass_constructors Discriminator \u00b6 OpenAPI discriminator allows making union deserialization time more homogeneous. from dataclasses import dataclass from timeit import timeit from typing import Annotated from apischema import deserialization_method , discriminator @dataclass class Cat : love_dog : bool = False @dataclass class Dog : love_cat : bool = False deserialize_union = deserialization_method ( Cat | Dog ) deserialize_discriminated = deserialization_method ( Annotated [ Cat | Dog , discriminator ( \"type\" )] ) ##### Without discrimininator print ( timeit ( 'deserialize_union({\"love_dog\": False})' , globals = globals ())) # Cat: 0.760085788 print ( timeit ( 'deserialize_union({\"love_cat\": False})' , globals = globals ())) # Dog: 3.078876515 \u2248 x4 ##### With discriminator print ( timeit ( 'deserialize_discriminated({\"type\": \"Cat\"})' , globals = globals ())) # Cat: 1.244204702 print ( timeit ( 'deserialize_discriminated({\"type\": \"Dog\"})' , globals = globals ())) # Dog: 1.234058598 \u2248 same Note As you can notice in the example, discriminator brings its own additional cost, but it's completely worth it. Benchmark \u00b6 Benchmark code is located benchmark directory or apischema repository. Performances are measured on two datasets: a simple, a more complex one. Benchmark is run by Github Actions workflow on ubuntu-latest with Python 3.10. Results are given relatively to the fastest library, i.e. apischema ; simple and complex results are detailed in the table, displayed result is the mean of both. Relative execution time (lower is better) \u00b6 library version deserialization serialization pydantic 2.0.3 / / apischema 0.18.0 x1.2 (0.8/1.6) x0.6 (0.5/0.8) Note Benchmark use binary optimization , but even running as a pure Python library, apischema still performs better than almost all of the competitors.","title":"Optimizations and benchmark"},{"location":"optimizations_and_benchmark/#optimizations-and-benchmark","text":"apischema is (a lot) faster than its known alternatives, thanks to advanced optimizations. Note Chart is truncated to a relative performance of 20x slower. Benchmark results are detailed in the results table .","title":"Optimizations and benchmark"},{"location":"optimizations_and_benchmark/#precomputed-deserialization-methods","text":"apischema precomputes (de)serialization methods depending on the (de)serialized type (and other parameters); type annotations processing is done in the precomputation. Methods are then cached using functools.lru_cache , so deserialize and serialize don't recompute them every time. Note The cache is automatically reset when global settings are modified, because it impacts the generated methods. However, if lru_cache is fast, using the methods directly is faster, so apischema provides apischema.deserialization_method and apischema.serialization_method . These functions share the same parameters than deserialize / serialize , except the data/object parameter to (de)serialize. Using the computed methods directly can increase performances by 10%. from dataclasses import dataclass from apischema import deserialization_method , serialization_method @dataclass class Foo : bar : int deserialize_foo = deserialization_method ( Foo ) serialize_foo = serialization_method ( Foo ) assert deserialize_foo ({ \"bar\" : 0 }) == Foo ( 0 ) assert serialize_foo ( Foo ( 0 )) == { \"bar\" : 0 } Warning Methods computed before settings modification will not be updated and use the old settings. Be careful to set your settings first.","title":"Precomputed (de)serialization methods"},{"location":"optimizations_and_benchmark/#avoid-unnecessary-copies","text":"As an example, when a list of integers is deserialized, json.load already return a list of integers. The loaded data can thus be \"reused\", and the deserialization just become a validation step. The same principle applies to serialization. It's controlled by the settings apischema.settings.deserialization.no_copy / apischema.settings.serialization.no_copy , or no_copy parameter of deserialize / serialize methods. Default behavior is to avoid these unnecessary copies, i.e. no_copy=False . from timeit import timeit from apischema import deserialize ints = list ( range ( 100 )) assert deserialize ( list [ int ], ints , no_copy = True ) is ints # default assert deserialize ( list [ int ], ints , no_copy = False ) is not ints print ( timeit ( \"deserialize(list[int], ints, no_copy=True)\" , globals = globals ())) # 8.596703557006549 print ( timeit ( \"deserialize(list[int], ints, no_copy=False)\" , globals = globals ())) # 9.365363762015477","title":"Avoid unnecessary copies"},{"location":"optimizations_and_benchmark/#serialization-passthrough","text":"JSON serialization libraries expect primitive data types ( dict / list / str /etc.). A non-negligible part of objects to be serialized are primitive. When type checking is disabled (this is default), objects annotated with primitive types doesn't need to be transformed or checked; apischema can simply \"pass through\" them, and it will result into an identity serialization method, just returning its argument. Container types like list or dict are passed through only when the contained types are passed through too (and when no_copy=True ) from apischema import serialize ints = list ( range ( 5 )) assert serialize ( list [ int ], ints ) is ints Note Enum subclasses which also inherit str / int are also passed through","title":"Serialization passthrough"},{"location":"optimizations_and_benchmark/#passthrough-options","text":"Some JSON serialization libraries natively support types like UUID or datetime , sometimes with a faster implementation than the apischema one \u2014 orjson , written in Rust, is a good example. To take advantage of that, apischema provides apischema.PassThroughOptions class to specify which type should be passed through, whether they are supported natively by JSON libraries (or handled in a default fallback). apischema.serialization_default can be used as default fallback in combination to PassThroughOptions . It has to be instantiated with the same kwargs parameters ( aliaser , etc.) than serialization_method . from collections.abc import Collection from uuid import UUID , uuid4 from apischema import PassThroughOptions , serialization_method uuids_method = serialization_method ( Collection [ UUID ], pass_through = PassThroughOptions ( collections = True , types = { UUID }) ) uuids = [ uuid4 () for _ in range ( 5 )] assert uuids_method ( uuids ) is uuids Important Passthrough optimization is a lot diminished with check_type=False . PassThroughOptions has the following parameters:","title":"Passthrough options"},{"location":"optimizations_and_benchmark/#any-pass-through-any","text":"","title":"any \u2014 pass through Any"},{"location":"optimizations_and_benchmark/#collections-pass-through-collections","text":"Standard collections list , tuple and dict are natively handled by JSON libraries, but set , for example, isn't. Moreover, standard abstract collections like Collection or Mapping , which are used a lot, are not guaranteed to have their runtime type supported (having a set annotated with Collection for instance). But, most of the time, collections runtime types are list / dict , so others can be handled in default fallback. Note Set-like type will not be passed through.","title":"collections \u2014 pass through collections"},{"location":"optimizations_and_benchmark/#dataclasses-pass-through-dataclasses","text":"Some JSON libraries, like orjson , support dataclasses natively. However, because apischema has a lot of specific features ( aliasing , flatten fields , conditional skipping , fields ordering , etc.), only dataclasses with none of these features, and only passed through fields, will be passed through too.","title":"dataclasses - pass through dataclasses"},{"location":"optimizations_and_benchmark/#enums-pass-through-enumenum-subclasses","text":"","title":"enums \u2014 pass through enum.Enum subclasses"},{"location":"optimizations_and_benchmark/#tuple-pass-through-tuple","text":"Even if tuple is often supported by JSON serializers, if this options is not enabled, tuples will be serialized as lists. It also allows easier test writing for example. Note collections=True implies tuple=True ;","title":"tuple \u2014 pass through tuple"},{"location":"optimizations_and_benchmark/#types-pass-through-arbitrary-types","text":"Either a collection of types, or a predicate to determine if type has to be passed through.","title":"types \u2014 pass through arbitrary types"},{"location":"optimizations_and_benchmark/#binary-compilation-using-cython","text":"apischema use Cython in order to compile critical parts of the code, i.e. the (de)serialization methods. However, apischema remains a pure Python library \u2014 it can work without binary modules. Cython source files ( .pyx ) are in fact generated from Python modules. It allows notably keeping the code simple, by adding switch-case optimization to replace dynamic dispatch, avoiding big chains of elif in Python code. Note Compilation is disabled when using PyPy, because it's even faster with the bare Python code. That's another interest of generating .pyx files: keeping Python source for PyPy.","title":"Binary compilation using Cython"},{"location":"optimizations_and_benchmark/#override-dataclass-constructors","text":"Warning This feature is still experimental and disabled by default. Test carefully its impact on your code before enable it in production. Dataclass constructors calls is the slowest part of the deserialization, about 50% of its runtime! They are indeed pure Python functions and cannot be compiled. In case of \"normal\" dataclass (no __slots__ , __post_init__ , or __init__ / __new__ / __setattr__ overriding), apischema can override the constructor with a compilable code. This feature can be toggled on/off globally using apischema.settings.deserialization.override_dataclass_constructors","title":"Override dataclass constructors"},{"location":"optimizations_and_benchmark/#discriminator","text":"OpenAPI discriminator allows making union deserialization time more homogeneous. from dataclasses import dataclass from timeit import timeit from typing import Annotated from apischema import deserialization_method , discriminator @dataclass class Cat : love_dog : bool = False @dataclass class Dog : love_cat : bool = False deserialize_union = deserialization_method ( Cat | Dog ) deserialize_discriminated = deserialization_method ( Annotated [ Cat | Dog , discriminator ( \"type\" )] ) ##### Without discrimininator print ( timeit ( 'deserialize_union({\"love_dog\": False})' , globals = globals ())) # Cat: 0.760085788 print ( timeit ( 'deserialize_union({\"love_cat\": False})' , globals = globals ())) # Dog: 3.078876515 \u2248 x4 ##### With discriminator print ( timeit ( 'deserialize_discriminated({\"type\": \"Cat\"})' , globals = globals ())) # Cat: 1.244204702 print ( timeit ( 'deserialize_discriminated({\"type\": \"Dog\"})' , globals = globals ())) # Dog: 1.234058598 \u2248 same Note As you can notice in the example, discriminator brings its own additional cost, but it's completely worth it.","title":"Discriminator"},{"location":"optimizations_and_benchmark/#benchmark","text":"Benchmark code is located benchmark directory or apischema repository. Performances are measured on two datasets: a simple, a more complex one. Benchmark is run by Github Actions workflow on ubuntu-latest with Python 3.10. Results are given relatively to the fastest library, i.e. apischema ; simple and complex results are detailed in the table, displayed result is the mean of both.","title":"Benchmark"},{"location":"optimizations_and_benchmark/#relative-execution-time-lower-is-better","text":"library version deserialization serialization pydantic 2.0.3 / / apischema 0.18.0 x1.2 (0.8/1.6) x0.6 (0.5/0.8) Note Benchmark use binary optimization , but even running as a pure Python library, apischema still performs better than almost all of the competitors.","title":"Relative execution time (lower is better)"},{"location":"validation/","text":"Validation \u00b6 Validation is an important part of deserialization. By default, apischema validates types of data according to typing annotations, and schema constraints. But custom validators can also be add for a more precise validation. Deserialization and validation error \u00b6 ValidationError is raised when validation fails. This exception contains all the information about the ill-formed part of the data. It can be formatted/serialized using its errors property. from dataclasses import dataclass , field from typing import NewType import pytest from apischema import ValidationError , deserialize , schema Tag = NewType ( \"Tag\" , str ) schema ( min_len = 3 , pattern = r \"^\\w*$\" , examples = [ \"available\" , \"EMEA\" ])( Tag ) @dataclass class Resource : id : int tags : list [ Tag ] = field ( default_factory = list , metadata = schema ( description = \"regroup multiple resources\" , max_items = 3 , unique = True ), ) with pytest . raises ( ValidationError ) as err : # pytest check exception is raised deserialize ( Resource , { \"id\" : 42 , \"tags\" : [ \"tag\" , \"duplicate\" , \"duplicate\" , \"bad&\" , \"_\" ]} ) assert err . value . errors == [ { \"loc\" : [ \"tags\" ], \"err\" : \"item count greater than 3 (maxItems)\" }, { \"loc\" : [ \"tags\" ], \"err\" : \"duplicate items (uniqueItems)\" }, { \"loc\" : [ \"tags\" , 3 ], \"err\" : \"not matching pattern ^ \\\\ w*$ (pattern)\" }, { \"loc\" : [ \"tags\" , 4 ], \"err\" : \"string length lower than 3 (minLength)\" }, ] As shown in the example, apischema will not stop at the first error met but tries to validate all parts of the data. Note ValidationError can also be serialized using apischema.serialize (this will use errors internally). Constraint errors customization \u00b6 Constraints are validated at deserialization, with apischema providing default error messages. Messages can be customized by setting the corresponding attribute of apischema.settings.errors . They can be either a string which will be formatted with the constraint value (using str.format ), e.g. less than {} (minimum) , or a function with 2 parameters: the constraint value and the invalid data. import pytest from apischema import ValidationError , deserialize , schema , settings settings . errors . max_items = ( lambda constraint , data : f \"too-many-items: { len ( data ) } > { constraint } \" ) with pytest . raises ( ValidationError ) as err : deserialize ( list [ int ], [ 0 , 1 , 2 , 3 ], schema = schema ( max_items = 3 )) assert err . value . errors == [{ \"loc\" : [], \"err\" : \"too-many-items: 4 > 3\" }] Note Default error messages doesn't include the invalid data for security reason (data could for example be a password too short). Note Other error message can be customized, for example missing property for missing required properties, etc. Dataclass validators \u00b6 Dataclass validation can be completed by custom validators. These are simple decorated methods which will be executed during validation, after all fields having been deserialized. Note Previously to v0.17, validators could raise arbitrary exceptions (except AssertionError of course); see FAQ for the reason of this change. from dataclasses import dataclass import pytest from apischema import ValidationError , deserialize , validator @dataclass class PasswordForm : password : str confirmation : str @validator def password_match ( self ): # DO NOT use assert if self . password != self . confirmation : raise ValidationError ( \"password doesn't match its confirmation\" ) with pytest . raises ( ValidationError ) as err : deserialize ( PasswordForm , { \"password\" : \"p455w0rd\" , \"confirmation\" : \"...\" }) assert err . value . errors == [ { \"loc\" : [], \"err\" : \"password doesn't match its confirmation\" } ] Warning DO NOT use assert statement to validate external data, ever. In fact, this statement is made to be disabled when executed in optimized mode (see documentation ), so validation would be disabled too. This warning doesn't concern only apischema ; assert is only for internal assertion in debug/development environment. That's why apischema will not catch AssertionError as a validation error but reraises it, making deserialize fail. Note Validators are always executed in order of declaration. Automatic dependency management \u00b6 It makes no sense to execute a validator using a field that is ill-formed. Hopefully, apischema is able to compute validator dependencies \u2014 the fields used in validator; validator is executed only if the all its dependencies are ok. from dataclasses import dataclass import pytest from apischema import ValidationError , deserialize , validator @dataclass class PasswordForm : password : str confirmation : str @validator def password_match ( self ): if self . password != self . confirmation : raise ValueError ( \"password doesn't match its confirmation\" ) with pytest . raises ( ValidationError ) as err : deserialize ( PasswordForm , { \"password\" : \"p455w0rd\" }) assert err . value . errors == [ # validator is not executed because confirmation is missing { \"loc\" : [ \"confirmation\" ], \"err\" : \"missing property\" } ] Note Despite the fact that validator uses the self argument, it can be called during validation even if all the fields of the class are not ok and the class not really instantiated. In fact, instance is kind of mocked for validation with only the needed field. Raise more than one error with yield \u00b6 Validation of a list field can require raising several exceptions, one for each bad element. With raise , this is not possible, because you can raise only once. However, apischema provides a way of raising as many errors as needed by using yield . Moreover, with this syntax, it is possible to add a \"path\" (see below ) to the error to precise its location in the validated data. This path will be added to the loc key of the error. from dataclasses import dataclass from ipaddress import IPv4Address , IPv4Network import pytest from apischema import ValidationError , deserialize , validator from apischema.objects import get_alias @dataclass class SubnetIps : subnet : IPv4Network ips : list [ IPv4Address ] @validator def check_ips_in_subnet ( self ): for index , ip in enumerate ( self . ips ): if ip not in self . subnet : # yield <error path>, <error message> yield ( get_alias ( self ) . ips , index ), \"ip not in subnet\" with pytest . raises ( ValidationError ) as err : deserialize ( SubnetIps , { \"subnet\" : \"126.42.18.0/24\" , \"ips\" : [ \"126.42.18.1\" , \"126.42.19.0\" , \"0.0.0.0\" ]}, ) assert err . value . errors == [ { \"loc\" : [ \"ips\" , 1 ], \"err\" : \"ip not in subnet\" }, { \"loc\" : [ \"ips\" , 2 ], \"err\" : \"ip not in subnet\" }, ] Error path \u00b6 In the example, the validator yields a tuple of an \"error path\" and the error message. Error path can be: a field alias (obtained with apischema.objects.get_alias ); an integer, for list indices; a raw string, for dict key (or field); an apischema.objects.AliasedStr , a string subclass which will be aliased by deserialization aliaser; an iterable, e.g. a tuple, of this 4 components. yield can also be used with only an error message. Note For dataclass field error path, it's advised to use apischema.objects.get_alias instead of raw string, because it will take into account potential aliasing and it will be better handled by IDE (refactoring, cross-referencing, etc.) Discard \u00b6 If one of your validators fails because a field is corrupted, maybe you don't want subsequent validators to be executed. validator decorator provides a discard parameter to discard fields of the remaining validation. All the remaining validators having discarded fields in dependencies will not be executed. from dataclasses import dataclass , field import pytest from apischema import ValidationError , deserialize , validator from apischema.objects import get_alias , get_field @dataclass class BoundedValues : # field must be assign to be used, even with empty `field()` bounds : tuple [ int , int ] = field () values : list [ int ] # validator(\"bounds\") would also work, but it's not handled by IDE refactoring, etc. @validator ( discard = bounds ) def bounds_are_sorted ( self ): min_bound , max_bound = self . bounds if min_bound > max_bound : yield get_alias ( self ) . bounds , \"bounds are not sorted\" @validator def values_dont_exceed_bounds ( self ): min_bound , max_bound = self . bounds for index , value in enumerate ( self . values ): if not min_bound <= value <= max_bound : yield ( get_alias ( self ) . values , index ), \"value exceeds bounds\" # Outside class, fields can still be accessed in a \"static\" way, to avoid use raw string @validator ( discard = get_field ( BoundedValues ) . bounds ) def bounds_are_sorted_equivalent ( bounded : BoundedValues ): min_bound , max_bound = bounded . bounds if min_bound > max_bound : yield get_alias ( bounded ) . bounds , \"bounds are not sorted\" with pytest . raises ( ValidationError ) as err : deserialize ( BoundedValues , { \"bounds\" : [ 10 , 0 ], \"values\" : [ - 1 , 2 , 4 ]}) assert err . value . errors == [ { \"loc\" : [ \"bounds\" ], \"err\" : \"bounds are not sorted\" } # Without discard, there would have been an other error # {\"loc\": [\"values\", 1], \"err\": \"value exceeds bounds\"} ] You can notice in this example that apischema tries to avoid using raw strings to identify fields. In every function of the library using fields identifier ( apischema.validator , apischema.dependent_required , apischema.fields.set_fields , etc.), you have always three ways to pass them: - using field object, preferred in dataclass definition; - using apischema.objects.get_field , to be used outside of class definition; it works with NamedTuple too \u2014 the object returned is the apischema internal field representation, common to dataclass , NamedTuple and TypedDict ; - using raw strings, thus not handled by static tools like refactoring, but it works; Field validators \u00b6 At field level \u00b6 Fields are validated according to their types and schema. But it's also possible to add validators to fields. from dataclasses import dataclass , field import pytest from apischema import ValidationError , deserialize from apischema.metadata import validators def check_no_duplicate_digits ( n : int ): if len ( str ( n )) != len ( set ( str ( n ))): raise ValidationError ( \"number has duplicate digits\" ) @dataclass class Foo : bar : str = field ( metadata = validators ( check_no_duplicate_digits )) with pytest . raises ( ValidationError ) as err : deserialize ( Foo , { \"bar\" : \"11\" }) assert err . value . errors == [{ \"loc\" : [ \"bar\" ], \"err\" : \"number has duplicate digits\" }] When validation fails for a field, it is discarded and cannot be used in class validators, as is the case when field schema validation fails. Note field_validator allows reusing the the same validator for several fields. However in this case, using a custom type (for example a NewType ) with validators (see next section ) could often be a better solution. Using other fields \u00b6 A common pattern can be to validate a field using other fields values. This is achieved with dataclass validators seen above. However, there is a shortcut for this use case: from dataclasses import dataclass , field from enum import Enum import pytest from apischema import ValidationError , deserialize , validator from apischema.objects import get_alias , get_field class Parity ( Enum ): EVEN = \"even\" ODD = \"odd\" @dataclass class NumberWithParity : parity : Parity number : int = field () @validator ( number ) def check_parity ( self ): if ( self . parity == Parity . EVEN ) != ( self . number % 2 == 0 ): yield \"number doesn't respect parity\" # A field validator is equivalent to a discard argument and all error paths prefixed # with the field alias @validator ( discard = number ) def check_parity_equivalent ( self ): if ( self . parity == Parity . EVEN ) != ( self . number % 2 == 0 ): yield get_alias ( self ) . number , \"number doesn't respect parity\" @validator ( get_field ( NumberWithParity ) . number ) def check_parity_other_equivalent ( number2 : NumberWithParity ): if ( number2 . parity == Parity . EVEN ) != ( number2 . number % 2 == 0 ): yield \"number doesn't respect parity\" with pytest . raises ( ValidationError ) as err : deserialize ( NumberWithParity , { \"parity\" : \"even\" , \"number\" : 1 }) assert err . value . errors == [{ \"loc\" : [ \"number\" ], \"err\" : \"number doesn't respect parity\" }] Validators inheritance \u00b6 Validators are inherited just like other class fields. from dataclasses import dataclass import pytest from apischema import ValidationError , deserialize , validator @dataclass class PasswordForm : password : str confirmation : str @validator def password_match ( self ): if self . password != self . confirmation : raise ValidationError ( \"password doesn't match its confirmation\" ) @dataclass class CompleteForm ( PasswordForm ): username : str with pytest . raises ( ValidationError ) as err : deserialize ( CompleteForm , { \"username\" : \"wyfo\" , \"password\" : \"p455w0rd\" , \"confirmation\" : \"...\" }, ) assert err . value . errors == [ { \"loc\" : [], \"err\" : \"password doesn't match its confirmation\" } ] Validator with InitVar \u00b6 Dataclasses InitVar are accessible in validators by using parameters the same way __post_init__ does. Only the needed fields have to be put in parameters, they are then added to validator dependencies. from dataclasses import InitVar , dataclass , field import pytest from apischema import ValidationError , deserialize , validator from apischema.metadata import init_var @dataclass class Foo : bar : InitVar [ int ] = field ( metadata = init_var ( int )) @validator ( bar ) def validate ( self , bar : int ): if bar < 0 : yield \"negative\" with pytest . raises ( ValidationError ) as err : deserialize ( Foo , { \"bar\" : - 1 }) assert err . value . errors == [{ \"loc\" : [ \"bar\" ], \"err\" : \"negative\" }] Validators are not run on default values \u00b6 If all validator dependencies are initialized with their default values, they are not run. from dataclasses import dataclass , field from apischema import deserialize , validator validator_run = False @dataclass class Foo : bar : int = field ( default = 0 ) @validator ( bar ) def password_match ( self ): global validator_run validator_run = True if self . bar < 0 : raise ValueError ( \"negative\" ) deserialize ( Foo , {}) assert not validator_run Validators for every type \u00b6 Validators can also be declared as regular functions, in which case annotation of the first param is used to associate it to the validated type (you can also use the owner parameter); this allows adding a validator to every type. Last but not least, validators can be embedded directly into Annotated arguments using validators metadata. from typing import Annotated , NewType import pytest from apischema import ValidationError , deserialize , validator from apischema.metadata import validators Palindrome = NewType ( \"Palindrome\" , str ) @validator # could also use @validator(owner=Palindrome) def check_palindrome ( s : Palindrome ): for i in range ( len ( s ) // 2 ): if s [ i ] != s [ - 1 - i ]: raise ValidationError ( \"Not a palindrome\" ) assert deserialize ( Palindrome , \"tacocat\" ) == \"tacocat\" with pytest . raises ( ValidationError ) as err : deserialize ( Palindrome , \"palindrome\" ) assert err . value . errors == [{ \"loc\" : [], \"err\" : \"Not a palindrome\" }] # Using Annotated with pytest . raises ( ValidationError ) as err : deserialize ( Annotated [ str , validators ( check_palindrome )], \"palindrom\" ) assert err . value . errors == [{ \"loc\" : [], \"err\" : \"Not a palindrome\" }] FAQ \u00b6 How are validator dependencies computed? \u00b6 ast.NodeVisitor and the Python black magic begins... Why only validate at deserialization and not at instantiation? \u00b6 apischema uses type annotations, so every objects used can already be statically type-checked (with Mypy / Pycharm /etc.) at instantiation but also at modification. Why use validators for dataclasses instead of doing validation in __post_init__ ? \u00b6 Actually, validation can completely be done in __post_init__ , there is no problem with that. However, validators offers one thing that cannot be achieved with __post_init__ : they are run before __init__ , so they can validate incomplete data. Moreover, they are only run during deserialization, so they don't add overhead to normal class instantiation. Why validators cannot raise arbitrary exception? \u00b6 Allowing arbitrary exception is in fact a security issue, because unwanted exception could be raised, and their message displayed in validation error. It could either contain sensitive data, or give information about the implementation which could be used to hack it. By the way, it's possible to define a decorator to convert precise exceptions to ValidationError : from collections.abc import Callable from functools import wraps from typing import TypeVar from apischema import ValidationError Func = TypeVar ( \"Func\" , bound = Callable ) def catch ( * exceptions ) -> Callable [[ Func ], Func ]: def decorator ( func : Func ) -> Func : @wraps ( func ) def wrapper ( * args , ** kwargs ): try : return func ( * args , ** kwargs ) except Exception as err : raise ValidationError ( str ( err )) if isinstance ( err , exceptions ) else err return wrapper return decorator","title":"Validation"},{"location":"validation/#validation","text":"Validation is an important part of deserialization. By default, apischema validates types of data according to typing annotations, and schema constraints. But custom validators can also be add for a more precise validation.","title":"Validation"},{"location":"validation/#deserialization-and-validation-error","text":"ValidationError is raised when validation fails. This exception contains all the information about the ill-formed part of the data. It can be formatted/serialized using its errors property. from dataclasses import dataclass , field from typing import NewType import pytest from apischema import ValidationError , deserialize , schema Tag = NewType ( \"Tag\" , str ) schema ( min_len = 3 , pattern = r \"^\\w*$\" , examples = [ \"available\" , \"EMEA\" ])( Tag ) @dataclass class Resource : id : int tags : list [ Tag ] = field ( default_factory = list , metadata = schema ( description = \"regroup multiple resources\" , max_items = 3 , unique = True ), ) with pytest . raises ( ValidationError ) as err : # pytest check exception is raised deserialize ( Resource , { \"id\" : 42 , \"tags\" : [ \"tag\" , \"duplicate\" , \"duplicate\" , \"bad&\" , \"_\" ]} ) assert err . value . errors == [ { \"loc\" : [ \"tags\" ], \"err\" : \"item count greater than 3 (maxItems)\" }, { \"loc\" : [ \"tags\" ], \"err\" : \"duplicate items (uniqueItems)\" }, { \"loc\" : [ \"tags\" , 3 ], \"err\" : \"not matching pattern ^ \\\\ w*$ (pattern)\" }, { \"loc\" : [ \"tags\" , 4 ], \"err\" : \"string length lower than 3 (minLength)\" }, ] As shown in the example, apischema will not stop at the first error met but tries to validate all parts of the data. Note ValidationError can also be serialized using apischema.serialize (this will use errors internally).","title":"Deserialization and validation error"},{"location":"validation/#constraint-errors-customization","text":"Constraints are validated at deserialization, with apischema providing default error messages. Messages can be customized by setting the corresponding attribute of apischema.settings.errors . They can be either a string which will be formatted with the constraint value (using str.format ), e.g. less than {} (minimum) , or a function with 2 parameters: the constraint value and the invalid data. import pytest from apischema import ValidationError , deserialize , schema , settings settings . errors . max_items = ( lambda constraint , data : f \"too-many-items: { len ( data ) } > { constraint } \" ) with pytest . raises ( ValidationError ) as err : deserialize ( list [ int ], [ 0 , 1 , 2 , 3 ], schema = schema ( max_items = 3 )) assert err . value . errors == [{ \"loc\" : [], \"err\" : \"too-many-items: 4 > 3\" }] Note Default error messages doesn't include the invalid data for security reason (data could for example be a password too short). Note Other error message can be customized, for example missing property for missing required properties, etc.","title":"Constraint errors customization"},{"location":"validation/#dataclass-validators","text":"Dataclass validation can be completed by custom validators. These are simple decorated methods which will be executed during validation, after all fields having been deserialized. Note Previously to v0.17, validators could raise arbitrary exceptions (except AssertionError of course); see FAQ for the reason of this change. from dataclasses import dataclass import pytest from apischema import ValidationError , deserialize , validator @dataclass class PasswordForm : password : str confirmation : str @validator def password_match ( self ): # DO NOT use assert if self . password != self . confirmation : raise ValidationError ( \"password doesn't match its confirmation\" ) with pytest . raises ( ValidationError ) as err : deserialize ( PasswordForm , { \"password\" : \"p455w0rd\" , \"confirmation\" : \"...\" }) assert err . value . errors == [ { \"loc\" : [], \"err\" : \"password doesn't match its confirmation\" } ] Warning DO NOT use assert statement to validate external data, ever. In fact, this statement is made to be disabled when executed in optimized mode (see documentation ), so validation would be disabled too. This warning doesn't concern only apischema ; assert is only for internal assertion in debug/development environment. That's why apischema will not catch AssertionError as a validation error but reraises it, making deserialize fail. Note Validators are always executed in order of declaration.","title":"Dataclass validators"},{"location":"validation/#automatic-dependency-management","text":"It makes no sense to execute a validator using a field that is ill-formed. Hopefully, apischema is able to compute validator dependencies \u2014 the fields used in validator; validator is executed only if the all its dependencies are ok. from dataclasses import dataclass import pytest from apischema import ValidationError , deserialize , validator @dataclass class PasswordForm : password : str confirmation : str @validator def password_match ( self ): if self . password != self . confirmation : raise ValueError ( \"password doesn't match its confirmation\" ) with pytest . raises ( ValidationError ) as err : deserialize ( PasswordForm , { \"password\" : \"p455w0rd\" }) assert err . value . errors == [ # validator is not executed because confirmation is missing { \"loc\" : [ \"confirmation\" ], \"err\" : \"missing property\" } ] Note Despite the fact that validator uses the self argument, it can be called during validation even if all the fields of the class are not ok and the class not really instantiated. In fact, instance is kind of mocked for validation with only the needed field.","title":"Automatic dependency management"},{"location":"validation/#raise-more-than-one-error-with-yield","text":"Validation of a list field can require raising several exceptions, one for each bad element. With raise , this is not possible, because you can raise only once. However, apischema provides a way of raising as many errors as needed by using yield . Moreover, with this syntax, it is possible to add a \"path\" (see below ) to the error to precise its location in the validated data. This path will be added to the loc key of the error. from dataclasses import dataclass from ipaddress import IPv4Address , IPv4Network import pytest from apischema import ValidationError , deserialize , validator from apischema.objects import get_alias @dataclass class SubnetIps : subnet : IPv4Network ips : list [ IPv4Address ] @validator def check_ips_in_subnet ( self ): for index , ip in enumerate ( self . ips ): if ip not in self . subnet : # yield <error path>, <error message> yield ( get_alias ( self ) . ips , index ), \"ip not in subnet\" with pytest . raises ( ValidationError ) as err : deserialize ( SubnetIps , { \"subnet\" : \"126.42.18.0/24\" , \"ips\" : [ \"126.42.18.1\" , \"126.42.19.0\" , \"0.0.0.0\" ]}, ) assert err . value . errors == [ { \"loc\" : [ \"ips\" , 1 ], \"err\" : \"ip not in subnet\" }, { \"loc\" : [ \"ips\" , 2 ], \"err\" : \"ip not in subnet\" }, ]","title":"Raise more than one error with yield"},{"location":"validation/#error-path","text":"In the example, the validator yields a tuple of an \"error path\" and the error message. Error path can be: a field alias (obtained with apischema.objects.get_alias ); an integer, for list indices; a raw string, for dict key (or field); an apischema.objects.AliasedStr , a string subclass which will be aliased by deserialization aliaser; an iterable, e.g. a tuple, of this 4 components. yield can also be used with only an error message. Note For dataclass field error path, it's advised to use apischema.objects.get_alias instead of raw string, because it will take into account potential aliasing and it will be better handled by IDE (refactoring, cross-referencing, etc.)","title":"Error path"},{"location":"validation/#discard","text":"If one of your validators fails because a field is corrupted, maybe you don't want subsequent validators to be executed. validator decorator provides a discard parameter to discard fields of the remaining validation. All the remaining validators having discarded fields in dependencies will not be executed. from dataclasses import dataclass , field import pytest from apischema import ValidationError , deserialize , validator from apischema.objects import get_alias , get_field @dataclass class BoundedValues : # field must be assign to be used, even with empty `field()` bounds : tuple [ int , int ] = field () values : list [ int ] # validator(\"bounds\") would also work, but it's not handled by IDE refactoring, etc. @validator ( discard = bounds ) def bounds_are_sorted ( self ): min_bound , max_bound = self . bounds if min_bound > max_bound : yield get_alias ( self ) . bounds , \"bounds are not sorted\" @validator def values_dont_exceed_bounds ( self ): min_bound , max_bound = self . bounds for index , value in enumerate ( self . values ): if not min_bound <= value <= max_bound : yield ( get_alias ( self ) . values , index ), \"value exceeds bounds\" # Outside class, fields can still be accessed in a \"static\" way, to avoid use raw string @validator ( discard = get_field ( BoundedValues ) . bounds ) def bounds_are_sorted_equivalent ( bounded : BoundedValues ): min_bound , max_bound = bounded . bounds if min_bound > max_bound : yield get_alias ( bounded ) . bounds , \"bounds are not sorted\" with pytest . raises ( ValidationError ) as err : deserialize ( BoundedValues , { \"bounds\" : [ 10 , 0 ], \"values\" : [ - 1 , 2 , 4 ]}) assert err . value . errors == [ { \"loc\" : [ \"bounds\" ], \"err\" : \"bounds are not sorted\" } # Without discard, there would have been an other error # {\"loc\": [\"values\", 1], \"err\": \"value exceeds bounds\"} ] You can notice in this example that apischema tries to avoid using raw strings to identify fields. In every function of the library using fields identifier ( apischema.validator , apischema.dependent_required , apischema.fields.set_fields , etc.), you have always three ways to pass them: - using field object, preferred in dataclass definition; - using apischema.objects.get_field , to be used outside of class definition; it works with NamedTuple too \u2014 the object returned is the apischema internal field representation, common to dataclass , NamedTuple and TypedDict ; - using raw strings, thus not handled by static tools like refactoring, but it works;","title":"Discard"},{"location":"validation/#field-validators","text":"","title":"Field validators"},{"location":"validation/#at-field-level","text":"Fields are validated according to their types and schema. But it's also possible to add validators to fields. from dataclasses import dataclass , field import pytest from apischema import ValidationError , deserialize from apischema.metadata import validators def check_no_duplicate_digits ( n : int ): if len ( str ( n )) != len ( set ( str ( n ))): raise ValidationError ( \"number has duplicate digits\" ) @dataclass class Foo : bar : str = field ( metadata = validators ( check_no_duplicate_digits )) with pytest . raises ( ValidationError ) as err : deserialize ( Foo , { \"bar\" : \"11\" }) assert err . value . errors == [{ \"loc\" : [ \"bar\" ], \"err\" : \"number has duplicate digits\" }] When validation fails for a field, it is discarded and cannot be used in class validators, as is the case when field schema validation fails. Note field_validator allows reusing the the same validator for several fields. However in this case, using a custom type (for example a NewType ) with validators (see next section ) could often be a better solution.","title":"At field level"},{"location":"validation/#using-other-fields","text":"A common pattern can be to validate a field using other fields values. This is achieved with dataclass validators seen above. However, there is a shortcut for this use case: from dataclasses import dataclass , field from enum import Enum import pytest from apischema import ValidationError , deserialize , validator from apischema.objects import get_alias , get_field class Parity ( Enum ): EVEN = \"even\" ODD = \"odd\" @dataclass class NumberWithParity : parity : Parity number : int = field () @validator ( number ) def check_parity ( self ): if ( self . parity == Parity . EVEN ) != ( self . number % 2 == 0 ): yield \"number doesn't respect parity\" # A field validator is equivalent to a discard argument and all error paths prefixed # with the field alias @validator ( discard = number ) def check_parity_equivalent ( self ): if ( self . parity == Parity . EVEN ) != ( self . number % 2 == 0 ): yield get_alias ( self ) . number , \"number doesn't respect parity\" @validator ( get_field ( NumberWithParity ) . number ) def check_parity_other_equivalent ( number2 : NumberWithParity ): if ( number2 . parity == Parity . EVEN ) != ( number2 . number % 2 == 0 ): yield \"number doesn't respect parity\" with pytest . raises ( ValidationError ) as err : deserialize ( NumberWithParity , { \"parity\" : \"even\" , \"number\" : 1 }) assert err . value . errors == [{ \"loc\" : [ \"number\" ], \"err\" : \"number doesn't respect parity\" }]","title":"Using other fields"},{"location":"validation/#validators-inheritance","text":"Validators are inherited just like other class fields. from dataclasses import dataclass import pytest from apischema import ValidationError , deserialize , validator @dataclass class PasswordForm : password : str confirmation : str @validator def password_match ( self ): if self . password != self . confirmation : raise ValidationError ( \"password doesn't match its confirmation\" ) @dataclass class CompleteForm ( PasswordForm ): username : str with pytest . raises ( ValidationError ) as err : deserialize ( CompleteForm , { \"username\" : \"wyfo\" , \"password\" : \"p455w0rd\" , \"confirmation\" : \"...\" }, ) assert err . value . errors == [ { \"loc\" : [], \"err\" : \"password doesn't match its confirmation\" } ]","title":"Validators inheritance"},{"location":"validation/#validator-with-initvar","text":"Dataclasses InitVar are accessible in validators by using parameters the same way __post_init__ does. Only the needed fields have to be put in parameters, they are then added to validator dependencies. from dataclasses import InitVar , dataclass , field import pytest from apischema import ValidationError , deserialize , validator from apischema.metadata import init_var @dataclass class Foo : bar : InitVar [ int ] = field ( metadata = init_var ( int )) @validator ( bar ) def validate ( self , bar : int ): if bar < 0 : yield \"negative\" with pytest . raises ( ValidationError ) as err : deserialize ( Foo , { \"bar\" : - 1 }) assert err . value . errors == [{ \"loc\" : [ \"bar\" ], \"err\" : \"negative\" }]","title":"Validator with InitVar"},{"location":"validation/#validators-are-not-run-on-default-values","text":"If all validator dependencies are initialized with their default values, they are not run. from dataclasses import dataclass , field from apischema import deserialize , validator validator_run = False @dataclass class Foo : bar : int = field ( default = 0 ) @validator ( bar ) def password_match ( self ): global validator_run validator_run = True if self . bar < 0 : raise ValueError ( \"negative\" ) deserialize ( Foo , {}) assert not validator_run","title":"Validators are not run on default values"},{"location":"validation/#validators-for-every-type","text":"Validators can also be declared as regular functions, in which case annotation of the first param is used to associate it to the validated type (you can also use the owner parameter); this allows adding a validator to every type. Last but not least, validators can be embedded directly into Annotated arguments using validators metadata. from typing import Annotated , NewType import pytest from apischema import ValidationError , deserialize , validator from apischema.metadata import validators Palindrome = NewType ( \"Palindrome\" , str ) @validator # could also use @validator(owner=Palindrome) def check_palindrome ( s : Palindrome ): for i in range ( len ( s ) // 2 ): if s [ i ] != s [ - 1 - i ]: raise ValidationError ( \"Not a palindrome\" ) assert deserialize ( Palindrome , \"tacocat\" ) == \"tacocat\" with pytest . raises ( ValidationError ) as err : deserialize ( Palindrome , \"palindrome\" ) assert err . value . errors == [{ \"loc\" : [], \"err\" : \"Not a palindrome\" }] # Using Annotated with pytest . raises ( ValidationError ) as err : deserialize ( Annotated [ str , validators ( check_palindrome )], \"palindrom\" ) assert err . value . errors == [{ \"loc\" : [], \"err\" : \"Not a palindrome\" }]","title":"Validators for every type"},{"location":"validation/#faq","text":"","title":"FAQ"},{"location":"validation/#how-are-validator-dependencies-computed","text":"ast.NodeVisitor and the Python black magic begins...","title":"How are validator dependencies computed?"},{"location":"validation/#why-only-validate-at-deserialization-and-not-at-instantiation","text":"apischema uses type annotations, so every objects used can already be statically type-checked (with Mypy / Pycharm /etc.) at instantiation but also at modification.","title":"Why only validate at deserialization and not at instantiation?"},{"location":"validation/#why-use-validators-for-dataclasses-instead-of-doing-validation-in-__post_init__","text":"Actually, validation can completely be done in __post_init__ , there is no problem with that. However, validators offers one thing that cannot be achieved with __post_init__ : they are run before __init__ , so they can validate incomplete data. Moreover, they are only run during deserialization, so they don't add overhead to normal class instantiation.","title":"Why use validators for dataclasses instead of doing validation in __post_init__?"},{"location":"validation/#why-validators-cannot-raise-arbitrary-exception","text":"Allowing arbitrary exception is in fact a security issue, because unwanted exception could be raised, and their message displayed in validation error. It could either contain sensitive data, or give information about the implementation which could be used to hack it. By the way, it's possible to define a decorator to convert precise exceptions to ValidationError : from collections.abc import Callable from functools import wraps from typing import TypeVar from apischema import ValidationError Func = TypeVar ( \"Func\" , bound = Callable ) def catch ( * exceptions ) -> Callable [[ Func ], Func ]: def decorator ( func : Func ) -> Func : @wraps ( func ) def wrapper ( * args , ** kwargs ): try : return func ( * args , ** kwargs ) except Exception as err : raise ValidationError ( str ( err )) if isinstance ( err , exceptions ) else err return wrapper return decorator","title":"Why validators cannot raise arbitrary exception?"},{"location":"examples/attrs_support/","text":"Attrs support \u00b6 from typing import Sequence import attrs from apischema import deserialize , serialize , settings from apischema.json_schema import deserialization_schema from apischema.objects import ObjectField prev_default_object_fields = settings . default_object_fields def attrs_fields ( cls : type ) -> Sequence [ ObjectField ] | None : if hasattr ( cls , \"__attrs_attrs__\" ): return [ ObjectField ( a . name , a . type , required = a . default == attrs . NOTHING , default = a . default ) for a in getattr ( cls , \"__attrs_attrs__\" ) ] else : return prev_default_object_fields ( cls ) settings . default_object_fields = attrs_fields @attrs . define class Foo : bar : int assert deserialize ( Foo , { \"bar\" : 0 }) == Foo ( 0 ) assert serialize ( Foo , Foo ( 0 )) == { \"bar\" : 0 } assert deserialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : \"integer\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , }","title":"Attrs support"},{"location":"examples/attrs_support/#attrs-support","text":"from typing import Sequence import attrs from apischema import deserialize , serialize , settings from apischema.json_schema import deserialization_schema from apischema.objects import ObjectField prev_default_object_fields = settings . default_object_fields def attrs_fields ( cls : type ) -> Sequence [ ObjectField ] | None : if hasattr ( cls , \"__attrs_attrs__\" ): return [ ObjectField ( a . name , a . type , required = a . default == attrs . NOTHING , default = a . default ) for a in getattr ( cls , \"__attrs_attrs__\" ) ] else : return prev_default_object_fields ( cls ) settings . default_object_fields = attrs_fields @attrs . define class Foo : bar : int assert deserialize ( Foo , { \"bar\" : 0 }) == Foo ( 0 ) assert serialize ( Foo , Foo ( 0 )) == { \"bar\" : 0 } assert deserialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : \"integer\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , }","title":"Attrs support"},{"location":"examples/inherited_deserializer/","text":"Inherited deserializer \u00b6 from collections.abc import Iterator from dataclasses import dataclass from typing import TypeVar from apischema import deserialize , deserializer from apischema.conversions import Conversion Foo_ = TypeVar ( \"Foo_\" , bound = \"Foo\" ) # Use a dataclass in order to be easily testable with == @dataclass class Foo : value : int @classmethod def deserialize ( cls : type [ Foo_ ], value : int ) -> Foo_ : return cls ( value ) def __init_subclass__ ( cls , ** kwargs ): super () . __init_subclass__ ( ** kwargs ) # Register subclasses' conversion in __init_subclass__ deserializer ( Conversion ( cls . deserialize , target = cls )) # Register main conversion after the class definition deserializer ( Conversion ( Foo . deserialize , target = Foo )) class Bar ( Foo ): pass assert deserialize ( Foo , 0 ) == Foo ( 0 ) assert deserialize ( Bar , 0 ) == Bar ( 0 ) # For external types (defines in imported library) @dataclass class ForeignType : value : int class ForeignSubtype ( ForeignType ): pass T = TypeVar ( \"T\" ) # Recursive implementation of type.__subclasses__ def rec_subclasses ( cls : type [ T ]) -> Iterator [ type [ T ]]: for sub_cls in cls . __subclasses__ (): yield sub_cls yield from rec_subclasses ( sub_cls ) # Register deserializers for all subclasses for cls in ( ForeignType , * rec_subclasses ( ForeignType )): # cls=cls is an lambda idiom to capture variable by value inside loop deserializer ( Conversion ( lambda value , cls = cls : cls ( value ), source = int , target = cls )) assert deserialize ( ForeignType , 0 ) == ForeignType ( 0 ) assert deserialize ( ForeignSubtype , 0 ) == ForeignSubtype ( 0 )","title":"Inherited deserializer"},{"location":"examples/inherited_deserializer/#inherited-deserializer","text":"from collections.abc import Iterator from dataclasses import dataclass from typing import TypeVar from apischema import deserialize , deserializer from apischema.conversions import Conversion Foo_ = TypeVar ( \"Foo_\" , bound = \"Foo\" ) # Use a dataclass in order to be easily testable with == @dataclass class Foo : value : int @classmethod def deserialize ( cls : type [ Foo_ ], value : int ) -> Foo_ : return cls ( value ) def __init_subclass__ ( cls , ** kwargs ): super () . __init_subclass__ ( ** kwargs ) # Register subclasses' conversion in __init_subclass__ deserializer ( Conversion ( cls . deserialize , target = cls )) # Register main conversion after the class definition deserializer ( Conversion ( Foo . deserialize , target = Foo )) class Bar ( Foo ): pass assert deserialize ( Foo , 0 ) == Foo ( 0 ) assert deserialize ( Bar , 0 ) == Bar ( 0 ) # For external types (defines in imported library) @dataclass class ForeignType : value : int class ForeignSubtype ( ForeignType ): pass T = TypeVar ( \"T\" ) # Recursive implementation of type.__subclasses__ def rec_subclasses ( cls : type [ T ]) -> Iterator [ type [ T ]]: for sub_cls in cls . __subclasses__ (): yield sub_cls yield from rec_subclasses ( sub_cls ) # Register deserializers for all subclasses for cls in ( ForeignType , * rec_subclasses ( ForeignType )): # cls=cls is an lambda idiom to capture variable by value inside loop deserializer ( Conversion ( lambda value , cls = cls : cls ( value ), source = int , target = cls )) assert deserialize ( ForeignType , 0 ) == ForeignType ( 0 ) assert deserialize ( ForeignSubtype , 0 ) == ForeignSubtype ( 0 )","title":"Inherited deserializer"},{"location":"examples/pydantic_support/","text":"Pydantic support \u00b6 It takes only 30 lines of code to support pydantic.BaseModel and all of its subclasses. You could add these lines to your project using pydantic and start to benefit from apischema features. This example deliberately doesn't use set_object_fields but instead the conversions feature in order to roughly include pydantic \"as is\": it will reuse pydantic coercion, error messages, JSON schema, etc. This makes a full retro-compatible support. As a result, lot of apischema features like GraphQL schema generation or NewType validation cannot be supported using this method \u2014 but they could be by using set_object_fields instead. import inspect from collections.abc import Mapping from typing import Any import pydantic import pytest from apischema import ( ValidationError , deserialize , schema , serialize , serializer , settings , ) from apischema.conversions import AnyConversion , Conversion from apischema.json_schema import deserialization_schema from apischema.schemas import Schema # ---------- Pydantic support code starts here ---------- prev_deserialization = settings . deserialization . default_conversion def default_deserialization ( tp : Any ) -> AnyConversion | None : if inspect . isclass ( tp ) and issubclass ( tp , pydantic . BaseModel ): def deserialize_pydantic ( data ): try : return tp . parse_obj ( data ) except pydantic . ValidationError as error : raise ValidationError . from_errors ( [{ \"loc\" : err [ \"loc\" ], \"err\" : err [ \"msg\" ]} for err in error . errors ()] ) return Conversion ( deserialize_pydantic , source = tp . __annotations__ . get ( \"__root__\" , Mapping [ str , Any ]), target = tp , ) else : return prev_deserialization ( tp ) settings . deserialization . default_conversion = default_deserialization prev_schema = settings . base_schema . type def default_schema ( tp : Any ) -> Schema | None : if inspect . isclass ( tp ) and issubclass ( tp , pydantic . BaseModel ): return schema ( extra = tp . schema (), override = True ) else : return prev_schema ( tp ) settings . base_schema . type = default_schema # No need to use settings.serialization because serializer is inherited @serializer def serialize_pydantic ( obj : pydantic . BaseModel ) -> Any : # There is currently no way to retrieve `serialize` parameters inside converters, # so exclude_unset is set to True as it's the default apischema setting return getattr ( obj , \"__root__\" , obj . dict ( exclude_unset = True )) # ---------- Pydantic support code ends here ---------- class Foo ( pydantic . BaseModel ): bar : int assert deserialize ( Foo , { \"bar\" : 0 }) == Foo ( bar = 0 ) assert serialize ( Foo , Foo ( bar = 0 )) == { \"bar\" : 0 } assert deserialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"title\" : \"Foo\" , # pydantic title \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"title\" : \"Bar\" , \"type\" : \"integer\" }}, \"required\" : [ \"bar\" ], } with pytest . raises ( ValidationError ) as err : deserialize ( Foo , { \"bar\" : \"not an int\" }) assert err . value . errors == [ { \"loc\" : [ \"bar\" ], \"err\" : \"value is not a valid integer\" } # pydantic error message ]","title":"Pydantic support"},{"location":"examples/pydantic_support/#pydantic-support","text":"It takes only 30 lines of code to support pydantic.BaseModel and all of its subclasses. You could add these lines to your project using pydantic and start to benefit from apischema features. This example deliberately doesn't use set_object_fields but instead the conversions feature in order to roughly include pydantic \"as is\": it will reuse pydantic coercion, error messages, JSON schema, etc. This makes a full retro-compatible support. As a result, lot of apischema features like GraphQL schema generation or NewType validation cannot be supported using this method \u2014 but they could be by using set_object_fields instead. import inspect from collections.abc import Mapping from typing import Any import pydantic import pytest from apischema import ( ValidationError , deserialize , schema , serialize , serializer , settings , ) from apischema.conversions import AnyConversion , Conversion from apischema.json_schema import deserialization_schema from apischema.schemas import Schema # ---------- Pydantic support code starts here ---------- prev_deserialization = settings . deserialization . default_conversion def default_deserialization ( tp : Any ) -> AnyConversion | None : if inspect . isclass ( tp ) and issubclass ( tp , pydantic . BaseModel ): def deserialize_pydantic ( data ): try : return tp . parse_obj ( data ) except pydantic . ValidationError as error : raise ValidationError . from_errors ( [{ \"loc\" : err [ \"loc\" ], \"err\" : err [ \"msg\" ]} for err in error . errors ()] ) return Conversion ( deserialize_pydantic , source = tp . __annotations__ . get ( \"__root__\" , Mapping [ str , Any ]), target = tp , ) else : return prev_deserialization ( tp ) settings . deserialization . default_conversion = default_deserialization prev_schema = settings . base_schema . type def default_schema ( tp : Any ) -> Schema | None : if inspect . isclass ( tp ) and issubclass ( tp , pydantic . BaseModel ): return schema ( extra = tp . schema (), override = True ) else : return prev_schema ( tp ) settings . base_schema . type = default_schema # No need to use settings.serialization because serializer is inherited @serializer def serialize_pydantic ( obj : pydantic . BaseModel ) -> Any : # There is currently no way to retrieve `serialize` parameters inside converters, # so exclude_unset is set to True as it's the default apischema setting return getattr ( obj , \"__root__\" , obj . dict ( exclude_unset = True )) # ---------- Pydantic support code ends here ---------- class Foo ( pydantic . BaseModel ): bar : int assert deserialize ( Foo , { \"bar\" : 0 }) == Foo ( bar = 0 ) assert serialize ( Foo , Foo ( bar = 0 )) == { \"bar\" : 0 } assert deserialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"title\" : \"Foo\" , # pydantic title \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"title\" : \"Bar\" , \"type\" : \"integer\" }}, \"required\" : [ \"bar\" ], } with pytest . raises ( ValidationError ) as err : deserialize ( Foo , { \"bar\" : \"not an int\" }) assert err . value . errors == [ { \"loc\" : [ \"bar\" ], \"err\" : \"value is not a valid integer\" } # pydantic error message ]","title":"Pydantic support"},{"location":"examples/recoverable_fields/","text":"Recoverable fields \u00b6 Inspired by https://github.com/samuelcolvin/pydantic/issues/800 from typing import Any , Dict , Generic , TypeVar import pytest from apischema import deserialize , deserializer , schema , serialize , serializer from apischema.json_schema import deserialization_schema , serialization_schema # Add a dummy placeholder comment in order to not have an empty schema # (because Union member with empty schema would \"contaminate\" whole Union schema) @schema ( extra = { \"$comment\" : \"recoverable\" }) class RecoverableRaw ( Exception ): def __init__ ( self , raw : Any ): self . raw = raw deserializer ( RecoverableRaw ) T = TypeVar ( \"T\" ) def remove_recoverable_schema ( json_schema : Dict [ str , Any ]): if \"anyOf\" in json_schema : # deserialization schema value_schema , recoverable_comment = json_schema . pop ( \"anyOf\" ) assert recoverable_comment == { \"$comment\" : \"recoverable\" } json_schema . update ( value_schema ) @schema ( extra = remove_recoverable_schema ) class Recoverable ( Generic [ T ]): def __init__ ( self , value : T | RecoverableRaw ): self . _value = value @property def value ( self ) -> T : if isinstance ( self . _value , RecoverableRaw ): raise self . _value return self . _value @value . setter def value ( self , value : T ): self . _value = value deserializer ( Recoverable ) serializer ( Recoverable . value ) assert deserialize ( Recoverable [ int ], 0 ) . value == 0 with pytest . raises ( RecoverableRaw ) as err : _ = deserialize ( Recoverable [ int ], \"bad\" ) . value assert err . value . raw == \"bad\" assert serialize ( Recoverable [ int ], Recoverable ( 0 )) == 0 with pytest . raises ( RecoverableRaw ) as err : serialize ( Recoverable [ int ], Recoverable ( RecoverableRaw ( \"bad\" ))) assert err . value . raw == \"bad\" assert ( deserialization_schema ( Recoverable [ int ]) == serialization_schema ( Recoverable [ int ]) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"type\" : \"integer\" } )","title":"Recoverable fields"},{"location":"examples/recoverable_fields/#recoverable-fields","text":"Inspired by https://github.com/samuelcolvin/pydantic/issues/800 from typing import Any , Dict , Generic , TypeVar import pytest from apischema import deserialize , deserializer , schema , serialize , serializer from apischema.json_schema import deserialization_schema , serialization_schema # Add a dummy placeholder comment in order to not have an empty schema # (because Union member with empty schema would \"contaminate\" whole Union schema) @schema ( extra = { \"$comment\" : \"recoverable\" }) class RecoverableRaw ( Exception ): def __init__ ( self , raw : Any ): self . raw = raw deserializer ( RecoverableRaw ) T = TypeVar ( \"T\" ) def remove_recoverable_schema ( json_schema : Dict [ str , Any ]): if \"anyOf\" in json_schema : # deserialization schema value_schema , recoverable_comment = json_schema . pop ( \"anyOf\" ) assert recoverable_comment == { \"$comment\" : \"recoverable\" } json_schema . update ( value_schema ) @schema ( extra = remove_recoverable_schema ) class Recoverable ( Generic [ T ]): def __init__ ( self , value : T | RecoverableRaw ): self . _value = value @property def value ( self ) -> T : if isinstance ( self . _value , RecoverableRaw ): raise self . _value return self . _value @value . setter def value ( self , value : T ): self . _value = value deserializer ( Recoverable ) serializer ( Recoverable . value ) assert deserialize ( Recoverable [ int ], 0 ) . value == 0 with pytest . raises ( RecoverableRaw ) as err : _ = deserialize ( Recoverable [ int ], \"bad\" ) . value assert err . value . raw == \"bad\" assert serialize ( Recoverable [ int ], Recoverable ( 0 )) == 0 with pytest . raises ( RecoverableRaw ) as err : serialize ( Recoverable [ int ], Recoverable ( RecoverableRaw ( \"bad\" ))) assert err . value . raw == \"bad\" assert ( deserialization_schema ( Recoverable [ int ]) == serialization_schema ( Recoverable [ int ]) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"type\" : \"integer\" } )","title":"Recoverable fields"},{"location":"examples/sqlalchemy_support/","text":"SQLAlchemy support \u00b6 This example shows simple support for SQLAlchemy . from collections.abc import Collection from inspect import getmembers from itertools import starmap from typing import Any from graphql import print_schema from sqlalchemy import Column , Integer , String from sqlalchemy.ext.declarative import as_declarative from apischema import Undefined , deserialize , serialize from apischema.graphql import graphql_schema from apischema.json_schema import deserialization_schema from apischema.objects import ObjectField , set_object_fields def column_field ( name : str , column : Column ) -> ObjectField : required = False default : Any = ... if column . default is not None : default = column . default elif column . server_default is not None : default = Undefined elif column . nullable : default = None else : required = True col_type = column . type . python_type if column . nullable : col_type = col_type | None return ObjectField ( column . name or name , col_type , required , default = default ) # Very basic SQLAlchemy support @as_declarative () class Base : def __init_subclass__ ( cls , ** kwargs ): super () . __init_subclass__ ( ** kwargs ) columns = getmembers ( cls , lambda m : isinstance ( m , Column )) if not columns : return set_object_fields ( cls , starmap ( column_field , columns )) class Foo ( Base ): __tablename__ = \"foo\" bar = Column ( Integer , primary_key = True ) baz = Column ( String ) foo = deserialize ( Foo , { \"bar\" : 0 }) assert isinstance ( foo , Foo ) assert foo . bar == 0 assert serialize ( Foo , foo ) == { \"bar\" : 0 , \"baz\" : None } assert deserialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : \"integer\" }, \"baz\" : { \"type\" : [ \"string\" , \"null\" ], \"default\" : None }, }, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , } def foos () -> Collection [ Foo ] | None : ... schema = graphql_schema ( query = [ foos ]) schema_str = \"\"\" \\ type Query { foos: [Foo!] } type Foo { bar: Int! baz: String }\"\"\" assert print_schema ( schema ) == schema_str","title":"SQLAlchemy support"},{"location":"examples/sqlalchemy_support/#sqlalchemy-support","text":"This example shows simple support for SQLAlchemy . from collections.abc import Collection from inspect import getmembers from itertools import starmap from typing import Any from graphql import print_schema from sqlalchemy import Column , Integer , String from sqlalchemy.ext.declarative import as_declarative from apischema import Undefined , deserialize , serialize from apischema.graphql import graphql_schema from apischema.json_schema import deserialization_schema from apischema.objects import ObjectField , set_object_fields def column_field ( name : str , column : Column ) -> ObjectField : required = False default : Any = ... if column . default is not None : default = column . default elif column . server_default is not None : default = Undefined elif column . nullable : default = None else : required = True col_type = column . type . python_type if column . nullable : col_type = col_type | None return ObjectField ( column . name or name , col_type , required , default = default ) # Very basic SQLAlchemy support @as_declarative () class Base : def __init_subclass__ ( cls , ** kwargs ): super () . __init_subclass__ ( ** kwargs ) columns = getmembers ( cls , lambda m : isinstance ( m , Column )) if not columns : return set_object_fields ( cls , starmap ( column_field , columns )) class Foo ( Base ): __tablename__ = \"foo\" bar = Column ( Integer , primary_key = True ) baz = Column ( String ) foo = deserialize ( Foo , { \"bar\" : 0 }) assert isinstance ( foo , Foo ) assert foo . bar == 0 assert serialize ( Foo , foo ) == { \"bar\" : 0 , \"baz\" : None } assert deserialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : \"integer\" }, \"baz\" : { \"type\" : [ \"string\" , \"null\" ], \"default\" : None }, }, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , } def foos () -> Collection [ Foo ] | None : ... schema = graphql_schema ( query = [ foos ]) schema_str = \"\"\" \\ type Query { foos: [Foo!] } type Foo { bar: Int! baz: String }\"\"\" assert print_schema ( schema ) == schema_str","title":"SQLAlchemy support"},{"location":"examples/subclass_tagged_union/","text":"Class as tagged union of its subclasses \u00b6 From https://github.com/wyfo/apischema/discussions/56 Tagged unions are useful when it comes to GraphQL input (or even output). from collections import defaultdict from collections.abc import AsyncIterable , Callable , Iterator from dataclasses import dataclass , field from types import new_class from typing import Annotated , Any , TypeVar , get_type_hints import graphql from apischema import deserializer , schema , serializer , type_name from apischema.conversions import Conversion from apischema.graphql import graphql_schema from apischema.metadata import conversion from apischema.objects import object_deserialization from apischema.tagged_unions import Tagged , TaggedUnion , get_tagged from apischema.utils import to_pascal_case _alternative_constructors : dict [ type , list [ Callable ]] = defaultdict ( list ) Func = TypeVar ( \"Func\" , bound = Callable ) def alternative_constructor ( func : Func ) -> Func : _alternative_constructors [ get_type_hints ( func )[ \"return\" ]] . append ( func ) return func def rec_subclasses ( cls : type ) -> Iterator [ type ]: \"\"\"Recursive implementation of type.__subclasses__\"\"\" for sub_cls in cls . __subclasses__ (): yield sub_cls yield from rec_subclasses ( sub_cls ) Cls = TypeVar ( \"Cls\" , bound = type ) def as_tagged_union ( cls : Cls ) -> Cls : def serialization () -> Conversion : annotations = { sub . __name__ : Tagged [ sub ] for sub in rec_subclasses ( cls )} namespace = { \"__annotations__\" : annotations } tagged_union = new_class ( cls . __name__ , ( TaggedUnion ,), exec_body = lambda ns : ns . update ( namespace ) ) return Conversion ( lambda obj : tagged_union ( ** { obj . __class__ . __name__ : obj }), source = cls , target = tagged_union , # Conversion must not be inherited because it would lead to # infinite recursion otherwise inherited = False , ) def deserialization () -> Conversion : annotations : dict [ str , Any ] = {} namespace : dict [ str , Any ] = { \"__annotations__\" : annotations } for sub in rec_subclasses ( cls ): annotations [ sub . __name__ ] = Tagged [ sub ] # Add tagged fields for all its alternative constructors for constructor in _alternative_constructors . get ( sub , ()): # Build the alias of the field alias = to_pascal_case ( constructor . __name__ ) # object_deserialization uses get_type_hints, but the constructor # return type is stringified and the class not defined yet, # so it must be assigned manually constructor . __annotations__ [ \"return\" ] = sub # Use object_deserialization to wrap constructor as deserializer deserialization = object_deserialization ( constructor , type_name ( alias )) # Add constructor tagged field with its conversion annotations [ alias ] = Tagged [ sub ] namespace [ alias ] = Tagged ( conversion ( deserialization = deserialization )) # Create the deserialization tagged union class tagged_union = new_class ( cls . __name__ , ( TaggedUnion ,), exec_body = lambda ns : ns . update ( namespace ) ) return Conversion ( lambda obj : get_tagged ( obj )[ 1 ], source = tagged_union , target = cls ) deserializer ( lazy = deserialization , target = cls ) serializer ( lazy = serialization , source = cls ) return cls @as_tagged_union class Drawing : def points ( self ) -> AsyncIterable [ float ]: raise NotImplementedError @dataclass class Line ( Drawing ): start : float stop : float step : float = field ( default = 1 , metadata = schema ( exc_min = 0 )) async def points ( self ) -> AsyncIterable [ float ]: point = self . start while point <= self . stop : yield point point += self . step @alternative_constructor def sized_line ( start : float , stop : float , size : Annotated [ float , schema ( min = 1 )] ) -> \"Line\" : return Line ( start = start , stop = stop , step = ( stop - start ) / ( size - 1 )) @dataclass class Concat ( Drawing ): left : Drawing right : Drawing async def points ( self ) -> AsyncIterable [ float ]: async for point in self . left . points (): yield point async for point in self . right . points (): yield point def echo ( drawing : Drawing = None ) -> Drawing | None : return drawing drawing_schema = graphql_schema ( query = [ echo ]) assert ( graphql . utilities . print_schema ( drawing_schema ) == \"\"\" \\ type Query { echo(drawing: DrawingInput): Drawing } type Drawing { Line: Line Concat: Concat } type Line { start: Float! stop: Float! step: Float! } type Concat { left: Drawing! right: Drawing! } input DrawingInput { Line: LineInput SizedLine: SizedLineInput Concat: ConcatInput } input LineInput { start: Float! stop: Float! step: Float! = 1 } input SizedLineInput { start: Float! stop: Float! size: Float! } input ConcatInput { left: DrawingInput! right: DrawingInput! }\"\"\" ) query = \"\"\" \\ { echo(drawing: { Concat: { left: { SizedLine: { start: 0, stop: 12, size: 3 }, }, right: { Line: { start: 12, stop: 13 }, } } }) { Concat { left { Line { start stop step } } right { Line { start stop step } } } } }\"\"\" assert graphql . graphql_sync ( drawing_schema , query ) . data == { \"echo\" : { \"Concat\" : { \"left\" : { \"Line\" : { \"start\" : 0.0 , \"stop\" : 12.0 , \"step\" : 6.0 }}, \"right\" : { \"Line\" : { \"start\" : 12.0 , \"stop\" : 13.0 , \"step\" : 1.0 }}, } } }","title":"Class as tagged union of its subclasses"},{"location":"examples/subclass_tagged_union/#class-as-tagged-union-of-its-subclasses","text":"From https://github.com/wyfo/apischema/discussions/56 Tagged unions are useful when it comes to GraphQL input (or even output). from collections import defaultdict from collections.abc import AsyncIterable , Callable , Iterator from dataclasses import dataclass , field from types import new_class from typing import Annotated , Any , TypeVar , get_type_hints import graphql from apischema import deserializer , schema , serializer , type_name from apischema.conversions import Conversion from apischema.graphql import graphql_schema from apischema.metadata import conversion from apischema.objects import object_deserialization from apischema.tagged_unions import Tagged , TaggedUnion , get_tagged from apischema.utils import to_pascal_case _alternative_constructors : dict [ type , list [ Callable ]] = defaultdict ( list ) Func = TypeVar ( \"Func\" , bound = Callable ) def alternative_constructor ( func : Func ) -> Func : _alternative_constructors [ get_type_hints ( func )[ \"return\" ]] . append ( func ) return func def rec_subclasses ( cls : type ) -> Iterator [ type ]: \"\"\"Recursive implementation of type.__subclasses__\"\"\" for sub_cls in cls . __subclasses__ (): yield sub_cls yield from rec_subclasses ( sub_cls ) Cls = TypeVar ( \"Cls\" , bound = type ) def as_tagged_union ( cls : Cls ) -> Cls : def serialization () -> Conversion : annotations = { sub . __name__ : Tagged [ sub ] for sub in rec_subclasses ( cls )} namespace = { \"__annotations__\" : annotations } tagged_union = new_class ( cls . __name__ , ( TaggedUnion ,), exec_body = lambda ns : ns . update ( namespace ) ) return Conversion ( lambda obj : tagged_union ( ** { obj . __class__ . __name__ : obj }), source = cls , target = tagged_union , # Conversion must not be inherited because it would lead to # infinite recursion otherwise inherited = False , ) def deserialization () -> Conversion : annotations : dict [ str , Any ] = {} namespace : dict [ str , Any ] = { \"__annotations__\" : annotations } for sub in rec_subclasses ( cls ): annotations [ sub . __name__ ] = Tagged [ sub ] # Add tagged fields for all its alternative constructors for constructor in _alternative_constructors . get ( sub , ()): # Build the alias of the field alias = to_pascal_case ( constructor . __name__ ) # object_deserialization uses get_type_hints, but the constructor # return type is stringified and the class not defined yet, # so it must be assigned manually constructor . __annotations__ [ \"return\" ] = sub # Use object_deserialization to wrap constructor as deserializer deserialization = object_deserialization ( constructor , type_name ( alias )) # Add constructor tagged field with its conversion annotations [ alias ] = Tagged [ sub ] namespace [ alias ] = Tagged ( conversion ( deserialization = deserialization )) # Create the deserialization tagged union class tagged_union = new_class ( cls . __name__ , ( TaggedUnion ,), exec_body = lambda ns : ns . update ( namespace ) ) return Conversion ( lambda obj : get_tagged ( obj )[ 1 ], source = tagged_union , target = cls ) deserializer ( lazy = deserialization , target = cls ) serializer ( lazy = serialization , source = cls ) return cls @as_tagged_union class Drawing : def points ( self ) -> AsyncIterable [ float ]: raise NotImplementedError @dataclass class Line ( Drawing ): start : float stop : float step : float = field ( default = 1 , metadata = schema ( exc_min = 0 )) async def points ( self ) -> AsyncIterable [ float ]: point = self . start while point <= self . stop : yield point point += self . step @alternative_constructor def sized_line ( start : float , stop : float , size : Annotated [ float , schema ( min = 1 )] ) -> \"Line\" : return Line ( start = start , stop = stop , step = ( stop - start ) / ( size - 1 )) @dataclass class Concat ( Drawing ): left : Drawing right : Drawing async def points ( self ) -> AsyncIterable [ float ]: async for point in self . left . points (): yield point async for point in self . right . points (): yield point def echo ( drawing : Drawing = None ) -> Drawing | None : return drawing drawing_schema = graphql_schema ( query = [ echo ]) assert ( graphql . utilities . print_schema ( drawing_schema ) == \"\"\" \\ type Query { echo(drawing: DrawingInput): Drawing } type Drawing { Line: Line Concat: Concat } type Line { start: Float! stop: Float! step: Float! } type Concat { left: Drawing! right: Drawing! } input DrawingInput { Line: LineInput SizedLine: SizedLineInput Concat: ConcatInput } input LineInput { start: Float! stop: Float! step: Float! = 1 } input SizedLineInput { start: Float! stop: Float! size: Float! } input ConcatInput { left: DrawingInput! right: DrawingInput! }\"\"\" ) query = \"\"\" \\ { echo(drawing: { Concat: { left: { SizedLine: { start: 0, stop: 12, size: 3 }, }, right: { Line: { start: 12, stop: 13 }, } } }) { Concat { left { Line { start stop step } } right { Line { start stop step } } } } }\"\"\" assert graphql . graphql_sync ( drawing_schema , query ) . data == { \"echo\" : { \"Concat\" : { \"left\" : { \"Line\" : { \"start\" : 0.0 , \"stop\" : 12.0 , \"step\" : 6.0 }}, \"right\" : { \"Line\" : { \"start\" : 12.0 , \"stop\" : 13.0 , \"step\" : 1.0 }}, } } }","title":"Class as tagged union of its subclasses"},{"location":"examples/subclass_union/","text":"Class as union of its subclasses \u00b6 Inspired by https://github.com/samuelcolvin/pydantic/issues/2036 A class can easily be deserialized as a union of its subclasses using deserializers. Indeed, when more than one deserializer is registered, it results in a union. from dataclasses import dataclass from typing import Any , Union from apischema import deserialize , deserializer , identity , serializer from apischema.conversions import Conversion from apischema.json_schema import deserialization_schema , serialization_schema class Base : _union : Any = None # You can use __init_subclass__ to register new subclass automatically def __init_subclass__ ( cls , ** kwargs ): super () . __init_subclass__ ( ** kwargs ) # Deserializers stack directly as a Union deserializer ( Conversion ( identity , source = cls , target = Base )) # Only Base serializer must be registered (and updated for each subclass) as # a Union, and not be inherited Base . _union = cls if Base . _union is None else Union [ Base . _union , cls ] serializer ( Conversion ( identity , source = Base , target = Base . _union , inherited = False ) ) @dataclass class Foo ( Base ): foo : int @dataclass class Bar ( Base ): bar : str assert ( deserialization_schema ( Base ) == serialization_schema ( Base ) == { \"anyOf\" : [ { \"type\" : \"object\" , \"properties\" : { \"foo\" : { \"type\" : \"integer\" }}, \"required\" : [ \"foo\" ], \"additionalProperties\" : False , }, { \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : \"string\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , }, ], \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , } ) assert deserialize ( Base , { \"foo\" : 0 }) == Foo ( 0 )","title":"Class as union of its subclasses"},{"location":"examples/subclass_union/#class-as-union-of-its-subclasses","text":"Inspired by https://github.com/samuelcolvin/pydantic/issues/2036 A class can easily be deserialized as a union of its subclasses using deserializers. Indeed, when more than one deserializer is registered, it results in a union. from dataclasses import dataclass from typing import Any , Union from apischema import deserialize , deserializer , identity , serializer from apischema.conversions import Conversion from apischema.json_schema import deserialization_schema , serialization_schema class Base : _union : Any = None # You can use __init_subclass__ to register new subclass automatically def __init_subclass__ ( cls , ** kwargs ): super () . __init_subclass__ ( ** kwargs ) # Deserializers stack directly as a Union deserializer ( Conversion ( identity , source = cls , target = Base )) # Only Base serializer must be registered (and updated for each subclass) as # a Union, and not be inherited Base . _union = cls if Base . _union is None else Union [ Base . _union , cls ] serializer ( Conversion ( identity , source = Base , target = Base . _union , inherited = False ) ) @dataclass class Foo ( Base ): foo : int @dataclass class Bar ( Base ): bar : str assert ( deserialization_schema ( Base ) == serialization_schema ( Base ) == { \"anyOf\" : [ { \"type\" : \"object\" , \"properties\" : { \"foo\" : { \"type\" : \"integer\" }}, \"required\" : [ \"foo\" ], \"additionalProperties\" : False , }, { \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : \"string\" }}, \"required\" : [ \"bar\" ], \"additionalProperties\" : False , }, ], \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , } ) assert deserialize ( Base , { \"foo\" : 0 }) == Foo ( 0 )","title":"Class as union of its subclasses"},{"location":"graphql/data_model_and_resolvers/","text":"Data model and resolvers \u00b6 Almost everything in the Data model section remains valid in GraphQL integration, with a few differences. GraphQL specific data model \u00b6 Enum \u00b6 Enum members are represented in the schema using their name instead of their value. This is more consistent with the way GraphQL represents enumerations. TypedDict \u00b6 TypedDict is not supported as an output type. (see FAQ ) Union \u00b6 Unions are only supported between output object type, which means dataclass and NamedTuple (and conversions / dataclass model ). There are 2 exceptions which can be always be used in Union : None / Optional : Types are non-null (marked with an exclamation mark ! in GraphQL schema) by default; Optional types however results in normal GraphQL types (without ! ). apischema.UndefinedType : it is simply ignored. It is useful in resolvers, see following section Non-null \u00b6 Types are assumed to be non-null by default, as in Python typing. Nullable types are obtained using typing.Optional (or typing.Union with a None argument). Note There is one exception, when resolver parameter default value is not serializable (and thus cannot be included in the schema), the parameter type is then set as nullable to make the parameter non-required. For example parameters not Optional but with Undefined default value will be marked as nullable. This is only for the schema, the default value is still used at execution. Undefined \u00b6 In output, Undefined is converted to None ; so in the schema, Union[T, UndefinedType] will be nullable. In input, fields become nullable when Undefined is their default value. Interfaces \u00b6 Interfaces are simply classes marked with apischema.graphql.interface decorator. An object type implements an interface when its class inherits from an interface-marked class, or when it has flattened fields of interface-marked dataclass. from dataclasses import dataclass from graphql import print_schema from apischema.graphql import graphql_schema , interface @interface @dataclass class Bar : bar : int @dataclass class Foo ( Bar ): baz : str def foo () -> Foo | None : ... schema = graphql_schema ( query = [ foo ]) schema_str = \"\"\" \\ type Query { foo: Foo } type Foo implements Bar { bar: Int! baz: String! } interface Bar { bar: Int! }\"\"\" assert print_schema ( schema ) == schema_str Resolvers \u00b6 All dataclass / NamedTuple fields (excepted skipped ) are resolved with their alias in the GraphQL schema. Custom resolvers can also be added by marking methods with apischema.graphql.resolver decorator \u2014 resolvers share a common interface with apischema.serialized , with a few differences. Methods can be synchronous or asynchronous (defined with async def or annotated with an typing.Awaitable return type). Resolvers parameters are included in the schema with their type, and their default value. from dataclasses import dataclass from graphql import print_schema from apischema.graphql import graphql_schema , resolver @dataclass class Bar : baz : int @dataclass class Foo : @resolver async def bar ( self , arg : int = 0 ) -> Bar : ... async def foo () -> Foo | None : ... schema = graphql_schema ( query = [ foo ]) schema_str = \"\"\" \\ type Query { foo: Foo } type Foo { bar(arg: Int! = 0): Bar! } type Bar { baz: Int! }\"\"\" assert print_schema ( schema ) == schema_str GraphQLResolveInfo parameter \u00b6 Resolvers can have an additional parameter of type graphql.GraphQLResolveInfo (or Optional[graphql.GraphQLResolveInfo] ), which is automatically injected when the resolver is executed in the context of a GraphQL request. This parameter contains the info about the current GraphQL request being executed. Undefined parameter default \u2014 null vs. undefined \u00b6 Undefined can be used as default value of resolver parameters. It can be to distinguish a null input from an absent/ undefined input. In fact, null value will result in a None argument where no value will use the default value, Undefined so. from graphql import graphql_sync from apischema import Undefined , UndefinedType from apischema.graphql import graphql_schema def arg_is_absent ( arg : int | UndefinedType | None = Undefined ) -> bool : return arg is Undefined schema = graphql_schema ( query = [ arg_is_absent ]) assert graphql_sync ( schema , \"{argIsAbsent(arg: null)}\" ) . data == { \"argIsAbsent\" : False } assert graphql_sync ( schema , \" {argIsAbsent} \" ) . data == { \"argIsAbsent\" : True } Error handling \u00b6 Errors occurring in resolvers can be caught in a dedicated error handler registered with error_handler parameter. This function takes in parameters the exception, the object, the info and the kwargs of the failing resolver; it can return a new value or raise the current or another exception \u2014 it can for example be used to log errors without throwing the complete serialization. The resulting serialization type will be a Union of the normal type and the error handling type; if the error handler always raises, use typing.NoReturn annotation. error_handler=None correspond to a default handler which only return None \u2014 exception is thus discarded and the resolver type becomes Optional . The error handler is only executed by apischema serialization process, it's not added to the function, so this one can be executed normally and raise an exception in the rest of your code. Error handler can be synchronous or asynchronous. from dataclasses import dataclass from logging import getLogger from typing import Any import graphql from graphql.utilities import print_schema from apischema.graphql import graphql_schema , resolver logger = getLogger ( __name__ ) def log_error ( error : Exception , obj : Any , info : graphql . GraphQLResolveInfo , ** kwargs ) -> None : logger . error ( \"Resolve error in %s \" , \".\" . join ( map ( str , info . path . as_list ())), exc_info = error ) return None @dataclass class Foo : @resolver ( error_handler = log_error ) def bar ( self ) -> int : raise RuntimeError ( \"Bar error\" ) @resolver def baz ( self ) -> int : raise RuntimeError ( \"Baz error\" ) def foo ( info : graphql . GraphQLResolveInfo ) -> Foo : return Foo () schema = graphql_schema ( query = [ foo ]) # Notice that bar is Int while baz is Int! schema_str = \"\"\" \\ type Query { foo: Foo! } type Foo { bar: Int baz: Int! }\"\"\" assert print_schema ( schema ) == schema_str # Logs \"Resolve error in foo.bar\", no error raised assert graphql . graphql_sync ( schema , \"{foo {bar} }\" ) . data == { \"foo\" : { \"bar\" : None }} # Error is raised assert graphql . graphql_sync ( schema , \"{foo {baz} }\" ) . errors [ 0 ] . message == \"Baz error\" Parameters metadata \u00b6 Resolvers parameters can have metadata like dataclass fields. They can be passed using typing.Annotated . from dataclasses import dataclass from typing import Annotated from graphql.utilities import print_schema from apischema import alias , schema from apischema.graphql import graphql_schema , resolver @dataclass class Foo : @resolver def bar ( self , param : Annotated [ int , alias ( \"arg\" ) | schema ( description = \"argument\" )] ) -> int : return param def foo () -> Foo : return Foo () schema_ = graphql_schema ( query = [ foo ]) # Notice that bar is Int while baz is Int! schema_str = ''' \\ type Query { foo: Foo! } type Foo { bar( \"\"\"argument\"\"\" arg: Int! ): Int! }''' assert print_schema ( schema_ ) == schema_str Note Metadata can also be passed with parameters_metadata parameter; it takes a mapping of parameter names as key and mapped metadata as value. Parameters base schema \u00b6 Following the example of type/field/method base schema , resolver parameters also support a base schema definition import inspect from dataclasses import dataclass from typing import Any , Callable import docstring_parser from graphql.utilities import print_schema from apischema import schema , settings from apischema.graphql import graphql_schema , resolver from apischema.schemas import Schema @dataclass class Foo : @resolver def bar ( self , arg : str ) -> int : \"\"\"bar method :param arg: arg parameter \"\"\" ... def method_base_schema ( tp : Any , method : Callable , alias : str ) -> Schema | None : return schema ( description = docstring_parser . parse ( method . __doc__ ) . short_description ) def parameter_base_schema ( method : Callable , parameter : inspect . Parameter , alias : str ) -> Schema | None : for doc_param in docstring_parser . parse ( method . __doc__ ) . params : if doc_param . arg_name == parameter . name : return schema ( description = doc_param . description ) return None settings . base_schema . method = method_base_schema settings . base_schema . parameter = parameter_base_schema def foo () -> Foo : ... schema_ = graphql_schema ( query = [ foo ]) schema_str = ''' \\ type Query { foo: Foo! } type Foo { \"\"\"bar method\"\"\" bar( \"\"\"arg parameter\"\"\" arg: String! ): Int! }''' assert print_schema ( schema_ ) == schema_str Scalars \u00b6 NewType or non-object types annotated with type_name will be translated in the GraphQL schema by a scalar . By the way, Any will automatically be translated to a JSON scalar, as it is deserialized from and serialized to JSON. from dataclasses import dataclass from typing import Any from uuid import UUID from graphql.utilities import print_schema from apischema.graphql import graphql_schema @dataclass class Foo : id : UUID content : Any def foo () -> Foo | None : ... schema = graphql_schema ( query = [ foo ]) schema_str = \"\"\" \\ type Query { foo: Foo } type Foo { id: UUID! content: JSON } scalar UUID scalar JSON\"\"\" assert print_schema ( schema ) == schema_str ID type \u00b6 GraphQL ID has no precise specification and is defined according API needs; it can be a UUID or/and ObjectId, etc. apischema.graphql_schema has a parameter id_types which can be used to define which types will be marked as ID in the generated schema. Parameter value can be either a collection of types (each type will then be mapped to ID scalar), or a predicate returning if the given type must be marked as ID . from dataclasses import dataclass from uuid import UUID from graphql import print_schema from apischema.graphql import graphql_schema @dataclass class Foo : bar : UUID def foo () -> Foo | None : ... # id_types={UUID} is equivalent to id_types=lambda t: t in {UUID} schema = graphql_schema ( query = [ foo ], id_types = { UUID }) schema_str = \"\"\" \\ type Query { foo: Foo } type Foo { bar: ID! }\"\"\" assert print_schema ( schema ) == schema_str Note ID type could also be identified using typing.Annotated and a predicate looking into annotations. apischema also provides a simple ID type with apischema.graphql.ID . It is just defined as a NewType of string, so you can use it when you want to manipulate raw ID strings in your resolvers. ID encoding \u00b6 ID encoding can directly be controlled the id_encoding parameters of graphql_schema . A current practice is to use base64 encoding for ID . from base64 import b64decode , b64encode from dataclasses import dataclass from uuid import UUID from graphql import graphql_sync from apischema.graphql import graphql_schema @dataclass class Foo : id : UUID def foo () -> Foo | None : return Foo ( UUID ( \"58c88e87-5769-4723-8974-f9ec5007a38b\" )) schema = graphql_schema ( query = [ foo ], id_types = { UUID }, id_encoding = ( lambda s : b64decode ( s ) . decode (), lambda s : b64encode ( s . encode ()) . decode (), ), ) assert graphql_sync ( schema , \"{foo {id} }\" ) . data == { \"foo\" : { \"id\" : \"NThjODhlODctNTc2OS00NzIzLTg5NzQtZjllYzUwMDdhMzhi\" } } Note You can also use relay.base64_encoding (see next section ) Note ID serialization (respectively deserialization) is applied after apischema conversions (respectively before apischema conversion): in the example, uuid is already converted into string before being passed to id_serializer . If you use base64 encodeing and an ID type which is converted by apischema to a base64 str, you will get a double encoded base64 string Tagged unions \u00b6 Important This feature has a provisional status, as the concerned GraphQL RFC is not finalized. apischema provides a apischema.tagged_unions.TaggedUnion base class which helps to implement the tagged union pattern. It's fields must be typed using apischema.tagged_unions.Tagged generic type. from dataclasses import dataclass import pytest from apischema import Undefined , ValidationError , alias , deserialize , schema , serialize from apischema.tagged_unions import Tagged , TaggedUnion , get_tagged @dataclass class Bar : field : str class Foo ( TaggedUnion ): bar : Tagged [ Bar ] # Tagged can have metadata like a dataclass fields i : Tagged [ int ] = Tagged ( alias ( \"baz\" ) | schema ( min = 0 )) # Instantiate using class fields tagged_bar = Foo . bar ( Bar ( \"value\" )) # you can also use default constructor, but it's not typed-checked assert tagged_bar == Foo ( bar = Bar ( \"value\" )) # All fields that are not tagged are Undefined assert tagged_bar . bar is not Undefined and tagged_bar . i is Undefined # get_tagged allows to retrieve the tag and it's value # (but the value is not typed-checked) assert get_tagged ( tagged_bar ) == ( \"bar\" , Bar ( \"value\" )) # (De)serialization works as expected assert deserialize ( Foo , { \"bar\" : { \"field\" : \"value\" }}) == tagged_bar assert serialize ( Foo , tagged_bar ) == { \"bar\" : { \"field\" : \"value\" }} with pytest . raises ( ValidationError ) as err : deserialize ( Foo , { \"unknown\" : 42 }) assert err . value . errors == [{ \"loc\" : [ \"unknown\" ], \"err\" : \"unexpected property\" }] with pytest . raises ( ValidationError ) as err : deserialize ( Foo , { \"bar\" : { \"field\" : \"value\" }, \"baz\" : 0 }) assert err . value . errors == [ { \"loc\" : [], \"err\" : \"property count greater than 1 (maxProperties)\" } ] JSON schema \u00b6 Tagged unions JSON schema uses minProperties: 1 and maxProperties: 1 . from dataclasses import dataclass from apischema.json_schema import deserialization_schema , serialization_schema from apischema.tagged_unions import Tagged , TaggedUnion @dataclass class Bar : field : str class Foo ( TaggedUnion ): bar : Tagged [ Bar ] baz : Tagged [ int ] assert ( deserialization_schema ( Foo ) == serialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : \"object\" , \"properties\" : { \"field\" : { \"type\" : \"string\" }}, \"required\" : [ \"field\" ], \"additionalProperties\" : False , }, \"baz\" : { \"type\" : \"integer\" }, }, \"additionalProperties\" : False , \"minProperties\" : 1 , \"maxProperties\" : 1 , } ) GraphQL schema \u00b6 As tagged unions are not (yet?) part of the GraphQL spec, they are just implemented as normal (input) object type with nullable fields. An error is raised if several tags are passed in input. from dataclasses import dataclass from graphql import graphql_sync from graphql.utilities import print_schema from apischema.graphql import graphql_schema from apischema.tagged_unions import Tagged , TaggedUnion @dataclass class Bar : field : str class Foo ( TaggedUnion ): bar : Tagged [ Bar ] baz : Tagged [ int ] def query ( foo : Foo ) -> Foo : return foo schema = graphql_schema ( query = [ query ]) schema_str = \"\"\" \\ type Query { query(foo: FooInput!): Foo! } type Foo { bar: Bar baz: Int } type Bar { field: String! } input FooInput { bar: BarInput baz: Int } input BarInput { field: String! }\"\"\" assert print_schema ( schema ) == schema_str query_str = \"\"\" { query(foo: {bar: {field: \"value\"}}) { bar { field } baz } }\"\"\" assert graphql_sync ( schema , query_str ) . data == { \"query\" : { \"bar\" : { \"field\" : \"value\" }, \"baz\" : None } } FAQ \u00b6 Why TypedDict is not supported as an output type? \u00b6 At first, TypedDict subclasses are not real classes, so they cannot be used to check types at runtime. Runtime check is however requried to disambiguate unions/interfaces. A hack could be done to solve this issue, but there is another one which cannot be hacked: TypedDict inheritance hierarchy is lost at runtime, so they don't play nicely with the interface concept.","title":"Data model and resolvers"},{"location":"graphql/data_model_and_resolvers/#data-model-and-resolvers","text":"Almost everything in the Data model section remains valid in GraphQL integration, with a few differences.","title":"Data model and resolvers"},{"location":"graphql/data_model_and_resolvers/#graphql-specific-data-model","text":"","title":"GraphQL specific data model"},{"location":"graphql/data_model_and_resolvers/#enum","text":"Enum members are represented in the schema using their name instead of their value. This is more consistent with the way GraphQL represents enumerations.","title":"Enum"},{"location":"graphql/data_model_and_resolvers/#typeddict","text":"TypedDict is not supported as an output type. (see FAQ )","title":"TypedDict"},{"location":"graphql/data_model_and_resolvers/#union","text":"Unions are only supported between output object type, which means dataclass and NamedTuple (and conversions / dataclass model ). There are 2 exceptions which can be always be used in Union : None / Optional : Types are non-null (marked with an exclamation mark ! in GraphQL schema) by default; Optional types however results in normal GraphQL types (without ! ). apischema.UndefinedType : it is simply ignored. It is useful in resolvers, see following section","title":"Union"},{"location":"graphql/data_model_and_resolvers/#non-null","text":"Types are assumed to be non-null by default, as in Python typing. Nullable types are obtained using typing.Optional (or typing.Union with a None argument). Note There is one exception, when resolver parameter default value is not serializable (and thus cannot be included in the schema), the parameter type is then set as nullable to make the parameter non-required. For example parameters not Optional but with Undefined default value will be marked as nullable. This is only for the schema, the default value is still used at execution.","title":"Non-null"},{"location":"graphql/data_model_and_resolvers/#undefined","text":"In output, Undefined is converted to None ; so in the schema, Union[T, UndefinedType] will be nullable. In input, fields become nullable when Undefined is their default value.","title":"Undefined"},{"location":"graphql/data_model_and_resolvers/#interfaces","text":"Interfaces are simply classes marked with apischema.graphql.interface decorator. An object type implements an interface when its class inherits from an interface-marked class, or when it has flattened fields of interface-marked dataclass. from dataclasses import dataclass from graphql import print_schema from apischema.graphql import graphql_schema , interface @interface @dataclass class Bar : bar : int @dataclass class Foo ( Bar ): baz : str def foo () -> Foo | None : ... schema = graphql_schema ( query = [ foo ]) schema_str = \"\"\" \\ type Query { foo: Foo } type Foo implements Bar { bar: Int! baz: String! } interface Bar { bar: Int! }\"\"\" assert print_schema ( schema ) == schema_str","title":"Interfaces"},{"location":"graphql/data_model_and_resolvers/#resolvers","text":"All dataclass / NamedTuple fields (excepted skipped ) are resolved with their alias in the GraphQL schema. Custom resolvers can also be added by marking methods with apischema.graphql.resolver decorator \u2014 resolvers share a common interface with apischema.serialized , with a few differences. Methods can be synchronous or asynchronous (defined with async def or annotated with an typing.Awaitable return type). Resolvers parameters are included in the schema with their type, and their default value. from dataclasses import dataclass from graphql import print_schema from apischema.graphql import graphql_schema , resolver @dataclass class Bar : baz : int @dataclass class Foo : @resolver async def bar ( self , arg : int = 0 ) -> Bar : ... async def foo () -> Foo | None : ... schema = graphql_schema ( query = [ foo ]) schema_str = \"\"\" \\ type Query { foo: Foo } type Foo { bar(arg: Int! = 0): Bar! } type Bar { baz: Int! }\"\"\" assert print_schema ( schema ) == schema_str","title":"Resolvers"},{"location":"graphql/data_model_and_resolvers/#graphqlresolveinfo-parameter","text":"Resolvers can have an additional parameter of type graphql.GraphQLResolveInfo (or Optional[graphql.GraphQLResolveInfo] ), which is automatically injected when the resolver is executed in the context of a GraphQL request. This parameter contains the info about the current GraphQL request being executed.","title":"GraphQLResolveInfo parameter"},{"location":"graphql/data_model_and_resolvers/#undefined-parameter-default-null-vs-undefined","text":"Undefined can be used as default value of resolver parameters. It can be to distinguish a null input from an absent/ undefined input. In fact, null value will result in a None argument where no value will use the default value, Undefined so. from graphql import graphql_sync from apischema import Undefined , UndefinedType from apischema.graphql import graphql_schema def arg_is_absent ( arg : int | UndefinedType | None = Undefined ) -> bool : return arg is Undefined schema = graphql_schema ( query = [ arg_is_absent ]) assert graphql_sync ( schema , \"{argIsAbsent(arg: null)}\" ) . data == { \"argIsAbsent\" : False } assert graphql_sync ( schema , \" {argIsAbsent} \" ) . data == { \"argIsAbsent\" : True }","title":"Undefined parameter default \u2014 null vs. undefined"},{"location":"graphql/data_model_and_resolvers/#error-handling","text":"Errors occurring in resolvers can be caught in a dedicated error handler registered with error_handler parameter. This function takes in parameters the exception, the object, the info and the kwargs of the failing resolver; it can return a new value or raise the current or another exception \u2014 it can for example be used to log errors without throwing the complete serialization. The resulting serialization type will be a Union of the normal type and the error handling type; if the error handler always raises, use typing.NoReturn annotation. error_handler=None correspond to a default handler which only return None \u2014 exception is thus discarded and the resolver type becomes Optional . The error handler is only executed by apischema serialization process, it's not added to the function, so this one can be executed normally and raise an exception in the rest of your code. Error handler can be synchronous or asynchronous. from dataclasses import dataclass from logging import getLogger from typing import Any import graphql from graphql.utilities import print_schema from apischema.graphql import graphql_schema , resolver logger = getLogger ( __name__ ) def log_error ( error : Exception , obj : Any , info : graphql . GraphQLResolveInfo , ** kwargs ) -> None : logger . error ( \"Resolve error in %s \" , \".\" . join ( map ( str , info . path . as_list ())), exc_info = error ) return None @dataclass class Foo : @resolver ( error_handler = log_error ) def bar ( self ) -> int : raise RuntimeError ( \"Bar error\" ) @resolver def baz ( self ) -> int : raise RuntimeError ( \"Baz error\" ) def foo ( info : graphql . GraphQLResolveInfo ) -> Foo : return Foo () schema = graphql_schema ( query = [ foo ]) # Notice that bar is Int while baz is Int! schema_str = \"\"\" \\ type Query { foo: Foo! } type Foo { bar: Int baz: Int! }\"\"\" assert print_schema ( schema ) == schema_str # Logs \"Resolve error in foo.bar\", no error raised assert graphql . graphql_sync ( schema , \"{foo {bar} }\" ) . data == { \"foo\" : { \"bar\" : None }} # Error is raised assert graphql . graphql_sync ( schema , \"{foo {baz} }\" ) . errors [ 0 ] . message == \"Baz error\"","title":"Error handling"},{"location":"graphql/data_model_and_resolvers/#parameters-metadata","text":"Resolvers parameters can have metadata like dataclass fields. They can be passed using typing.Annotated . from dataclasses import dataclass from typing import Annotated from graphql.utilities import print_schema from apischema import alias , schema from apischema.graphql import graphql_schema , resolver @dataclass class Foo : @resolver def bar ( self , param : Annotated [ int , alias ( \"arg\" ) | schema ( description = \"argument\" )] ) -> int : return param def foo () -> Foo : return Foo () schema_ = graphql_schema ( query = [ foo ]) # Notice that bar is Int while baz is Int! schema_str = ''' \\ type Query { foo: Foo! } type Foo { bar( \"\"\"argument\"\"\" arg: Int! ): Int! }''' assert print_schema ( schema_ ) == schema_str Note Metadata can also be passed with parameters_metadata parameter; it takes a mapping of parameter names as key and mapped metadata as value.","title":"Parameters metadata"},{"location":"graphql/data_model_and_resolvers/#parameters-base-schema","text":"Following the example of type/field/method base schema , resolver parameters also support a base schema definition import inspect from dataclasses import dataclass from typing import Any , Callable import docstring_parser from graphql.utilities import print_schema from apischema import schema , settings from apischema.graphql import graphql_schema , resolver from apischema.schemas import Schema @dataclass class Foo : @resolver def bar ( self , arg : str ) -> int : \"\"\"bar method :param arg: arg parameter \"\"\" ... def method_base_schema ( tp : Any , method : Callable , alias : str ) -> Schema | None : return schema ( description = docstring_parser . parse ( method . __doc__ ) . short_description ) def parameter_base_schema ( method : Callable , parameter : inspect . Parameter , alias : str ) -> Schema | None : for doc_param in docstring_parser . parse ( method . __doc__ ) . params : if doc_param . arg_name == parameter . name : return schema ( description = doc_param . description ) return None settings . base_schema . method = method_base_schema settings . base_schema . parameter = parameter_base_schema def foo () -> Foo : ... schema_ = graphql_schema ( query = [ foo ]) schema_str = ''' \\ type Query { foo: Foo! } type Foo { \"\"\"bar method\"\"\" bar( \"\"\"arg parameter\"\"\" arg: String! ): Int! }''' assert print_schema ( schema_ ) == schema_str","title":"Parameters base schema"},{"location":"graphql/data_model_and_resolvers/#scalars","text":"NewType or non-object types annotated with type_name will be translated in the GraphQL schema by a scalar . By the way, Any will automatically be translated to a JSON scalar, as it is deserialized from and serialized to JSON. from dataclasses import dataclass from typing import Any from uuid import UUID from graphql.utilities import print_schema from apischema.graphql import graphql_schema @dataclass class Foo : id : UUID content : Any def foo () -> Foo | None : ... schema = graphql_schema ( query = [ foo ]) schema_str = \"\"\" \\ type Query { foo: Foo } type Foo { id: UUID! content: JSON } scalar UUID scalar JSON\"\"\" assert print_schema ( schema ) == schema_str","title":"Scalars"},{"location":"graphql/data_model_and_resolvers/#id-type","text":"GraphQL ID has no precise specification and is defined according API needs; it can be a UUID or/and ObjectId, etc. apischema.graphql_schema has a parameter id_types which can be used to define which types will be marked as ID in the generated schema. Parameter value can be either a collection of types (each type will then be mapped to ID scalar), or a predicate returning if the given type must be marked as ID . from dataclasses import dataclass from uuid import UUID from graphql import print_schema from apischema.graphql import graphql_schema @dataclass class Foo : bar : UUID def foo () -> Foo | None : ... # id_types={UUID} is equivalent to id_types=lambda t: t in {UUID} schema = graphql_schema ( query = [ foo ], id_types = { UUID }) schema_str = \"\"\" \\ type Query { foo: Foo } type Foo { bar: ID! }\"\"\" assert print_schema ( schema ) == schema_str Note ID type could also be identified using typing.Annotated and a predicate looking into annotations. apischema also provides a simple ID type with apischema.graphql.ID . It is just defined as a NewType of string, so you can use it when you want to manipulate raw ID strings in your resolvers.","title":"ID type"},{"location":"graphql/data_model_and_resolvers/#id-encoding","text":"ID encoding can directly be controlled the id_encoding parameters of graphql_schema . A current practice is to use base64 encoding for ID . from base64 import b64decode , b64encode from dataclasses import dataclass from uuid import UUID from graphql import graphql_sync from apischema.graphql import graphql_schema @dataclass class Foo : id : UUID def foo () -> Foo | None : return Foo ( UUID ( \"58c88e87-5769-4723-8974-f9ec5007a38b\" )) schema = graphql_schema ( query = [ foo ], id_types = { UUID }, id_encoding = ( lambda s : b64decode ( s ) . decode (), lambda s : b64encode ( s . encode ()) . decode (), ), ) assert graphql_sync ( schema , \"{foo {id} }\" ) . data == { \"foo\" : { \"id\" : \"NThjODhlODctNTc2OS00NzIzLTg5NzQtZjllYzUwMDdhMzhi\" } } Note You can also use relay.base64_encoding (see next section ) Note ID serialization (respectively deserialization) is applied after apischema conversions (respectively before apischema conversion): in the example, uuid is already converted into string before being passed to id_serializer . If you use base64 encodeing and an ID type which is converted by apischema to a base64 str, you will get a double encoded base64 string","title":"ID encoding"},{"location":"graphql/data_model_and_resolvers/#tagged-unions","text":"Important This feature has a provisional status, as the concerned GraphQL RFC is not finalized. apischema provides a apischema.tagged_unions.TaggedUnion base class which helps to implement the tagged union pattern. It's fields must be typed using apischema.tagged_unions.Tagged generic type. from dataclasses import dataclass import pytest from apischema import Undefined , ValidationError , alias , deserialize , schema , serialize from apischema.tagged_unions import Tagged , TaggedUnion , get_tagged @dataclass class Bar : field : str class Foo ( TaggedUnion ): bar : Tagged [ Bar ] # Tagged can have metadata like a dataclass fields i : Tagged [ int ] = Tagged ( alias ( \"baz\" ) | schema ( min = 0 )) # Instantiate using class fields tagged_bar = Foo . bar ( Bar ( \"value\" )) # you can also use default constructor, but it's not typed-checked assert tagged_bar == Foo ( bar = Bar ( \"value\" )) # All fields that are not tagged are Undefined assert tagged_bar . bar is not Undefined and tagged_bar . i is Undefined # get_tagged allows to retrieve the tag and it's value # (but the value is not typed-checked) assert get_tagged ( tagged_bar ) == ( \"bar\" , Bar ( \"value\" )) # (De)serialization works as expected assert deserialize ( Foo , { \"bar\" : { \"field\" : \"value\" }}) == tagged_bar assert serialize ( Foo , tagged_bar ) == { \"bar\" : { \"field\" : \"value\" }} with pytest . raises ( ValidationError ) as err : deserialize ( Foo , { \"unknown\" : 42 }) assert err . value . errors == [{ \"loc\" : [ \"unknown\" ], \"err\" : \"unexpected property\" }] with pytest . raises ( ValidationError ) as err : deserialize ( Foo , { \"bar\" : { \"field\" : \"value\" }, \"baz\" : 0 }) assert err . value . errors == [ { \"loc\" : [], \"err\" : \"property count greater than 1 (maxProperties)\" } ]","title":"Tagged unions"},{"location":"graphql/data_model_and_resolvers/#json-schema","text":"Tagged unions JSON schema uses minProperties: 1 and maxProperties: 1 . from dataclasses import dataclass from apischema.json_schema import deserialization_schema , serialization_schema from apischema.tagged_unions import Tagged , TaggedUnion @dataclass class Bar : field : str class Foo ( TaggedUnion ): bar : Tagged [ Bar ] baz : Tagged [ int ] assert ( deserialization_schema ( Foo ) == serialization_schema ( Foo ) == { \"$schema\" : \"http://json-schema.org/draft/2020-12/schema#\" , \"type\" : \"object\" , \"properties\" : { \"bar\" : { \"type\" : \"object\" , \"properties\" : { \"field\" : { \"type\" : \"string\" }}, \"required\" : [ \"field\" ], \"additionalProperties\" : False , }, \"baz\" : { \"type\" : \"integer\" }, }, \"additionalProperties\" : False , \"minProperties\" : 1 , \"maxProperties\" : 1 , } )","title":"JSON schema"},{"location":"graphql/data_model_and_resolvers/#graphql-schema","text":"As tagged unions are not (yet?) part of the GraphQL spec, they are just implemented as normal (input) object type with nullable fields. An error is raised if several tags are passed in input. from dataclasses import dataclass from graphql import graphql_sync from graphql.utilities import print_schema from apischema.graphql import graphql_schema from apischema.tagged_unions import Tagged , TaggedUnion @dataclass class Bar : field : str class Foo ( TaggedUnion ): bar : Tagged [ Bar ] baz : Tagged [ int ] def query ( foo : Foo ) -> Foo : return foo schema = graphql_schema ( query = [ query ]) schema_str = \"\"\" \\ type Query { query(foo: FooInput!): Foo! } type Foo { bar: Bar baz: Int } type Bar { field: String! } input FooInput { bar: BarInput baz: Int } input BarInput { field: String! }\"\"\" assert print_schema ( schema ) == schema_str query_str = \"\"\" { query(foo: {bar: {field: \"value\"}}) { bar { field } baz } }\"\"\" assert graphql_sync ( schema , query_str ) . data == { \"query\" : { \"bar\" : { \"field\" : \"value\" }, \"baz\" : None } }","title":"GraphQL schema"},{"location":"graphql/data_model_and_resolvers/#faq","text":"","title":"FAQ"},{"location":"graphql/data_model_and_resolvers/#why-typeddict-is-not-supported-as-an-output-type","text":"At first, TypedDict subclasses are not real classes, so they cannot be used to check types at runtime. Runtime check is however requried to disambiguate unions/interfaces. A hack could be done to solve this issue, but there is another one which cannot be hacked: TypedDict inheritance hierarchy is lost at runtime, so they don't play nicely with the interface concept.","title":"Why TypedDict is not supported as an output type?"},{"location":"graphql/overview/","text":"GraphQL Overview \u00b6 apischema supports GraphQL through the graphql-core library. You can install this dependency directly with apischema using the following extra requirement: pip install apischema [ graphql ] GraphQL supports consists of generating a GraphQL schema graphql.GraphQLSchema from your data model and endpoints (queries/mutations/subscribtions), in a similar way than the JSON schema generation. This schema can then be used through graphql-core library to query/mutate/subscribe. from dataclasses import dataclass from datetime import date , datetime from typing import Collection from uuid import UUID , uuid4 from graphql import graphql_sync , print_schema from apischema.graphql import graphql_schema , resolver @dataclass class User : id : UUID username : str birthday : date | None = None @resolver def posts ( self ) -> Collection [ \"Post\" ]: return [ post for post in POSTS if post . author . id == self . id ] @dataclass class Post : id : UUID author : User date : datetime content : str USERS = [ User ( uuid4 (), \"foo\" ), User ( uuid4 (), \"bar\" )] POSTS = [ Post ( uuid4 (), USERS [ 0 ], datetime . now (), \"Hello world!\" )] def users () -> Collection [ User ]: return USERS def posts () -> Collection [ Post ]: return POSTS def user ( username : str ) -> User | None : for user in users (): if user . username == username : return user else : return None schema = graphql_schema ( query = [ users , user , posts ], id_types = { UUID }) schema_str = \"\"\" \\ type Query { users: [User!]! user(username: String!): User posts: [Post!]! } type User { id: ID! username: String! birthday: Date posts: [Post!]! } scalar Date type Post { id: ID! author: User! date: Datetime! content: String! } scalar Datetime\"\"\" assert print_schema ( schema ) == schema_str query = \"\"\" { users { username posts { content } } }\"\"\" assert graphql_sync ( schema , query ) . data == { \"users\" : [ { \"username\" : \"foo\" , \"posts\" : [{ \"content\" : \"Hello world!\" }]}, { \"username\" : \"bar\" , \"posts\" : []}, ] } GraphQL is fully integrated with the rest of apischema features, especially conversions , so it's easy to integrate ORM and other custom types in the generated schema; this concerns query results but also arguments. By the way, while GraphQL doesn't support constraints, apischema still offers you all the power of its validation feature . In fact, apischema deserialize and validate all the arguments passed to resolvers. FAQ \u00b6 Is it possible to use the same classes to do both GraphQL and REST-API? \u00b6 Yes it is. GraphQL has some restrictions in comparison to JSON schema (see next section ), but this taken in account, all of your code can be reused. In fact, GraphQL endpoints can also be used both by a GraphQL API and a more traditional REST or RPC API.","title":"Overview"},{"location":"graphql/overview/#graphql-overview","text":"apischema supports GraphQL through the graphql-core library. You can install this dependency directly with apischema using the following extra requirement: pip install apischema [ graphql ] GraphQL supports consists of generating a GraphQL schema graphql.GraphQLSchema from your data model and endpoints (queries/mutations/subscribtions), in a similar way than the JSON schema generation. This schema can then be used through graphql-core library to query/mutate/subscribe. from dataclasses import dataclass from datetime import date , datetime from typing import Collection from uuid import UUID , uuid4 from graphql import graphql_sync , print_schema from apischema.graphql import graphql_schema , resolver @dataclass class User : id : UUID username : str birthday : date | None = None @resolver def posts ( self ) -> Collection [ \"Post\" ]: return [ post for post in POSTS if post . author . id == self . id ] @dataclass class Post : id : UUID author : User date : datetime content : str USERS = [ User ( uuid4 (), \"foo\" ), User ( uuid4 (), \"bar\" )] POSTS = [ Post ( uuid4 (), USERS [ 0 ], datetime . now (), \"Hello world!\" )] def users () -> Collection [ User ]: return USERS def posts () -> Collection [ Post ]: return POSTS def user ( username : str ) -> User | None : for user in users (): if user . username == username : return user else : return None schema = graphql_schema ( query = [ users , user , posts ], id_types = { UUID }) schema_str = \"\"\" \\ type Query { users: [User!]! user(username: String!): User posts: [Post!]! } type User { id: ID! username: String! birthday: Date posts: [Post!]! } scalar Date type Post { id: ID! author: User! date: Datetime! content: String! } scalar Datetime\"\"\" assert print_schema ( schema ) == schema_str query = \"\"\" { users { username posts { content } } }\"\"\" assert graphql_sync ( schema , query ) . data == { \"users\" : [ { \"username\" : \"foo\" , \"posts\" : [{ \"content\" : \"Hello world!\" }]}, { \"username\" : \"bar\" , \"posts\" : []}, ] } GraphQL is fully integrated with the rest of apischema features, especially conversions , so it's easy to integrate ORM and other custom types in the generated schema; this concerns query results but also arguments. By the way, while GraphQL doesn't support constraints, apischema still offers you all the power of its validation feature . In fact, apischema deserialize and validate all the arguments passed to resolvers.","title":"GraphQL Overview"},{"location":"graphql/overview/#faq","text":"","title":"FAQ"},{"location":"graphql/overview/#is-it-possible-to-use-the-same-classes-to-do-both-graphql-and-rest-api","text":"Yes it is. GraphQL has some restrictions in comparison to JSON schema (see next section ), but this taken in account, all of your code can be reused. In fact, GraphQL endpoints can also be used both by a GraphQL API and a more traditional REST or RPC API.","title":"Is it possible to use the same classes to do both GraphQL and REST-API?"},{"location":"graphql/relay/","text":"Relay \u00b6 apischema provides some facilities to implement a GraphQL server following Relay GraphQL server specification . They are included in the module apischema.graphql.relay . Note These facilities are independent of each others \u2014 you could keep only the mutations part and use your own identification and connection system for example. (Global) Object Identification \u00b6 apischema defines a generic relay.Node[Id] interface which can be used which can be used as base class of all identified resources. This class contains a unique generic field of type Id , which will be automatically converted into an ID! in the schema. The Id type chosen has to be serializable into a string-convertible value (it can register conversions if needed). Each node has to implement the classmethod get_by_id(cls: type[T], id: Id, info: graphql.GraphQLResolveInfo=None) -> T . All nodes defined can be retrieved using relay.nodes , while the node query is defined as relay.node . relay.nodes() can be passed to graphql_schema types parameter in order to add them in the schema even if they don't appear in any resolvers. from dataclasses import dataclass from uuid import UUID import graphql from graphql.utilities import print_schema from apischema.graphql import graphql_schema , relay @dataclass class Ship ( relay . Node [ UUID ]): # Let's use an UUID for Ship id name : str @classmethod async def get_by_id ( cls , id : UUID , info : graphql . GraphQLResolveInfo = None ): ... @dataclass class Faction ( relay . Node [ int ]): # Nodes can have different id types name : str @classmethod def get_by_id ( cls , id : int , info : graphql . GraphQLResolveInfo = None ) -> \"Faction\" : ... schema = graphql_schema ( query = [ relay . node ], types = relay . nodes ()) schema_str = \"\"\" \\ type Ship implements Node { id: ID! name: String! } interface Node { id: ID! } type Faction implements Node { id: ID! name: String! } type Query { node(id: ID!): Node! }\"\"\" assert print_schema ( schema ) == schema_str Warning For now, even if its result is not used, relay.nodes must be called before generating the schema. Global ID \u00b6 apischema defines a relay.GlobalId type with the following signature : @dataclass class GlobalId ( Generic [ Node ]): id : str node_class : type [ Node ] In fact, it is GlobalId type which is serialized and deserialized as an ID! , not the Id parameter of the Node class; apischema automatically add a field converter to make the conversion between the Id (for example an UUID ) of a given node and the corresponding GlobalId . Node instance global id can be retrieved with global_id property. from dataclasses import dataclass import graphql from apischema import serialize from apischema.graphql import graphql_schema , relay @dataclass class Faction ( relay . Node [ int ]): name : str @classmethod def get_by_id ( cls , id : int , info : graphql . GraphQLResolveInfo = None ) -> \"Faction\" : return [ Faction ( 0 , \"Empire\" ), Faction ( 1 , \"Rebels\" )][ id ] schema = graphql_schema ( query = [ relay . node ], types = relay . nodes ()) some_global_id = Faction . get_by_id ( 0 ) . global_id # Let's pick a global id ... assert some_global_id == relay . GlobalId ( \"0\" , Faction ) query = \"\"\" query factionName($id: ID!) { node(id: $id) { ... on Faction { name } } }\"\"\" assert graphql . graphql_sync ( # ... and use it in a query schema , query , variable_values = { \"id\" : serialize ( relay . GlobalId , some_global_id )} ) . data == { \"node\" : { \"name\" : \"Empire\" }} Id encoding \u00b6 Relay specifications encourage the use of base64 encoding, so apischema defines a relay.base64_encoding that you can pass to graphql_schema id_encoding parameter. Connections \u00b6 apischema provides a generic relay.Connection[Node, Cursor, Edge] type, which can be used directly without subclassing it; it's also possible to subclass it to add fields to a given connection (or to all the connection which will subclass the subclass). relay.Edge[Node, Cursor] can also be subclassed to add fields to the edges. Connection dataclass has the following declaration: @dataclass class Connection ( Generic [ Node , Cursor , Edge ]): edges : Optional [ Sequence [ Optional [ Edge ]]] has_previous_page : bool = field ( default = False , metadata = skip ) has_next_page : bool = field ( default = False , metadata = skip ) start_cursor : Optional [ Cursor ] = field ( default = None , metadata = skip ) end_cursor : Optional [ Cursor ] = field ( default = None , metadata = skip ) @resolver def page_info ( self ) -> PageInfo [ Cursor ]: ... The pageInfo field is computed by a resolver; it uses the cursors of the first and the last edge when they are not provided. Here is an example of Connection use: from dataclasses import dataclass from typing import Optional , TypeVar import graphql from graphql.utilities import print_schema from apischema.graphql import graphql_schema , relay , resolver Cursor = int # let's use an integer cursor in all our connection Node = TypeVar ( \"Node\" ) Connection = relay . Connection [ Node , Cursor , relay . Edge [ Node , Cursor ]] # Connection can now be used just like Connection[Ship] or Connection[Faction | None] @dataclass class Ship : name : str @dataclass class Faction : @resolver def ships ( self , first : int | None , after : Cursor | None ) -> Connection [ Optional [ Ship ]] | None : edges = [ relay . Edge ( Ship ( \"X-Wing\" ), 0 ), relay . Edge ( Ship ( \"B-Wing\" ), 1 )] return Connection ( edges , relay . PageInfo . from_edges ( edges )) def faction () -> Faction | None : return Faction () schema = graphql_schema ( query = [ faction ]) schema_str = \"\"\" \\ type Query { faction: Faction } type Faction { ships(first: Int, after: Int): ShipConnection } type ShipConnection { edges: [ShipEdge] pageInfo: PageInfo! } type ShipEdge { node: Ship cursor: Int! } type Ship { name: String! } type PageInfo { hasPreviousPage: Boolean! hasNextPage: Boolean! startCursor: Int endCursor: Int }\"\"\" assert print_schema ( schema ) == schema_str query = \"\"\" { faction { ships { pageInfo { endCursor hasNextPage } edges { cursor node { name } } } } }\"\"\" assert graphql . graphql_sync ( schema , query ) . data == { \"faction\" : { \"ships\" : { \"pageInfo\" : { \"endCursor\" : 1 , \"hasNextPage\" : False }, \"edges\" : [ { \"cursor\" : 0 , \"node\" : { \"name\" : \"X-Wing\" }}, { \"cursor\" : 1 , \"node\" : { \"name\" : \"B-Wing\" }}, ], } } } Custom connections/edges \u00b6 Connections can be customized by simply subclassing relay.Connection class and adding the additional fields. For the edges, relay.Edge can be subclassed too, and the subclass has then to be passed as type argument to the generic connection. from dataclasses import dataclass from typing import Optional , TypeVar from graphql import print_schema from apischema.graphql import graphql_schema , relay , resolver Cursor = int Node = TypeVar ( \"Node\" ) Edge = TypeVar ( \"Edge\" , bound = relay . Edge ) @dataclass class MyConnection ( relay . Connection [ Node , Cursor , Edge ]): connection_field : bool @dataclass class MyEdge ( relay . Edge [ Node , Cursor ]): edge_field : int | None Connection = MyConnection [ Node , MyEdge [ Node ]] @dataclass class Ship : name : str @dataclass class Faction : @resolver def ships ( self , first : int | None , after : Cursor | None ) -> Connection [ Optional [ Ship ]] | None : ... def faction () -> Faction | None : return Faction () schema = graphql_schema ( query = [ faction ]) schema_str = \"\"\" \\ type Query { faction: Faction } type Faction { ships(first: Int, after: Int): ShipConnection } type ShipConnection { edges: [ShipEdge] pageInfo: PageInfo! connectionField: Boolean! } type ShipEdge { node: Ship cursor: Int! edgeField: Int } type Ship { name: String! } type PageInfo { hasPreviousPage: Boolean! hasNextPage: Boolean! startCursor: Int endCursor: Int }\"\"\" assert print_schema ( schema ) == schema_str Mutations \u00b6 Relay compliant mutations can be declared with a dataclass subclassing the relay.Mutation class; its fields will be put in the payload type of the mutation. This class must implement a classmethod / staticmethod name mutate ; it can be synchronous or asynchronous. The arguments of the method will correspond to the input type fields. The mutation will be named after the name of the mutation class. All the mutations declared can be retrieved with relay.mutations , in order to be passed to graphql_schema . from dataclasses import dataclass from graphql.utilities import print_schema from apischema.graphql import graphql_schema , relay @dataclass class Ship : ... @dataclass class Faction : ... @dataclass class IntroduceShip ( relay . Mutation ): ship : Ship faction : Faction @staticmethod def mutate ( faction_id : str , ship_name : str ) -> \"IntroduceShip\" : ... def hello () -> str : return \"world\" schema = graphql_schema ( query = [ hello ], mutation = relay . mutations ()) schema_str = \"\"\" \\ type Query { hello: String! } type Mutation { introduceShip(input: IntroduceShipInput!): IntroduceShipPayload! } type IntroduceShipPayload { ship: Ship! faction: Faction! clientMutationId: String } type Ship type Faction input IntroduceShipInput { factionId: String! shipName: String! clientMutationId: String }\"\"\" assert print_schema ( schema ) == schema_str ClientMutationId \u00b6 As you can see in the previous example, the field named clientMutationId is automatically added to the input and the payload types. The forward of the mutation id from the input to the payload is automatically handled. It's value can be accessed by declaring a parameter of type relay.ClientMutationId \u2014 even if the parameter is not named client_mutation_id , it will be renamed internally. This feature is controlled by a Mutation class variable _client_mutation_id , with 3 possible values: None (automatic, the default): clientMutationId field will be nullable unless it's declared as a required parameter (without default value) in the mutate method. False : their will be no clientMutationId field added (having a dedicated parameter will raise an error) True : clientMutationId is added and forced to be non-null. from dataclasses import dataclass from graphql.utilities import print_schema from apischema.graphql import graphql_schema , relay @dataclass class Ship : ... @dataclass class Faction : ... @dataclass class IntroduceShip ( relay . Mutation ): ship : Ship faction : Faction @staticmethod def mutate ( # mut_id is required because no default value faction_id : str , ship_name : str , mut_id : relay . ClientMutationId , ) -> \"IntroduceShip\" : ... def hello () -> str : return \"world\" schema = graphql_schema ( query = [ hello ], mutation = relay . mutations ()) # clientMutationId field becomes non nullable in introduceShip types schema_str = \"\"\" \\ type Query { hello: String! } type Mutation { introduceShip(input: IntroduceShipInput!): IntroduceShipPayload! } type IntroduceShipPayload { ship: Ship! faction: Faction! clientMutationId: String! } type Ship type Faction input IntroduceShipInput { factionId: String! shipName: String! clientMutationId: String! }\"\"\" assert print_schema ( schema ) == schema_str Error handling and other resolver arguments \u00b6 Relay mutation are operations , so they can be configured with the same parameters. As they are declared as classes, parameters will be passed as class variables, prefixed by _ ( error_handler becomes _error_handler ) Note Because parameters are class variables, you can reuse them by setting their value in a base class; for example, to share a same error_handler in a group of mutations.","title":"Relay"},{"location":"graphql/relay/#relay","text":"apischema provides some facilities to implement a GraphQL server following Relay GraphQL server specification . They are included in the module apischema.graphql.relay . Note These facilities are independent of each others \u2014 you could keep only the mutations part and use your own identification and connection system for example.","title":"Relay"},{"location":"graphql/relay/#global-object-identification","text":"apischema defines a generic relay.Node[Id] interface which can be used which can be used as base class of all identified resources. This class contains a unique generic field of type Id , which will be automatically converted into an ID! in the schema. The Id type chosen has to be serializable into a string-convertible value (it can register conversions if needed). Each node has to implement the classmethod get_by_id(cls: type[T], id: Id, info: graphql.GraphQLResolveInfo=None) -> T . All nodes defined can be retrieved using relay.nodes , while the node query is defined as relay.node . relay.nodes() can be passed to graphql_schema types parameter in order to add them in the schema even if they don't appear in any resolvers. from dataclasses import dataclass from uuid import UUID import graphql from graphql.utilities import print_schema from apischema.graphql import graphql_schema , relay @dataclass class Ship ( relay . Node [ UUID ]): # Let's use an UUID for Ship id name : str @classmethod async def get_by_id ( cls , id : UUID , info : graphql . GraphQLResolveInfo = None ): ... @dataclass class Faction ( relay . Node [ int ]): # Nodes can have different id types name : str @classmethod def get_by_id ( cls , id : int , info : graphql . GraphQLResolveInfo = None ) -> \"Faction\" : ... schema = graphql_schema ( query = [ relay . node ], types = relay . nodes ()) schema_str = \"\"\" \\ type Ship implements Node { id: ID! name: String! } interface Node { id: ID! } type Faction implements Node { id: ID! name: String! } type Query { node(id: ID!): Node! }\"\"\" assert print_schema ( schema ) == schema_str Warning For now, even if its result is not used, relay.nodes must be called before generating the schema.","title":"(Global) Object Identification"},{"location":"graphql/relay/#global-id","text":"apischema defines a relay.GlobalId type with the following signature : @dataclass class GlobalId ( Generic [ Node ]): id : str node_class : type [ Node ] In fact, it is GlobalId type which is serialized and deserialized as an ID! , not the Id parameter of the Node class; apischema automatically add a field converter to make the conversion between the Id (for example an UUID ) of a given node and the corresponding GlobalId . Node instance global id can be retrieved with global_id property. from dataclasses import dataclass import graphql from apischema import serialize from apischema.graphql import graphql_schema , relay @dataclass class Faction ( relay . Node [ int ]): name : str @classmethod def get_by_id ( cls , id : int , info : graphql . GraphQLResolveInfo = None ) -> \"Faction\" : return [ Faction ( 0 , \"Empire\" ), Faction ( 1 , \"Rebels\" )][ id ] schema = graphql_schema ( query = [ relay . node ], types = relay . nodes ()) some_global_id = Faction . get_by_id ( 0 ) . global_id # Let's pick a global id ... assert some_global_id == relay . GlobalId ( \"0\" , Faction ) query = \"\"\" query factionName($id: ID!) { node(id: $id) { ... on Faction { name } } }\"\"\" assert graphql . graphql_sync ( # ... and use it in a query schema , query , variable_values = { \"id\" : serialize ( relay . GlobalId , some_global_id )} ) . data == { \"node\" : { \"name\" : \"Empire\" }}","title":"Global ID"},{"location":"graphql/relay/#id-encoding","text":"Relay specifications encourage the use of base64 encoding, so apischema defines a relay.base64_encoding that you can pass to graphql_schema id_encoding parameter.","title":"Id encoding"},{"location":"graphql/relay/#connections","text":"apischema provides a generic relay.Connection[Node, Cursor, Edge] type, which can be used directly without subclassing it; it's also possible to subclass it to add fields to a given connection (or to all the connection which will subclass the subclass). relay.Edge[Node, Cursor] can also be subclassed to add fields to the edges. Connection dataclass has the following declaration: @dataclass class Connection ( Generic [ Node , Cursor , Edge ]): edges : Optional [ Sequence [ Optional [ Edge ]]] has_previous_page : bool = field ( default = False , metadata = skip ) has_next_page : bool = field ( default = False , metadata = skip ) start_cursor : Optional [ Cursor ] = field ( default = None , metadata = skip ) end_cursor : Optional [ Cursor ] = field ( default = None , metadata = skip ) @resolver def page_info ( self ) -> PageInfo [ Cursor ]: ... The pageInfo field is computed by a resolver; it uses the cursors of the first and the last edge when they are not provided. Here is an example of Connection use: from dataclasses import dataclass from typing import Optional , TypeVar import graphql from graphql.utilities import print_schema from apischema.graphql import graphql_schema , relay , resolver Cursor = int # let's use an integer cursor in all our connection Node = TypeVar ( \"Node\" ) Connection = relay . Connection [ Node , Cursor , relay . Edge [ Node , Cursor ]] # Connection can now be used just like Connection[Ship] or Connection[Faction | None] @dataclass class Ship : name : str @dataclass class Faction : @resolver def ships ( self , first : int | None , after : Cursor | None ) -> Connection [ Optional [ Ship ]] | None : edges = [ relay . Edge ( Ship ( \"X-Wing\" ), 0 ), relay . Edge ( Ship ( \"B-Wing\" ), 1 )] return Connection ( edges , relay . PageInfo . from_edges ( edges )) def faction () -> Faction | None : return Faction () schema = graphql_schema ( query = [ faction ]) schema_str = \"\"\" \\ type Query { faction: Faction } type Faction { ships(first: Int, after: Int): ShipConnection } type ShipConnection { edges: [ShipEdge] pageInfo: PageInfo! } type ShipEdge { node: Ship cursor: Int! } type Ship { name: String! } type PageInfo { hasPreviousPage: Boolean! hasNextPage: Boolean! startCursor: Int endCursor: Int }\"\"\" assert print_schema ( schema ) == schema_str query = \"\"\" { faction { ships { pageInfo { endCursor hasNextPage } edges { cursor node { name } } } } }\"\"\" assert graphql . graphql_sync ( schema , query ) . data == { \"faction\" : { \"ships\" : { \"pageInfo\" : { \"endCursor\" : 1 , \"hasNextPage\" : False }, \"edges\" : [ { \"cursor\" : 0 , \"node\" : { \"name\" : \"X-Wing\" }}, { \"cursor\" : 1 , \"node\" : { \"name\" : \"B-Wing\" }}, ], } } }","title":"Connections"},{"location":"graphql/relay/#custom-connectionsedges","text":"Connections can be customized by simply subclassing relay.Connection class and adding the additional fields. For the edges, relay.Edge can be subclassed too, and the subclass has then to be passed as type argument to the generic connection. from dataclasses import dataclass from typing import Optional , TypeVar from graphql import print_schema from apischema.graphql import graphql_schema , relay , resolver Cursor = int Node = TypeVar ( \"Node\" ) Edge = TypeVar ( \"Edge\" , bound = relay . Edge ) @dataclass class MyConnection ( relay . Connection [ Node , Cursor , Edge ]): connection_field : bool @dataclass class MyEdge ( relay . Edge [ Node , Cursor ]): edge_field : int | None Connection = MyConnection [ Node , MyEdge [ Node ]] @dataclass class Ship : name : str @dataclass class Faction : @resolver def ships ( self , first : int | None , after : Cursor | None ) -> Connection [ Optional [ Ship ]] | None : ... def faction () -> Faction | None : return Faction () schema = graphql_schema ( query = [ faction ]) schema_str = \"\"\" \\ type Query { faction: Faction } type Faction { ships(first: Int, after: Int): ShipConnection } type ShipConnection { edges: [ShipEdge] pageInfo: PageInfo! connectionField: Boolean! } type ShipEdge { node: Ship cursor: Int! edgeField: Int } type Ship { name: String! } type PageInfo { hasPreviousPage: Boolean! hasNextPage: Boolean! startCursor: Int endCursor: Int }\"\"\" assert print_schema ( schema ) == schema_str","title":"Custom connections/edges"},{"location":"graphql/relay/#mutations","text":"Relay compliant mutations can be declared with a dataclass subclassing the relay.Mutation class; its fields will be put in the payload type of the mutation. This class must implement a classmethod / staticmethod name mutate ; it can be synchronous or asynchronous. The arguments of the method will correspond to the input type fields. The mutation will be named after the name of the mutation class. All the mutations declared can be retrieved with relay.mutations , in order to be passed to graphql_schema . from dataclasses import dataclass from graphql.utilities import print_schema from apischema.graphql import graphql_schema , relay @dataclass class Ship : ... @dataclass class Faction : ... @dataclass class IntroduceShip ( relay . Mutation ): ship : Ship faction : Faction @staticmethod def mutate ( faction_id : str , ship_name : str ) -> \"IntroduceShip\" : ... def hello () -> str : return \"world\" schema = graphql_schema ( query = [ hello ], mutation = relay . mutations ()) schema_str = \"\"\" \\ type Query { hello: String! } type Mutation { introduceShip(input: IntroduceShipInput!): IntroduceShipPayload! } type IntroduceShipPayload { ship: Ship! faction: Faction! clientMutationId: String } type Ship type Faction input IntroduceShipInput { factionId: String! shipName: String! clientMutationId: String }\"\"\" assert print_schema ( schema ) == schema_str","title":"Mutations"},{"location":"graphql/relay/#clientmutationid","text":"As you can see in the previous example, the field named clientMutationId is automatically added to the input and the payload types. The forward of the mutation id from the input to the payload is automatically handled. It's value can be accessed by declaring a parameter of type relay.ClientMutationId \u2014 even if the parameter is not named client_mutation_id , it will be renamed internally. This feature is controlled by a Mutation class variable _client_mutation_id , with 3 possible values: None (automatic, the default): clientMutationId field will be nullable unless it's declared as a required parameter (without default value) in the mutate method. False : their will be no clientMutationId field added (having a dedicated parameter will raise an error) True : clientMutationId is added and forced to be non-null. from dataclasses import dataclass from graphql.utilities import print_schema from apischema.graphql import graphql_schema , relay @dataclass class Ship : ... @dataclass class Faction : ... @dataclass class IntroduceShip ( relay . Mutation ): ship : Ship faction : Faction @staticmethod def mutate ( # mut_id is required because no default value faction_id : str , ship_name : str , mut_id : relay . ClientMutationId , ) -> \"IntroduceShip\" : ... def hello () -> str : return \"world\" schema = graphql_schema ( query = [ hello ], mutation = relay . mutations ()) # clientMutationId field becomes non nullable in introduceShip types schema_str = \"\"\" \\ type Query { hello: String! } type Mutation { introduceShip(input: IntroduceShipInput!): IntroduceShipPayload! } type IntroduceShipPayload { ship: Ship! faction: Faction! clientMutationId: String! } type Ship type Faction input IntroduceShipInput { factionId: String! shipName: String! clientMutationId: String! }\"\"\" assert print_schema ( schema ) == schema_str","title":"ClientMutationId"},{"location":"graphql/relay/#error-handling-and-other-resolver-arguments","text":"Relay mutation are operations , so they can be configured with the same parameters. As they are declared as classes, parameters will be passed as class variables, prefixed by _ ( error_handler becomes _error_handler ) Note Because parameters are class variables, you can reuse them by setting their value in a base class; for example, to share a same error_handler in a group of mutations.","title":"Error handling and other resolver arguments"},{"location":"graphql/schema/","text":"GraphQL schema \u00b6 GraphQL schema is generated by passing all the operations (query/mutation/subscription) functions to apischema.graphql.graphql_schema . Functions parameters and return types are then processed by apischema to generate the Query / Mutation / Subscription types with their resolvers/subscribers, which are then passed to graphql.GraphQLSchema . In fact, graphql_schema is just a wrapper around graphql.GraphQLSchema (same parameters plus a few extras); it just uses apischema abstraction to build GraphQL object types directly from your code. Operations metadata \u00b6 GraphQL operations can be passed to graphql_schema either using simple functions or wrapping it into apischema.graphql.Query / apischema.graphql.Mutation / apischema.graphql.Subscription . These wrappers have the same parameters as apischema.graphql.resolver : alias , conversions , error_handler , order and schema ( Subscription has an additional parameter ). from dataclasses import dataclass from graphql import print_schema from apischema.graphql import Query , graphql_schema , resolver @dataclass class Foo : @resolver async def bar ( self , arg : int = 0 ) -> str : ... async def get_foo () -> Foo : ... schema = graphql_schema ( query = [ Query ( get_foo , alias = \"foo\" , error_handler = None )]) schema_str = \"\"\" \\ type Query { foo: Foo } type Foo { bar(arg: Int! = 0): String! }\"\"\" assert print_schema ( schema ) == schema_str camelCase \u00b6 GraphQL use camelCase as a convention for resolvers; apischema follows this convention by automatically convert all resolver names (and their parameters) to camelCase . graphql_schema has an aliaser parameter if you want to use another case. Type names \u00b6 Schema types are named the same way they are in generated JSON schema: type name is used by default, and it can be overridden using apischema.type_name from dataclasses import dataclass from graphql import print_schema from apischema import type_name from apischema.graphql import graphql_schema @type_name ( \"Foo\" ) @dataclass class FooFoo : bar : int def foo () -> FooFoo | None : ... schema = graphql_schema ( query = [ foo ]) schema_str = \"\"\" \\ type Query { foo: Foo } type Foo { bar: Int! }\"\"\" assert print_schema ( schema ) == schema_str Note Type names can be distinguished between JSON schema and GraphQL schema using type_name named parameter. Indeed, type_name(\"foo\") is equivalent to type_name(json_schema=\"foo\", graphql=\"foo\") . However, in GraphQL schema, unions must be named, so typing.Union used should be annotated with apischema.type_name . graphql_schema also provides a union_ref parameter which can be passed as a function to generate a type name from the union argument. Default union_ref is \"Or\".join meaning typing.Union[Foo, Bar] will result in union FooOrBar = Foo | Bar from dataclasses import dataclass from graphql import print_schema from apischema.graphql import graphql_schema @dataclass class Foo : foo : int @dataclass class Bar : bar : int def foo_or_bar () -> Foo | Bar : ... # union_ref default value is made explicit here schema = graphql_schema ( query = [ foo_or_bar ], union_name = \"Or\" . join ) schema_str = \"\"\" \\ type Query { fooOrBar: FooOrBar! } union FooOrBar = Foo | Bar type Foo { foo: Int! } type Bar { bar: Int! }\"\"\" assert print_schema ( schema ) == schema_str Enum metadata \u00b6 Contrary to dataclasses, Enum doesn't provide a way to set metadata for its members, especially description, but also deprecation reason. They can however be passed using enum_schemas parameter of graphql_schema . from enum import Enum from graphql import graphql_sync from graphql.utilities import print_schema from apischema import schema from apischema.graphql import graphql_schema class MyEnum ( Enum ): FOO = \"FOO\" BAR = \"BAR\" def echo ( enum : MyEnum ) -> MyEnum : return enum schema_ = graphql_schema ( query = [ echo ], enum_schemas = { MyEnum . FOO : schema ( description = \"foo\" )} ) schema_str = ''' \\ type Query { echo(enum: MyEnum!): MyEnum! } enum MyEnum { \"\"\"foo\"\"\" FOO BAR }''' assert print_schema ( schema_ ) == schema_str assert graphql_sync ( schema_ , \"{echo(enum: FOO)}\" ) . data == { \"echo\" : \"FOO\" } Additional types \u00b6 apischema will only include in the schema the types annotating resolvers. However, it is possible to add other types by using the types parameter of graphql_schema . This is especially useful to add interface implementations where only interface is used in resolver types. from dataclasses import dataclass from graphql import print_schema from apischema.graphql import graphql_schema , interface @interface @dataclass class Bar : bar : int @dataclass class Foo ( Bar ): baz : str def bar () -> Bar : ... schema = graphql_schema ( query = [ bar ], types = [ Foo ]) # type Foo would have not been present if Foo was not put in types schema_str = \"\"\" \\ type Foo implements Bar { bar: Int! baz: String! } interface Bar { bar: Int! } type Query { bar: Bar! }\"\"\" assert print_schema ( schema ) == schema_str Subscriptions \u00b6 Subscriptions are particular operations which must return an AsyncIterable ; this event generator can come with a dedicated resolver to post-process the event. Event generator only \u00b6 import asyncio from typing import AsyncIterable import graphql from graphql import print_schema from apischema.graphql import graphql_schema def hello () -> str : return \"world\" async def events () -> AsyncIterable [ str ]: yield \"bonjour\" yield \"au revoir\" schema = graphql_schema ( query = [ hello ], subscription = [ events ]) schema_str = \"\"\" \\ type Query { hello: String! } type Subscription { events: String! }\"\"\" assert print_schema ( schema ) == schema_str async def test (): subscription = await graphql . subscribe ( schema , graphql . parse ( \"subscription {events} \" ) ) assert [ event . data async for event in subscription ] == [ { \"events\" : \"bonjour\" }, { \"events\" : \"au revoir\" }, ] asyncio . run ( test ()) Note Because there is no post-processing of generated event in a dedicated resolver, error_handler cannot be called, but it will still modify the type of the event. Event generator + resolver \u00b6 A resolver can be added by using the resolver parameter of Subscription . In this case, apischema will map subscription name, parameters and return type on the resolver instead of the event generator. It allows using the same event generator with several resolvers to create different subscriptions. The first resolver argument will be the event yielded by the event generator. import asyncio from dataclasses import dataclass from typing import AsyncIterable import graphql from graphql import print_schema from apischema.graphql import Subscription , graphql_schema def hello () -> str : return \"world\" async def events () -> AsyncIterable [ str ]: yield \"bonjour\" yield \"au revoir\" @dataclass class Message : body : str # Message can also be used directly as a function schema = graphql_schema ( query = [ hello ], subscription = [ Subscription ( events , alias = \"messageReceived\" , resolver = Message )], ) schema_str = \"\"\" \\ type Query { hello: String! } type Subscription { messageReceived: Message! } type Message { body: String! }\"\"\" assert print_schema ( schema ) == schema_str async def test (): subscription = await graphql . subscribe ( schema , graphql . parse ( \"subscription {messageReceived {body} }\" ) ) assert [ event . data async for event in subscription ] == [ { \"messageReceived\" : { \"body\" : \"bonjour\" }}, { \"messageReceived\" : { \"body\" : \"au revoir\" }}, ] asyncio . run ( test ())","title":"GraphQL schema"},{"location":"graphql/schema/#graphql-schema","text":"GraphQL schema is generated by passing all the operations (query/mutation/subscription) functions to apischema.graphql.graphql_schema . Functions parameters and return types are then processed by apischema to generate the Query / Mutation / Subscription types with their resolvers/subscribers, which are then passed to graphql.GraphQLSchema . In fact, graphql_schema is just a wrapper around graphql.GraphQLSchema (same parameters plus a few extras); it just uses apischema abstraction to build GraphQL object types directly from your code.","title":"GraphQL schema"},{"location":"graphql/schema/#operations-metadata","text":"GraphQL operations can be passed to graphql_schema either using simple functions or wrapping it into apischema.graphql.Query / apischema.graphql.Mutation / apischema.graphql.Subscription . These wrappers have the same parameters as apischema.graphql.resolver : alias , conversions , error_handler , order and schema ( Subscription has an additional parameter ). from dataclasses import dataclass from graphql import print_schema from apischema.graphql import Query , graphql_schema , resolver @dataclass class Foo : @resolver async def bar ( self , arg : int = 0 ) -> str : ... async def get_foo () -> Foo : ... schema = graphql_schema ( query = [ Query ( get_foo , alias = \"foo\" , error_handler = None )]) schema_str = \"\"\" \\ type Query { foo: Foo } type Foo { bar(arg: Int! = 0): String! }\"\"\" assert print_schema ( schema ) == schema_str","title":"Operations metadata"},{"location":"graphql/schema/#camelcase","text":"GraphQL use camelCase as a convention for resolvers; apischema follows this convention by automatically convert all resolver names (and their parameters) to camelCase . graphql_schema has an aliaser parameter if you want to use another case.","title":"camelCase"},{"location":"graphql/schema/#type-names","text":"Schema types are named the same way they are in generated JSON schema: type name is used by default, and it can be overridden using apischema.type_name from dataclasses import dataclass from graphql import print_schema from apischema import type_name from apischema.graphql import graphql_schema @type_name ( \"Foo\" ) @dataclass class FooFoo : bar : int def foo () -> FooFoo | None : ... schema = graphql_schema ( query = [ foo ]) schema_str = \"\"\" \\ type Query { foo: Foo } type Foo { bar: Int! }\"\"\" assert print_schema ( schema ) == schema_str Note Type names can be distinguished between JSON schema and GraphQL schema using type_name named parameter. Indeed, type_name(\"foo\") is equivalent to type_name(json_schema=\"foo\", graphql=\"foo\") . However, in GraphQL schema, unions must be named, so typing.Union used should be annotated with apischema.type_name . graphql_schema also provides a union_ref parameter which can be passed as a function to generate a type name from the union argument. Default union_ref is \"Or\".join meaning typing.Union[Foo, Bar] will result in union FooOrBar = Foo | Bar from dataclasses import dataclass from graphql import print_schema from apischema.graphql import graphql_schema @dataclass class Foo : foo : int @dataclass class Bar : bar : int def foo_or_bar () -> Foo | Bar : ... # union_ref default value is made explicit here schema = graphql_schema ( query = [ foo_or_bar ], union_name = \"Or\" . join ) schema_str = \"\"\" \\ type Query { fooOrBar: FooOrBar! } union FooOrBar = Foo | Bar type Foo { foo: Int! } type Bar { bar: Int! }\"\"\" assert print_schema ( schema ) == schema_str","title":"Type names"},{"location":"graphql/schema/#enum-metadata","text":"Contrary to dataclasses, Enum doesn't provide a way to set metadata for its members, especially description, but also deprecation reason. They can however be passed using enum_schemas parameter of graphql_schema . from enum import Enum from graphql import graphql_sync from graphql.utilities import print_schema from apischema import schema from apischema.graphql import graphql_schema class MyEnum ( Enum ): FOO = \"FOO\" BAR = \"BAR\" def echo ( enum : MyEnum ) -> MyEnum : return enum schema_ = graphql_schema ( query = [ echo ], enum_schemas = { MyEnum . FOO : schema ( description = \"foo\" )} ) schema_str = ''' \\ type Query { echo(enum: MyEnum!): MyEnum! } enum MyEnum { \"\"\"foo\"\"\" FOO BAR }''' assert print_schema ( schema_ ) == schema_str assert graphql_sync ( schema_ , \"{echo(enum: FOO)}\" ) . data == { \"echo\" : \"FOO\" }","title":"Enum metadata"},{"location":"graphql/schema/#additional-types","text":"apischema will only include in the schema the types annotating resolvers. However, it is possible to add other types by using the types parameter of graphql_schema . This is especially useful to add interface implementations where only interface is used in resolver types. from dataclasses import dataclass from graphql import print_schema from apischema.graphql import graphql_schema , interface @interface @dataclass class Bar : bar : int @dataclass class Foo ( Bar ): baz : str def bar () -> Bar : ... schema = graphql_schema ( query = [ bar ], types = [ Foo ]) # type Foo would have not been present if Foo was not put in types schema_str = \"\"\" \\ type Foo implements Bar { bar: Int! baz: String! } interface Bar { bar: Int! } type Query { bar: Bar! }\"\"\" assert print_schema ( schema ) == schema_str","title":"Additional types"},{"location":"graphql/schema/#subscriptions","text":"Subscriptions are particular operations which must return an AsyncIterable ; this event generator can come with a dedicated resolver to post-process the event.","title":"Subscriptions"},{"location":"graphql/schema/#event-generator-only","text":"import asyncio from typing import AsyncIterable import graphql from graphql import print_schema from apischema.graphql import graphql_schema def hello () -> str : return \"world\" async def events () -> AsyncIterable [ str ]: yield \"bonjour\" yield \"au revoir\" schema = graphql_schema ( query = [ hello ], subscription = [ events ]) schema_str = \"\"\" \\ type Query { hello: String! } type Subscription { events: String! }\"\"\" assert print_schema ( schema ) == schema_str async def test (): subscription = await graphql . subscribe ( schema , graphql . parse ( \"subscription {events} \" ) ) assert [ event . data async for event in subscription ] == [ { \"events\" : \"bonjour\" }, { \"events\" : \"au revoir\" }, ] asyncio . run ( test ()) Note Because there is no post-processing of generated event in a dedicated resolver, error_handler cannot be called, but it will still modify the type of the event.","title":"Event generator only"},{"location":"graphql/schema/#event-generator-resolver","text":"A resolver can be added by using the resolver parameter of Subscription . In this case, apischema will map subscription name, parameters and return type on the resolver instead of the event generator. It allows using the same event generator with several resolvers to create different subscriptions. The first resolver argument will be the event yielded by the event generator. import asyncio from dataclasses import dataclass from typing import AsyncIterable import graphql from graphql import print_schema from apischema.graphql import Subscription , graphql_schema def hello () -> str : return \"world\" async def events () -> AsyncIterable [ str ]: yield \"bonjour\" yield \"au revoir\" @dataclass class Message : body : str # Message can also be used directly as a function schema = graphql_schema ( query = [ hello ], subscription = [ Subscription ( events , alias = \"messageReceived\" , resolver = Message )], ) schema_str = \"\"\" \\ type Query { hello: String! } type Subscription { messageReceived: Message! } type Message { body: String! }\"\"\" assert print_schema ( schema ) == schema_str async def test (): subscription = await graphql . subscribe ( schema , graphql . parse ( \"subscription {messageReceived {body} }\" ) ) assert [ event . data async for event in subscription ] == [ { \"messageReceived\" : { \"body\" : \"bonjour\" }}, { \"messageReceived\" : { \"body\" : \"au revoir\" }}, ] asyncio . run ( test ())","title":"Event generator + resolver"}]}